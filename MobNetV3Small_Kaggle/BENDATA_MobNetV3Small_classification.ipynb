{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fhwRSFoj6C_"
   },
   "source": [
    "# SWITCH TO **`T4 GPU`** OR THE **`HPC`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4V83PflfFkL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "kP6UczzNe1l2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O24_U-m8q-xv",
    "outputId": "f2298893-2e7e-4b8f-cc38-0caeb1a6a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IF_vPVifaU9V"
   },
   "outputs": [],
   "source": [
    "# makes it so pd dfs aren't truncated\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eocC68amnhEI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_MvRvYnfIM5"
   },
   "source": [
    "# 1) DATA PRE-PROCESSING\n",
    "\n",
    "a) Load in kaggle data labels + image file paths\n",
    "\n",
    "b) combine kaggle data labels and image file paths into one dataframe\n",
    "\n",
    "c) load in the extra 486 image file paths\n",
    "\n",
    "d) extract the speed and angle labels from the file path names\n",
    "\n",
    "e) store that extra data in a pandas df and do the value normalisation\n",
    "\n",
    "f) merge the kaggle and extra data dfs\n",
    "\n",
    "g) EDA\n",
    "\n",
    "h) convert the images to numerical RGB feature maps\n",
    "\n",
    "i) split data into training-validation sets\n",
    "\n",
    "j) data augmentation applied to training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU3TvBZ5hfhX"
   },
   "source": [
    "### 1a) load in kaggle data labels + image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZiNf_BxOfEH-"
   },
   "outputs": [],
   "source": [
    "# labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path\n",
    "labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)\n",
    "# labels_file_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_norm.csv' # tyler hpc file path (mlis2 cluster)\n",
    "labels_df = pd.read_csv(labels_file_path, index_col='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nOXmN--gb-Q9"
   },
   "outputs": [],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data' # OG data ben hpc file path (mlis2 cluster)\n",
    "# image_folder_path = '/home/ppytr13/machine-learning-in-science-ii-2025//training_data/training_data'\n",
    "# image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oeuvmeZaGSC"
   },
   "source": [
    "Checking labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pi13TZ2aFhO",
    "outputId": "fc675bb2-271b-48fd-a6c3-43834afb4500"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed\n",
       "image_id               \n",
       "1         0.4375    0.0\n",
       "2         0.8125    1.0\n",
       "3         0.4375    1.0\n",
       "4         0.6250    1.0\n",
       "5         0.5000    0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puEjGoOJaRS4"
   },
   "source": [
    "Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1suFSK7aWKH",
    "outputId": "c3cc2d29-d759-48ff-b92c-77dbd178f295"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      image_file_paths\n",
       "image_id                                                                                              \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjDdyYd6cMBE"
   },
   "source": [
    "### 1b) Combine the kaggle labels and image file paths into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6NdbonzPcLKB"
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')\n",
    "kaggle_df['speed'] = kaggle_df['speed'].round(6) # to get rid of floating point errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VstirIAdAZi",
    "outputId": "c03ff707-9e8d-4c3a-8965-f795919ace21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13796</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13794     0.6250    1.0   \n",
       "13795     0.4375    1.0   \n",
       "13796     0.5625    0.0   \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13794     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png  \n",
       "13795     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png  \n",
       "13796     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png  \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MgNoL8nfBm2",
    "outputId": "924e7562-25a4-4223-8305-c3fd02452846"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          angle  speed  \\\n",
       "image_id                 \n",
       "3139      0.750    1.0   \n",
       "3140      0.875    1.0   \n",
       "3142      0.625    0.0   \n",
       "3143      0.625    1.0   \n",
       "\n",
       "                                                                                         image_file_paths  \n",
       "image_id                                                                                                   \n",
       "3139      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png  \n",
       "3140      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png  \n",
       "3142      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png  \n",
       "3143      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.loc[3139:3143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7PCxqJbmXE6"
   },
   "source": [
    "The above cell shows that:\n",
    "\n",
    " 1) the image files and labels match (see image_id and the number at the end of the file path)\n",
    "\n",
    " 2) the missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOEWqBUYX6DL"
   },
   "source": [
    "### 1c) load in the extra 486 labels image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "petrudata_folder_path = '/home/apyba3/petru_data'\n",
    "bendata_folder_path = '/home/apyba3/bendata_1st_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filepaths(extradata_folderpath):\n",
    "    extradata_file_paths = [\n",
    "    os.path.join(extradata_folderpath, f)\n",
    "    for f in os.listdir(extradata_folderpath)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ]\n",
    "    return extradata_file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ofcGILO4et"
   },
   "source": [
    "### 1d) extract the speed and angle labels from the file path names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFsEI4MBRf2l"
   },
   "source": [
    "image file path name follows the pattern: `randomnumber_angle_speed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadin_extradata(extradata_folderpath):\n",
    "    extracted_filepaths = extract_filepaths(extradata_folderpath)\n",
    "    # Regex pattern to extract angle and speed values\n",
    "    pattern = r'(\\d+)_(\\d+)_(\\d+)\\.png'  # Fixed pattern to capture groups correctly\n",
    "\n",
    "    angle_value = []\n",
    "    speed_value = []\n",
    "\n",
    "    # Loop through file paths and extract angle and speed values\n",
    "    for file_path in extracted_filepaths:\n",
    "        match = re.search(pattern, file_path)\n",
    "        if match:\n",
    "            # Extract random number, angle, and speed values\n",
    "            random_number = match.group(1)\n",
    "            angle_value.append(int(match.group(2)))\n",
    "            speed_value.append(int(match.group(3)))\n",
    "        else:\n",
    "            print(f\"No match found for file: {file_path}\")\n",
    "\n",
    "    return angle_value, speed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8qIQJ8Y3t8"
   },
   "source": [
    "checking it has stored the labels correctly (check if the angle_value order matches that of the file path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyvljUTBZP0E"
   },
   "source": [
    "### 1e) store that extra data in a pandas df and do the value normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16402</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712922464963_75_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16403</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712918948764_95_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16404</th>\n",
       "      <td>0.3125</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712921132962_75_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712917893422_110_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16406</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712918494936_60_35.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed                                  image_file_paths\n",
       "image_id                                                                 \n",
       "16402     0.3125      1   /home/apyba3/petru_data/1712922464963_75_35.png\n",
       "16403     0.5625      1   /home/apyba3/petru_data/1712918948764_95_35.png\n",
       "16404     0.3125      1   /home/apyba3/petru_data/1712921132962_75_35.png\n",
       "16405     0.7500      1  /home/apyba3/petru_data/1712917893422_110_35.png\n",
       "16406     0.1250      1   /home/apyba3/petru_data/1712918494936_60_35.png"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_df_and_normalisation(folder_path):\n",
    "    max_image_id = len(loadin_extradata(folder_path)[0])\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'angle': loadin_extradata(folder_path)[0],\n",
    "            'speed': loadin_extradata(folder_path)[1],\n",
    "            'image_file_paths': extract_filepaths(folder_path)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df.loc[df['speed'] > 0, 'speed'] = 1\n",
    "    df['angle'] = (df['angle'] - 50)/80\n",
    "    return df\n",
    "\n",
    "def combine_dfs_add_imageid(df1, df2):\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    combined_df.index = pd.RangeIndex(start=13799, stop=13799 + len(combined_df), step=1)\n",
    "    combined_df.index.name = 'image_id'\n",
    "    return combined_df\n",
    "\n",
    "bendata_df = create_df_and_normalisation(bendata_folder_path)\n",
    "petrudata_df = create_df_and_normalisation(petrudata_folder_path)\n",
    "\n",
    "combinedextradata_df = combine_dfs_add_imageid(bendata_df, petrudata_df)\n",
    "display(combinedextradata_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv0MwDKsbOef"
   },
   "source": [
    "### 1f) merge the kaggle and extra data dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZMZPUn4b3Kc",
    "outputId": "86bd34db-0b48-442e-b5b2-5ff322d0764b"
   },
   "outputs": [],
   "source": [
    "merged_df = pd.concat([kaggle_df, combinedextradata_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1g) check for corrupted images and get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 16401 image files...\n",
      "\n",
      "Validation complete: 16373 valid files, 28 invalid files\n"
     ]
    }
   ],
   "source": [
    "def validate_image_files(dataframe):\n",
    "    \"\"\"\n",
    "    Check each image file in the dataframe and return a new dataframe with only valid files.\n",
    "    Also prints information about invalid files.\n",
    "    \"\"\"\n",
    "    valid_rows = []\n",
    "    invalid_files = []\n",
    "    \n",
    "    print(f\"Checking {len(dataframe)} image files...\")\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        path = row['image_file_paths']\n",
    "        \n",
    "        # # Print every 100 files to show progress\n",
    "        # if len(valid_rows) % 100 == 0 and len(valid_rows) > 0:\n",
    "        #     print(f\"Processed {len(valid_rows) + len(invalid_files)} files, {len(valid_rows)} valid...\")\n",
    "        \n",
    "        try:\n",
    "            # Basic file checks\n",
    "            if not os.path.exists(path):\n",
    "                # print(f\"File does not exist: {path}\")\n",
    "                invalid_files.append(path)\n",
    "                continue\n",
    "                \n",
    "            if os.path.getsize(path) == 0:\n",
    "                # print(f\"File is empty: {path}\")\n",
    "                invalid_files.append(path)\n",
    "                continue\n",
    "            \n",
    "            # Try to open the image with PIL to verify it's valid\n",
    "            with Image.open(path) as img:\n",
    "                # Just accessing img.size will verify the image can be read\n",
    "                width, height = img.size\n",
    "            \n",
    "            # If we get this far, the image is valid\n",
    "            valid_rows.append(row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {path}: {e}\")\n",
    "            invalid_files.append(path)\n",
    "    \n",
    "    # Create a new dataframe with only valid rows\n",
    "    valid_df = pd.DataFrame(valid_rows)\n",
    "    \n",
    "    print(f\"\\nValidation complete: {len(valid_df)} valid files, {len(invalid_files)} invalid files\")\n",
    "    \n",
    "    # if invalid_files:\n",
    "    #     print(\"\\nFirst few invalid files:\")\n",
    "    #     for path in invalid_files[:5]:\n",
    "    #         print(f\"  - {path}\")\n",
    "    \n",
    "    return valid_df\n",
    "\n",
    "# Run the validation\n",
    "valid_df = validate_image_files(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3OKLcn9u0Pz"
   },
   "source": [
    "### 1g) EDA - speed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWQCQrR-oCps",
    "outputId": "88bb4558-2c8a-482b-de5d-8f7876ed9bc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.000000    12553\n",
       "0.000000     3819\n",
       "1.428571        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4pZ65pYvdqb"
   },
   "source": [
    "note: imbalance datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMZq41-RkLz0"
   },
   "source": [
    "we want to remove the row containing the erroneous 1.428571 speed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TDMqIiOLSKGX"
   },
   "outputs": [],
   "source": [
    "cleaned_df = valid_df[valid_df['speed'] != 1.428571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.0    12553\n",
       "0.0     3819\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di6F6km_DBmj"
   },
   "source": [
    "### 1h) convert images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oeeBTruNCQ96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 19:12:18.513108: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, label, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, resized_shape)\n",
    "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((cleaned_df[\"image_file_paths\"], cleaned_df[\"speed\"])) # Convert pd df into a tf ds\n",
    "\n",
    "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(len(cleaned_df))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOlsWQeVlyC"
   },
   "source": [
    "lets check and see if what we have done works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBTNjNhMVk2g",
    "outputId": "b00f1443-c179-43a2-e6fd-7cc90ff698f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md6U_i84SiK5"
   },
   "source": [
    "### 1i) Splitting data into training and validation sets (test set is already provided in kaggle data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yYlssPh5dxaO"
   },
   "outputs": [],
   "source": [
    "# 90-10 split\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_size = int(0.9 * dataset_size)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPUE6rd8cgQN",
    "outputId": "a418b177-e08d-481c-d272-b9b7494882d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 460, validation size: 52\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_size}, validation size: {dataset_size - train_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ujsjhMPSw4f"
   },
   "source": [
    "### 1j) Data Augmentation applied to training set\n",
    "\n",
    "- Random Brightness Adjustment\n",
    "- Random Contrast Adjustment\n",
    "- Random Hue Adjustment\n",
    "- Random Saturation Adjustment\n",
    "- Random Horizontal Flip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "T9r811eWsYfe"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "  seed = (6, 9)\n",
    "  image = tf.image.stateless_random_brightness(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_contrast(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_saturation(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "  return image, label\n",
    "\n",
    "# Create a dataset of augmented images from the original train_dataset\n",
    "augmented_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "train_dataset = train_dataset.concatenate(augmented_dataset)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0     6865\n",
      "1.0    22575\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "angle_list = []\n",
    "\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    angle_list.extend(label_batch.numpy())  # add all 32 values from the batch\n",
    "\n",
    "angle_distribution = pd.Series(angle_list).value_counts().sort_index()\n",
    "print(angle_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOqizFg7rvKq"
   },
   "source": [
    "count how many images are in the training set - 22016 with no extradata and 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjlyfjAxLsrC",
    "outputId": "14dc79ee-e1b4-4c37-bfb1-b6525bc586c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train_dataset: 29440\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "for image_batch, _ in train_dataset:\n",
    "    total_images += image_batch.shape[0]  # Add the batch size\n",
    "\n",
    "print(f\"Total number of images in train_dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEdi-dUCTND1"
   },
   "source": [
    "checking to see if whats been done was successful or needs debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeboVhsQKGFS",
    "outputId": "b9c6bb08-d7ce-4951-b621-6775a6ee3bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1,10)\n",
    "\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset.take(1):  # Take one batch\n",
    "    for image in image_batch:  # Iterate through images in the batch\n",
    "        if i < 10:  # Only display the first 5 images\n",
    "            print('image shape: ', np.shape(image))\n",
    "            tf.print('label:', label_batch[i])  # Print label for the corresponding image\n",
    "            axarr[i].imshow(image)\n",
    "            axarr[i].axis('off')\n",
    "            i += 1\n",
    "        else:\n",
    "            break  # Stop after displaying 5 images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmetmzNHTWzU"
   },
   "source": [
    "# 2) CLASSIFICATION SPEED Model Building - MobNetV3Small Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48RHLVshdX5L"
   },
   "source": [
    "### 2a) Set up model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aHjqXG1jSnCr"
   },
   "outputs": [],
   "source": [
    "dropoutrate = 0.2\n",
    "input_shape = (224,224,3)\n",
    "num_classes = 1 # we're only predicting the prob of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ZPso3wBuN9L3",
    "outputId": "ef11a9ff-7117-4836-dd78-9bcadac15995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 576)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               147712    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130097 (4.31 MB)\n",
      "Trainable params: 190977 (746.00 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mobnetv3small.trainable = False  # freeze mobnetv3small layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Ca0JFQuuN8oI"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) train the model with the mobnetv3small layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WPTNOtr7WjLS",
    "outputId": "63cf5fac-6100-43db-dadd-47da686c9712",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.7586 - auc: 0.5185\n",
      "Epoch 1: val_loss improved from inf to 0.5510\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 1, Loss: 0.5576, Val Loss: 0.5510\n",
      "714/714 [==============================] - 68s 83ms/step - loss: 0.5576 - accuracy: 0.7586 - auc: 0.5185 - val_loss: 0.5510 - val_accuracy: 0.7530 - val_auc: 0.7601\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.7581 - auc: 0.5972\n",
      "Epoch 2: val_loss improved from 0.5510 to 0.5142\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 2, Loss: 0.5427, Val Loss: 0.5142\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.5427 - accuracy: 0.7581 - auc: 0.5972 - val_loss: 0.5142 - val_accuracy: 0.7652 - val_auc: 0.7884\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.7577 - auc: 0.6586\n",
      "Epoch 3: val_loss improved from 0.5142 to 0.4936\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 3, Loss: 0.5255, Val Loss: 0.4936\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.5255 - accuracy: 0.7577 - auc: 0.6586 - val_loss: 0.4936 - val_accuracy: 0.7554 - val_auc: 0.8392\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.7592 - auc: 0.6890Completed Epoch 4, Loss: 0.5129, Val Loss: 0.4955\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.5129 - accuracy: 0.7592 - auc: 0.6890 - val_loss: 0.4955 - val_accuracy: 0.7460 - val_auc: 0.8417\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.7636 - auc: 0.6996Completed Epoch 5, Loss: 0.5063, Val Loss: 0.5048\n",
      "714/714 [==============================] - 63s 81ms/step - loss: 0.5063 - accuracy: 0.7636 - auc: 0.6996 - val_loss: 0.5048 - val_accuracy: 0.7519 - val_auc: 0.8428\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7665 - auc: 0.7115\n",
      "Epoch 6: val_loss improved from 0.4936 to 0.4881\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 6, Loss: 0.5001, Val Loss: 0.4881\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.5001 - accuracy: 0.7665 - auc: 0.7115 - val_loss: 0.4881 - val_accuracy: 0.8332 - val_auc: 0.8688\n",
      "\n",
      "Starting Epoch 7...\n",
      "Epoch 7/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.7708 - auc: 0.7253\n",
      "Epoch 7: val_loss improved from 0.4881 to 0.4529\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 7, Loss: 0.4917, Val Loss: 0.4529\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4917 - accuracy: 0.7708 - auc: 0.7253 - val_loss: 0.4529 - val_accuracy: 0.7694 - val_auc: 0.8728\n",
      "\n",
      "Starting Epoch 8...\n",
      "Epoch 8/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.7698 - auc: 0.7282Completed Epoch 8, Loss: 0.4917, Val Loss: 0.5406\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4917 - accuracy: 0.7698 - auc: 0.7282 - val_loss: 0.5406 - val_accuracy: 0.7586 - val_auc: 0.8646\n",
      "\n",
      "Starting Epoch 9...\n",
      "Epoch 9/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.7700 - auc: 0.7368\n",
      "Epoch 9: val_loss improved from 0.4529 to 0.4482\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 9, Loss: 0.4876, Val Loss: 0.4482\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4876 - accuracy: 0.7700 - auc: 0.7368 - val_loss: 0.4482 - val_accuracy: 0.7744 - val_auc: 0.8739\n",
      "\n",
      "Starting Epoch 10...\n",
      "Epoch 10/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4851 - accuracy: 0.7779 - auc: 0.7346\n",
      "Epoch 10: val_loss improved from 0.4482 to 0.4334\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 10, Loss: 0.4851, Val Loss: 0.4334\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4851 - accuracy: 0.7779 - auc: 0.7346 - val_loss: 0.4334 - val_accuracy: 0.8367 - val_auc: 0.8742\n",
      "\n",
      "Starting Epoch 11...\n",
      "Epoch 11/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.7741 - auc: 0.7418Completed Epoch 11, Loss: 0.4834, Val Loss: 0.4780\n",
      "714/714 [==============================] - 63s 81ms/step - loss: 0.4834 - accuracy: 0.7741 - auc: 0.7418 - val_loss: 0.4780 - val_accuracy: 0.7579 - val_auc: 0.8677\n",
      "\n",
      "Starting Epoch 12...\n",
      "Epoch 12/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.7729 - auc: 0.7365Completed Epoch 12, Loss: 0.4873, Val Loss: 0.4496\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4873 - accuracy: 0.7729 - auc: 0.7365 - val_loss: 0.4496 - val_accuracy: 0.7733 - val_auc: 0.8800\n",
      "\n",
      "Starting Epoch 13...\n",
      "Epoch 13/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.7801 - auc: 0.7455Completed Epoch 13, Loss: 0.4793, Val Loss: 0.4947\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4793 - accuracy: 0.7801 - auc: 0.7455 - val_loss: 0.4947 - val_accuracy: 0.7512 - val_auc: 0.8750\n",
      "\n",
      "Starting Epoch 14...\n",
      "Epoch 14/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.7802 - auc: 0.7453Completed Epoch 14, Loss: 0.4783, Val Loss: 0.4720\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4783 - accuracy: 0.7802 - auc: 0.7453 - val_loss: 0.4720 - val_accuracy: 0.7579 - val_auc: 0.8855\n",
      "\n",
      "Starting Epoch 15...\n",
      "Epoch 15/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.7801 - auc: 0.7481Completed Epoch 15, Loss: 0.4781, Val Loss: 0.4371\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4781 - accuracy: 0.7801 - auc: 0.7481 - val_loss: 0.4371 - val_accuracy: 0.8097 - val_auc: 0.8898\n",
      "\n",
      "Starting Epoch 16...\n",
      "Epoch 16/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.7790 - auc: 0.7506\n",
      "Epoch 16: val_loss improved from 0.4334 to 0.4215\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 16, Loss: 0.4773, Val Loss: 0.4215\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4773 - accuracy: 0.7790 - auc: 0.7506 - val_loss: 0.4215 - val_accuracy: 0.8003 - val_auc: 0.8871\n",
      "\n",
      "Starting Epoch 17...\n",
      "Epoch 17/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.7798 - auc: 0.7540Completed Epoch 17, Loss: 0.4751, Val Loss: 0.4774\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4751 - accuracy: 0.7798 - auc: 0.7540 - val_loss: 0.4774 - val_accuracy: 0.8255 - val_auc: 0.8870\n",
      "\n",
      "Starting Epoch 18...\n",
      "Epoch 18/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7809 - auc: 0.7576Completed Epoch 18, Loss: 0.4741, Val Loss: 0.4585\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4741 - accuracy: 0.7809 - auc: 0.7576 - val_loss: 0.4585 - val_accuracy: 0.7744 - val_auc: 0.8945\n",
      "\n",
      "Starting Epoch 19...\n",
      "Epoch 19/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.7816 - auc: 0.7502Completed Epoch 19, Loss: 0.4766, Val Loss: 0.4735\n",
      "714/714 [==============================] - 63s 81ms/step - loss: 0.4766 - accuracy: 0.7816 - auc: 0.7502 - val_loss: 0.4735 - val_accuracy: 0.7565 - val_auc: 0.8922\n",
      "\n",
      "Starting Epoch 20...\n",
      "Epoch 20/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7818 - auc: 0.7536Completed Epoch 20, Loss: 0.4749, Val Loss: 0.4391\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4749 - accuracy: 0.7818 - auc: 0.7536 - val_loss: 0.4391 - val_accuracy: 0.8206 - val_auc: 0.8867\n",
      "\n",
      "Starting Epoch 21...\n",
      "Epoch 21/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.7831 - auc: 0.7624\n",
      "Epoch 21: val_loss improved from 0.4215 to 0.4145\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 21, Loss: 0.4694, Val Loss: 0.4145\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4694 - accuracy: 0.7831 - auc: 0.7624 - val_loss: 0.4145 - val_accuracy: 0.8322 - val_auc: 0.9009\n",
      "\n",
      "Starting Epoch 22...\n",
      "Epoch 22/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.7839 - auc: 0.7576Completed Epoch 22, Loss: 0.4706, Val Loss: 0.4239\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4706 - accuracy: 0.7839 - auc: 0.7576 - val_loss: 0.4239 - val_accuracy: 0.7954 - val_auc: 0.8787\n",
      "\n",
      "Starting Epoch 23...\n",
      "Epoch 23/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.7861 - auc: 0.7648Completed Epoch 23, Loss: 0.4647, Val Loss: 0.4426\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4647 - accuracy: 0.7861 - auc: 0.7648 - val_loss: 0.4426 - val_accuracy: 0.7810 - val_auc: 0.8774\n",
      "\n",
      "Starting Epoch 24...\n",
      "Epoch 24/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.7833 - auc: 0.7539Completed Epoch 24, Loss: 0.4740, Val Loss: 0.4687\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4740 - accuracy: 0.7833 - auc: 0.7539 - val_loss: 0.4687 - val_accuracy: 0.7659 - val_auc: 0.8927\n",
      "\n",
      "Starting Epoch 25...\n",
      "Epoch 25/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4674 - accuracy: 0.7815 - auc: 0.7635Completed Epoch 25, Loss: 0.4674, Val Loss: 0.4262\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4674 - accuracy: 0.7815 - auc: 0.7635 - val_loss: 0.4262 - val_accuracy: 0.8644 - val_auc: 0.9047\n",
      "\n",
      "Starting Epoch 26...\n",
      "Epoch 26/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.7821 - auc: 0.7583Completed Epoch 26, Loss: 0.4732, Val Loss: 0.4432\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4732 - accuracy: 0.7821 - auc: 0.7583 - val_loss: 0.4432 - val_accuracy: 0.7677 - val_auc: 0.8884\n",
      "\n",
      "Starting Epoch 27...\n",
      "Epoch 27/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.7834 - auc: 0.7630Completed Epoch 27, Loss: 0.4693, Val Loss: 0.4542\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4693 - accuracy: 0.7834 - auc: 0.7630 - val_loss: 0.4542 - val_accuracy: 0.7649 - val_auc: 0.8892\n",
      "\n",
      "Starting Epoch 28...\n",
      "Epoch 28/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.7840 - auc: 0.7640Completed Epoch 28, Loss: 0.4657, Val Loss: 0.4235\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4657 - accuracy: 0.7840 - auc: 0.7640 - val_loss: 0.4235 - val_accuracy: 0.8420 - val_auc: 0.9024\n",
      "\n",
      "Starting Epoch 29...\n",
      "Epoch 29/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4651 - accuracy: 0.7838 - auc: 0.7681Completed Epoch 29, Loss: 0.4651, Val Loss: 0.4365\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4651 - accuracy: 0.7838 - auc: 0.7681 - val_loss: 0.4365 - val_accuracy: 0.7744 - val_auc: 0.9042\n",
      "\n",
      "Starting Epoch 30...\n",
      "Epoch 30/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.7891 - auc: 0.7688Completed Epoch 30, Loss: 0.4622, Val Loss: 0.4774\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4622 - accuracy: 0.7891 - auc: 0.7688 - val_loss: 0.4774 - val_accuracy: 0.7516 - val_auc: 0.8949\n",
      "\n",
      "Starting Epoch 31...\n",
      "Epoch 31/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.7878 - auc: 0.7712Completed Epoch 31, Loss: 0.4611, Val Loss: 0.4758\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4611 - accuracy: 0.7878 - auc: 0.7712 - val_loss: 0.4758 - val_accuracy: 0.7572 - val_auc: 0.8938\n",
      "\n",
      "Starting Epoch 32...\n",
      "Epoch 32/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.7899 - auc: 0.7681Completed Epoch 32, Loss: 0.4631, Val Loss: 0.4599\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4631 - accuracy: 0.7899 - auc: 0.7681 - val_loss: 0.4599 - val_accuracy: 0.8132 - val_auc: 0.8900\n",
      "\n",
      "Starting Epoch 33...\n",
      "Epoch 33/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7860 - auc: 0.7731Completed Epoch 33, Loss: 0.4607, Val Loss: 0.4163\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4607 - accuracy: 0.7860 - auc: 0.7731 - val_loss: 0.4163 - val_accuracy: 0.7793 - val_auc: 0.8958\n",
      "\n",
      "Starting Epoch 34...\n",
      "Epoch 34/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.7846 - auc: 0.7647Completed Epoch 34, Loss: 0.4673, Val Loss: 0.4545\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4673 - accuracy: 0.7846 - auc: 0.7647 - val_loss: 0.4545 - val_accuracy: 0.8118 - val_auc: 0.8880\n",
      "\n",
      "Starting Epoch 35...\n",
      "Epoch 35/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.7876 - auc: 0.7719Completed Epoch 35, Loss: 0.4628, Val Loss: 0.4329\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4628 - accuracy: 0.7876 - auc: 0.7719 - val_loss: 0.4329 - val_accuracy: 0.8041 - val_auc: 0.8926\n",
      "\n",
      "Starting Epoch 36...\n",
      "Epoch 36/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.7902 - auc: 0.7721Completed Epoch 36, Loss: 0.4606, Val Loss: 0.4319\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4606 - accuracy: 0.7902 - auc: 0.7721 - val_loss: 0.4319 - val_accuracy: 0.7880 - val_auc: 0.8999\n",
      "\n",
      "Starting Epoch 37...\n",
      "Epoch 37/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.7908 - auc: 0.7745Completed Epoch 37, Loss: 0.4578, Val Loss: 0.4326\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4578 - accuracy: 0.7908 - auc: 0.7745 - val_loss: 0.4326 - val_accuracy: 0.8171 - val_auc: 0.9005\n",
      "\n",
      "Starting Epoch 38...\n",
      "Epoch 38/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.7888 - auc: 0.7669Completed Epoch 38, Loss: 0.4645, Val Loss: 0.4340\n",
      "714/714 [==============================] - 63s 81ms/step - loss: 0.4645 - accuracy: 0.7888 - auc: 0.7669 - val_loss: 0.4340 - val_accuracy: 0.7929 - val_auc: 0.8876\n",
      "\n",
      "Starting Epoch 39...\n",
      "Epoch 39/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.7907 - auc: 0.7750\n",
      "Epoch 39: val_loss improved from 0.4145 to 0.4052\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 39, Loss: 0.4582, Val Loss: 0.4052\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4582 - accuracy: 0.7907 - auc: 0.7750 - val_loss: 0.4052 - val_accuracy: 0.8199 - val_auc: 0.9024\n",
      "\n",
      "Starting Epoch 40...\n",
      "Epoch 40/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.7887 - auc: 0.7720Completed Epoch 40, Loss: 0.4619, Val Loss: 0.4326\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4619 - accuracy: 0.7887 - auc: 0.7720 - val_loss: 0.4326 - val_accuracy: 0.7708 - val_auc: 0.8962\n",
      "\n",
      "Starting Epoch 41...\n",
      "Epoch 41/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.7921 - auc: 0.7746Completed Epoch 41, Loss: 0.4585, Val Loss: 0.4125\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4585 - accuracy: 0.7921 - auc: 0.7746 - val_loss: 0.4125 - val_accuracy: 0.7992 - val_auc: 0.8855\n",
      "\n",
      "Starting Epoch 42...\n",
      "Epoch 42/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.7911 - auc: 0.7758\n",
      "Epoch 42: val_loss improved from 0.4052 to 0.3961\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/50epochs_frozen_mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 42, Loss: 0.4568, Val Loss: 0.3961\n",
      "714/714 [==============================] - 64s 81ms/step - loss: 0.4568 - accuracy: 0.7911 - auc: 0.7758 - val_loss: 0.3961 - val_accuracy: 0.8083 - val_auc: 0.9003\n",
      "\n",
      "Starting Epoch 43...\n",
      "Epoch 43/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.7936 - auc: 0.7754Completed Epoch 43, Loss: 0.4576, Val Loss: 0.4232\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4576 - accuracy: 0.7936 - auc: 0.7754 - val_loss: 0.4232 - val_accuracy: 0.7912 - val_auc: 0.8934\n",
      "\n",
      "Starting Epoch 44...\n",
      "Epoch 44/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.7938 - auc: 0.7795Completed Epoch 44, Loss: 0.4551, Val Loss: 0.4209\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4551 - accuracy: 0.7938 - auc: 0.7795 - val_loss: 0.4209 - val_accuracy: 0.8178 - val_auc: 0.8992\n",
      "\n",
      "Starting Epoch 45...\n",
      "Epoch 45/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4557 - accuracy: 0.7903 - auc: 0.7782Completed Epoch 45, Loss: 0.4557, Val Loss: 0.4178\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4557 - accuracy: 0.7903 - auc: 0.7782 - val_loss: 0.4178 - val_accuracy: 0.8371 - val_auc: 0.8982\n",
      "\n",
      "Starting Epoch 46...\n",
      "Epoch 46/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.7937 - auc: 0.7847Completed Epoch 46, Loss: 0.4506, Val Loss: 0.4013\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4506 - accuracy: 0.7937 - auc: 0.7847 - val_loss: 0.4013 - val_accuracy: 0.7950 - val_auc: 0.8849\n",
      "\n",
      "Starting Epoch 47...\n",
      "Epoch 47/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.7948 - auc: 0.7835Completed Epoch 47, Loss: 0.4527, Val Loss: 0.4496\n",
      "714/714 [==============================] - 63s 81ms/step - loss: 0.4527 - accuracy: 0.7948 - auc: 0.7835 - val_loss: 0.4496 - val_accuracy: 0.8304 - val_auc: 0.8931\n",
      "\n",
      "Starting Epoch 48...\n",
      "Epoch 48/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.7962 - auc: 0.7848Completed Epoch 48, Loss: 0.4497, Val Loss: 0.4326\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4497 - accuracy: 0.7962 - auc: 0.7848 - val_loss: 0.4326 - val_accuracy: 0.7730 - val_auc: 0.9195\n",
      "\n",
      "Starting Epoch 49...\n",
      "Epoch 49/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.7941 - auc: 0.7841Completed Epoch 49, Loss: 0.4516, Val Loss: 0.4386\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4516 - accuracy: 0.7941 - auc: 0.7841 - val_loss: 0.4386 - val_accuracy: 0.7677 - val_auc: 0.8962\n",
      "\n",
      "Starting Epoch 50...\n",
      "Epoch 50/50\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.7956 - auc: 0.7840Completed Epoch 50, Loss: 0.4526, Val Loss: 0.4217\n",
      "714/714 [==============================] - 63s 80ms/step - loss: 0.4526 - accuracy: 0.7956 - auc: 0.7840 - val_loss: 0.4217 - val_accuracy: 0.8073 - val_auc: 0.8984\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory and create it if it doesn't exist\n",
    "checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint filepath\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, '50epochs_frozen_mobnetv3smallcheckpoint.keras')\n",
    "\n",
    "# Create a custom callback for model checkpointing\n",
    "class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min'):\n",
    "        super(CustomModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "        # Initialize best value based on mode\n",
    "        self.mode = mode\n",
    "        if mode == 'min':\n",
    "            self.best = float('inf')\n",
    "            self.monitor_op = lambda current, best: current < best\n",
    "        else:  # 'max'\n",
    "            self.best = float('-inf')\n",
    "            self.monitor_op = lambda current, best: current > best\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        \n",
    "        if current is None:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Warning: {self.monitor} not found in logs\")\n",
    "            return\n",
    "            \n",
    "        if self.save_best_only:\n",
    "            if self.monitor_op(current, self.best):\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"\\nEpoch {epoch+1}: {self.monitor} improved from {self.best:.4f} to {current:.4f}\")\n",
    "                    print(f\"Saving model to {self.filepath}\")\n",
    "                self.best = current\n",
    "                \n",
    "                # Use direct model.save() without extra parameters\n",
    "                try:\n",
    "                    self.model.save(self.filepath)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving model: {e}\")\n",
    "        else:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"\\nEpoch {epoch+1}: saving model to {self.filepath}\")\n",
    "            try:\n",
    "                self.model.save(self.filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Create custom checkpoint callback\n",
    "custom_checkpoint = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define epoch callback \n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[custom_checkpoint, epoch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is complete\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_curves(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a professional-looking loss curve plot with training and validation curves.\n",
    "    \n",
    "    Args:\n",
    "        history: The history object returned by model.fit()\n",
    "        save_path: Path to save the plot (without extension)\n",
    "    \"\"\"\n",
    "    # Create figure with appropriate size\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    # Plot training & validation loss on the same figure\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'o-', color='blue', linewidth=2, \n",
    "            markersize=4, label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 's-', color='red', linewidth=2, \n",
    "            markersize=4, label='Validation Loss')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Model Training Performance', fontsize=16, pad=15)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    \n",
    "    # Apply your specific styling with legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=14, loc='lower right')  # Legend to distinguish between lines\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='both', which='minor', length=4, color='gray', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='major', length=6, color='black', labelsize=14)\n",
    "    plt.tick_params(top=True, right=True, direction='in', length=6)\n",
    "    plt.tick_params(which='minor', top=True, right=True, direction='in', length=4)\n",
    "    \n",
    "    # Tighten layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in both formats if a path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}.pdf\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{save_path}.png\", bbox_inches='tight', dpi=300)\n",
    "        print(f\"Loss curves saved to {save_path}.pdf and {save_path}.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# After training completes\n",
    "loss_plot_path = os.path.join(checkpoint_dir, 'loss_curves')\n",
    "plot_loss_curves(history, save_path=loss_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/50epochs_frozen_mobnetv3small_classification_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_path = '/home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/50epochs_frozen_mobnetv3small_classification_model.keras'\n",
    "model.save(model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FiHy6opSP2sQ"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot-1/MobNetV3Small_Kaggle/weights/50epochs_frozentraining_classification_mobnetv3small.weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FpLHyw20P93U"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #Clear keras session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENHbUvQdvyFe"
   },
   "source": [
    "### 2d) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ek_ytyw0KB"
   },
   "source": [
    "rebuild model after clearing keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 576)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               147712    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130097 (4.31 MB)\n",
      "Trainable params: 1117985 (4.26 MB)\n",
      "Non-trainable params: 12112 (47.31 KB)\n",
      "_________________________________________________________________\n",
      "Successfully loaded weights and prepared model for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Rebuild the model in EXACTLY the same state as when weights were saved\n",
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "# Create model with the SAME architecture as when saving\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# IMPORTANT: Set trainable to False BEFORE loading weights\n",
    "# This matches the state when weights were saved\n",
    "mobnetv3small.trainable = False\n",
    "\n",
    "# Step 3: Compile the model\n",
    "# The optimizer doesn't need to match, but having it compiled is important\n",
    "model.compile(\n",
    "    optimizer='adam',  # Doesn't need to match original\n",
    "    loss='binary_crossentropy',  # Use your original loss function\n",
    "    metrics=['accuracy']  # Use your original metrics\n",
    ")\n",
    "\n",
    "# Step 4: Load the weights\n",
    "weights_path = '/home/apyba3/PICAR-autopilot-1/MobNetV3Small_Kaggle/weights/50epochs_frozentraining_classification_mobnetv3small.weights.keras'\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "# Step 5: NOW you can set trainable to True for fine-tuning\n",
    "mobnetv3small.trainable = True\n",
    "\n",
    "# Step 6: Re-compile with appropriate learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate for fine-tuning\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Verify model structure\n",
    "model.summary()\n",
    "\n",
    "print(\"Successfully loaded weights and prepared model for fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDtRxBow89t"
   },
   "source": [
    "Initiate fine-tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714/714 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9311\n",
      "Epoch 1: val_loss improved from inf to 0.5329\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/finetuned_50epochs_mobnetv3small_classif_checkpoint.keras\n",
      "Completed Epoch 1, Loss: 0.1795, Val Loss: 0.5329\n",
      "714/714 [==============================] - 242s 331ms/step - loss: 0.1795 - accuracy: 0.9311 - val_loss: 0.5329 - val_accuracy: 0.7701\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9561Completed Epoch 2, Loss: 0.1145, Val Loss: 2.6340\n",
      "714/714 [==============================] - 241s 330ms/step - loss: 0.1145 - accuracy: 0.9561 - val_loss: 2.6340 - val_accuracy: 0.7460\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9658Completed Epoch 3, Loss: 0.0885, Val Loss: 4.6695\n",
      "714/714 [==============================] - 241s 329ms/step - loss: 0.0885 - accuracy: 0.9658 - val_loss: 4.6695 - val_accuracy: 0.7715\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9747Completed Epoch 4, Loss: 0.0686, Val Loss: 2.3777\n",
      "714/714 [==============================] - 240s 329ms/step - loss: 0.0686 - accuracy: 0.9747 - val_loss: 2.3777 - val_accuracy: 0.7519\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9800Completed Epoch 5, Loss: 0.0529, Val Loss: 3.2974\n",
      "714/714 [==============================] - 241s 329ms/step - loss: 0.0529 - accuracy: 0.9800 - val_loss: 3.2974 - val_accuracy: 0.7579\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9838Completed Epoch 6, Loss: 0.0423, Val Loss: 1.5592\n",
      "714/714 [==============================] - 242s 331ms/step - loss: 0.0423 - accuracy: 0.9838 - val_loss: 1.5592 - val_accuracy: 0.7694\n",
      "\n",
      "Starting Epoch 7...\n",
      "Epoch 7/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9871Completed Epoch 7, Loss: 0.0335, Val Loss: 1.3348\n",
      "714/714 [==============================] - 244s 334ms/step - loss: 0.0335 - accuracy: 0.9871 - val_loss: 1.3348 - val_accuracy: 0.7758\n",
      "\n",
      "Starting Epoch 8...\n",
      "Epoch 8/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9887Completed Epoch 8, Loss: 0.0315, Val Loss: 0.6284\n",
      "714/714 [==============================] - 241s 329ms/step - loss: 0.0315 - accuracy: 0.9887 - val_loss: 0.6284 - val_accuracy: 0.8297\n",
      "\n",
      "Starting Epoch 9...\n",
      "Epoch 9/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9893\n",
      "Epoch 9: val_loss improved from 0.5329 to 0.0989\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/finetuned_50epochs_mobnetv3small_classif_checkpoint.keras\n",
      "Completed Epoch 9, Loss: 0.0273, Val Loss: 0.0989\n",
      "714/714 [==============================] - 241s 329ms/step - loss: 0.0273 - accuracy: 0.9893 - val_loss: 0.0989 - val_accuracy: 0.9636\n",
      "\n",
      "Starting Epoch 10...\n",
      "Epoch 10/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9934Completed Epoch 10, Loss: 0.0181, Val Loss: 0.1770\n",
      "714/714 [==============================] - 240s 328ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.1770 - val_accuracy: 0.9387\n",
      "\n",
      "Starting Epoch 11...\n",
      "Epoch 11/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9930\n",
      "Epoch 11: val_loss improved from 0.0989 to 0.0763\n",
      "Saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints/finetuned_50epochs_mobnetv3small_classif_checkpoint.keras\n",
      "Completed Epoch 11, Loss: 0.0183, Val Loss: 0.0763\n",
      "714/714 [==============================] - 241s 330ms/step - loss: 0.0183 - accuracy: 0.9930 - val_loss: 0.0763 - val_accuracy: 0.9769\n",
      "\n",
      "Starting Epoch 12...\n",
      "Epoch 12/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9941Completed Epoch 12, Loss: 0.0166, Val Loss: 0.7760\n",
      "714/714 [==============================] - 241s 329ms/step - loss: 0.0166 - accuracy: 0.9941 - val_loss: 0.7760 - val_accuracy: 0.8959\n",
      "\n",
      "Starting Epoch 13...\n",
      "Epoch 13/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9954Completed Epoch 13, Loss: 0.0129, Val Loss: 0.1317\n",
      "714/714 [==============================] - 240s 329ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.1317 - val_accuracy: 0.9552\n",
      "\n",
      "Starting Epoch 14...\n",
      "Epoch 14/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9948Completed Epoch 14, Loss: 0.0138, Val Loss: 0.1778\n",
      "714/714 [==============================] - 240s 328ms/step - loss: 0.0138 - accuracy: 0.9948 - val_loss: 0.1778 - val_accuracy: 0.9390\n",
      "\n",
      "Starting Epoch 15...\n",
      "Epoch 15/20\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9949Completed Epoch 15, Loss: 0.0158, Val Loss: 2.4391\n",
      "714/714 [==============================] - 240s 328ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 2.4391 - val_accuracy: 0.7642\n",
      "\n",
      "Starting Epoch 16...\n",
      "Epoch 16/20\n",
      " 37/714 [>.............................] - ETA: 3:35 - loss: 0.0039 - accuracy: 0.9992"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m epoch_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(\n\u001b[1;32m     20\u001b[0m     on_epoch_begin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Training with callbacks\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcustom_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory and create it if it doesn't exist\n",
    "checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint filepath for the finetuned model\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'finetuned_50epochs_mobnetv3small_classif_checkpoint.keras')\n",
    "\n",
    "# Use the existing CustomModelCheckpoint class (no need to redefine it)\n",
    "# Just create a new instance with the updated filepath\n",
    "custom_checkpoint = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define epoch callback \n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[custom_checkpoint, epoch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/finetuned_50epochs_mobnetv3small_classification_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_path = '/home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/finetuned_50epochs_mobnetv3small_classification_model.keras'\n",
    "model.save(model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the weights learned from fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O14u6175RLjA"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/weights/fintuning_training_mobnetv3small_classification_weights.weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCbo4VcLxLgQ"
   },
   "source": [
    "# 3) CLASSIFICATION SPEED Test-Set Predictions\n",
    "\n",
    "a) load in test data\n",
    "\n",
    "b) convert test images to numerical RGB feature maps\n",
    "\n",
    "c) generate predictions on the test set\n",
    "\n",
    "d) correctly format the predictions into a pandas dataframe\n",
    "\n",
    "e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnygDJsKxYhA"
   },
   "source": [
    "### 3a) load in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "W-e59lQQRXKK",
    "outputId": "aa8566ec-e472-47a6-c7a0-92266b567a62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              image_file_paths\n",
       "image_id                                                                                      \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data'\n",
    "# image_folder_path = '/home/ppyt13/machine-learning-in-science-ii-2025/test_data/test_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'\n",
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-9i5trTyDTf"
   },
   "source": [
    "### 3b) convert test images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hT_c1s5TAR-"
   },
   "outputs": [],
   "source": [
    "def process_image_no_label(image_path, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Use decode_png for PNG images\n",
    "    image = tf.image.resize(image, resized_shape)  # Resize to uniform shape\n",
    "    image = image / 255.0  # Normalize pixel values to [0,1]\n",
    "    return image\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((imagefilepaths_df[\"image_file_paths\"]))\n",
    "\n",
    "test_dataset = test_dataset.map(process_image_no_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gobnK7PhyLa2"
   },
   "source": [
    "### 3c) generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtqcOFr7TAXa",
    "outputId": "73b4c96b-51bf-4e1c-e1b6-e8cde1321984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT1LJxHTPeQT"
   },
   "source": [
    "### 3d) correctly format the predictions into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFVWGi04fza7"
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OnO0K1rReHOT",
    "outputId": "d9cebb2e-3d36-4c7a-b024-eabb646e3bbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.370914e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.999998e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.998439e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          speed\n",
       "0  1.370914e-25\n",
       "1  1.000000e+00\n",
       "2  9.999998e-01\n",
       "3  1.000000e+00\n",
       "4  9.998439e-01"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[predictions_df['speed'] > 0.5] = 1\n",
    "predictions_df[predictions_df['speed'] < 0.5] = 0\n",
    "\n",
    "predictions_df['speed'] = predictions_df['speed'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed\n",
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CcRKL9KTAfs",
    "outputId": "277533cd-06aa-4709-d44e-9027cc7e9438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1    516\n",
       "0    504\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['speed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU-PhskZPaHD"
   },
   "source": [
    "### 3e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deXjPTO0TAiL"
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv('/home/apyba3/mobnetv3small_speedclassification_withvalidation_withpetrudata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
