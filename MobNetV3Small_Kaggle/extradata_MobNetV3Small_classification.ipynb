{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fhwRSFoj6C_"
   },
   "source": [
    "# SWITCH TO **`T4 GPU`** OR THE **`HPC`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4V83PflfFkL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kP6UczzNe1l2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O24_U-m8q-xv",
    "outputId": "f2298893-2e7e-4b8f-cc38-0caeb1a6a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IF_vPVifaU9V"
   },
   "outputs": [],
   "source": [
    "# makes it so pd dfs aren't truncated\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eocC68amnhEI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_MvRvYnfIM5"
   },
   "source": [
    "# 1) DATA PRE-PROCESSING\n",
    "\n",
    "a) Load in kaggle data labels + image file paths\n",
    "\n",
    "b) combine kaggle data labels and image file paths into one dataframe\n",
    "\n",
    "c) load in the extra 486 image file paths\n",
    "\n",
    "d) extract the speed and angle labels from the file path names\n",
    "\n",
    "e) store that extra data in a pandas df and do the value normalisation\n",
    "\n",
    "f) merge the kaggle and extra data dfs\n",
    "\n",
    "g) EDA\n",
    "\n",
    "h) convert the images to numerical RGB feature maps\n",
    "\n",
    "i) split data into training-validation sets\n",
    "\n",
    "j) data augmentation applied to training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU3TvBZ5hfhX"
   },
   "source": [
    "### 1a) load in kaggle data labels + image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZiNf_BxOfEH-"
   },
   "outputs": [],
   "source": [
    "# labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path\n",
    "labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)\n",
    "# labels_file_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_norm.csv' # tyler hpc file path (mlis2 cluster)\n",
    "labels_df = pd.read_csv(labels_file_path, index_col='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nOXmN--gb-Q9"
   },
   "outputs": [],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data' # OG data ben hpc file path (mlis2 cluster)\n",
    "# image_folder_path = '/home/ppytr13/machine-learning-in-science-ii-2025//training_data/training_data'\n",
    "# image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oeuvmeZaGSC"
   },
   "source": [
    "Checking labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pi13TZ2aFhO",
    "outputId": "fc675bb2-271b-48fd-a6c3-43834afb4500"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed\n",
       "image_id               \n",
       "1         0.4375    0.0\n",
       "2         0.8125    1.0\n",
       "3         0.4375    1.0\n",
       "4         0.6250    1.0\n",
       "5         0.5000    0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puEjGoOJaRS4"
   },
   "source": [
    "Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1suFSK7aWKH",
    "outputId": "c3cc2d29-d759-48ff-b92c-77dbd178f295"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      image_file_paths\n",
       "image_id                                                                                              \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjDdyYd6cMBE"
   },
   "source": [
    "### 1b) Combine the kaggle labels and image file paths into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6NdbonzPcLKB"
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')\n",
    "kaggle_df['speed'] = kaggle_df['speed'].round(6) # to get rid of floating point errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VstirIAdAZi",
    "outputId": "c03ff707-9e8d-4c3a-8965-f795919ace21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13796</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13794     0.6250    1.0   \n",
       "13795     0.4375    1.0   \n",
       "13796     0.5625    0.0   \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13794     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png  \n",
       "13795     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png  \n",
       "13796     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png  \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MgNoL8nfBm2",
    "outputId": "924e7562-25a4-4223-8305-c3fd02452846"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          angle  speed  \\\n",
       "image_id                 \n",
       "3139      0.750    1.0   \n",
       "3140      0.875    1.0   \n",
       "3142      0.625    0.0   \n",
       "3143      0.625    1.0   \n",
       "\n",
       "                                                                                         image_file_paths  \n",
       "image_id                                                                                                   \n",
       "3139      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png  \n",
       "3140      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png  \n",
       "3142      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png  \n",
       "3143      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.loc[3139:3143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7PCxqJbmXE6"
   },
   "source": [
    "The above cell shows that:\n",
    "\n",
    " 1) the image files and labels match (see image_id and the number at the end of the file path)\n",
    "\n",
    " 2) the missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOEWqBUYX6DL"
   },
   "source": [
    "### 1c) load in the extra 486 labels image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wvsDiCCLOvvs"
   },
   "outputs": [],
   "source": [
    "extradata_folder_path = '/home/apyba3/petru_data'\n",
    "\n",
    "extradata_file_paths = [\n",
    "    os.path.join(extradata_folder_path, f)\n",
    "    for f in os.listdir(extradata_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ofcGILO4et"
   },
   "source": [
    "### 1d) extract the speed and angle labels from the file path names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFsEI4MBRf2l"
   },
   "source": [
    "image file path name follows the pattern: `randomnumber_angle_speed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mY5-HDp-PJY9"
   },
   "outputs": [],
   "source": [
    "# Regex pattern to extract angle and speed values\n",
    "pattern = r'(\\d+)_([\\d]+)_([\\d]+)\\.png'\n",
    "\n",
    "angle_value = []\n",
    "speed_value = []\n",
    "\n",
    "# Loop through file paths and extract angle and speed values\n",
    "for file_path in extradata_file_paths:\n",
    "    match = re.search(pattern, file_path)\n",
    "    if match:\n",
    "        # Extract random number, angle, and speed values\n",
    "        random_number = match.group(1)\n",
    "        angle_value.append(int(match.group(2)))\n",
    "        speed_value.append(int(match.group(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8qIQJ8Y3t8"
   },
   "source": [
    "checking it has stored the labels correctly (check if the angle_value order matches that of the file path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf1bChw_OvsT",
    "outputId": "bdf648d9-3ab3-403e-c977-0c938ae1bf18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 100, 80]\n",
      "['/home/apyba3/petru_data/1712918428740_95_0.png', '/home/apyba3/petru_data/1712923220525_100_50.png', '/home/apyba3/petru_data/1712923068961_80_35.png']\n"
     ]
    }
   ],
   "source": [
    "print(angle_value[:3])\n",
    "print(extradata_file_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyvljUTBZP0E"
   },
   "source": [
    "### 1e) store that extra data in a pandas df and do the value normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tse95lu1OvnY",
    "outputId": "90ed60a7-5f9c-4901-f7ed-7442d739ccfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13801</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923068961_80_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13802</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712921566265_105_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13803</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712915924250_70_35.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed                                  image_file_paths\n",
       "image_id                                                                 \n",
       "13799     0.5625      0    /home/apyba3/petru_data/1712918428740_95_0.png\n",
       "13800     0.6250      1  /home/apyba3/petru_data/1712923220525_100_50.png\n",
       "13801     0.3750      1   /home/apyba3/petru_data/1712923068961_80_35.png\n",
       "13802     0.6875      0   /home/apyba3/petru_data/1712921566265_105_0.png\n",
       "13803     0.2500      1   /home/apyba3/petru_data/1712915924250_70_35.png"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extradata_df = pd.DataFrame({\n",
    "    'angle': angle_value,\n",
    "    'speed': speed_value,\n",
    "    'image_file_paths': extradata_file_paths\n",
    "})\n",
    "\n",
    "# conversions (see kaggle data section)\n",
    "extradata_df.loc[extradata_df['speed'] > 0, 'speed'] = 1\n",
    "extradata_df['speed'] = pd.to_numeric(extradata_df['speed'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "extradata_df['angle'] = (extradata_df['angle'] - 50)/80\n",
    "\n",
    "extradata_df.index = pd.RangeIndex(start=13799, stop=13799 + len(extradata_df), step=1)\n",
    "extradata_df.index.name = 'image_id'\n",
    "\n",
    "extradata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv0MwDKsbOef"
   },
   "source": [
    "### 1f) merge the kaggle and extra data dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZMZPUn4b3Kc",
    "outputId": "86bd34db-0b48-442e-b5b2-5ff322d0764b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "13799     0.5625    0.0   \n",
       "13800     0.6250    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  \n",
       "13799                                                       /home/apyba3/petru_data/1712918428740_95_0.png  \n",
       "13800                                                     /home/apyba3/petru_data/1712923220525_100_50.png  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([kaggle_df, extradata_df])\n",
    "merged_df.loc[13797:13800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3OKLcn9u0Pz"
   },
   "source": [
    "### 1g) EDA - speed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWQCQrR-oCps",
    "outputId": "88bb4558-2c8a-482b-de5d-8f7876ed9bc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.000000    10840\n",
       "0.000000     3438\n",
       "1.428571        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4pZ65pYvdqb"
   },
   "source": [
    "note: imbalance datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMZq41-RkLz0"
   },
   "source": [
    "we want to remove the row containing the erroneous 1.428571 speed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TDMqIiOLSKGX"
   },
   "outputs": [],
   "source": [
    "cleaned_df = merged_df[merged_df['speed'] != 1.428571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.0    10840\n",
       "0.0     3438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di6F6km_DBmj"
   },
   "source": [
    "### 1h) convert images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oeeBTruNCQ96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:42:54.050581: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, label, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, resized_shape)\n",
    "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((cleaned_df[\"image_file_paths\"], cleaned_df[\"speed\"])) # Convert pd df into a tf ds\n",
    "\n",
    "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(len(cleaned_df))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOlsWQeVlyC"
   },
   "source": [
    "lets check and see if what we have done works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBTNjNhMVk2g",
    "outputId": "b00f1443-c179-43a2-e6fd-7cc90ff698f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:43:04.434627: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 4008 of 14278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:43:08.821484: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md6U_i84SiK5"
   },
   "source": [
    "### 1i) Splitting data into training and validation sets (test set is already provided in kaggle data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yYlssPh5dxaO"
   },
   "outputs": [],
   "source": [
    "# 80-20 split\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPUE6rd8cgQN",
    "outputId": "a418b177-e08d-481c-d272-b9b7494882d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 357, validation size: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_size}, validation size: {dataset_size - train_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ujsjhMPSw4f"
   },
   "source": [
    "### 1j) Data Augmentation applied to training set\n",
    "\n",
    "- Random Brightness Adjustment\n",
    "- Random Contrast Adjustment\n",
    "- Random Hue Adjustment\n",
    "- Random Saturation Adjustment\n",
    "- Random Horizontal Flip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "T9r811eWsYfe"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "  seed = (6, 9)\n",
    "  image = tf.image.stateless_random_brightness(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_contrast(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_saturation(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "  return image, label\n",
    "\n",
    "# Create a dataset of augmented images from the original train_dataset\n",
    "augmented_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "train_dataset = train_dataset.concatenate(augmented_dataset)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0     5493\n",
      "1.0    17355\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "angle_list = []\n",
    "\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    angle_list.extend(label_batch.numpy())  # add all 32 values from the batch\n",
    "\n",
    "angle_distribution = pd.Series(angle_list).value_counts().sort_index()\n",
    "print(angle_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOqizFg7rvKq"
   },
   "source": [
    "count how many images are in the training set - 22016 with no extradata and 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjlyfjAxLsrC",
    "outputId": "14dc79ee-e1b4-4c37-bfb1-b6525bc586c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train_dataset: 22848\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "for image_batch, _ in train_dataset:\n",
    "    total_images += image_batch.shape[0]  # Add the batch size\n",
    "\n",
    "print(f\"Total number of images in train_dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEdi-dUCTND1"
   },
   "source": [
    "checking to see if whats been done was successful or needs debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeboVhsQKGFS",
    "outputId": "b9c6bb08-d7ce-4951-b621-6775a6ee3bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1,10)\n",
    "\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset.take(1):  # Take one batch\n",
    "    for image in image_batch:  # Iterate through images in the batch\n",
    "        if i < 10:  # Only display the first 5 images\n",
    "            print('image shape: ', np.shape(image))\n",
    "            tf.print('label:', label_batch[i])  # Print label for the corresponding image\n",
    "            axarr[i].imshow(image)\n",
    "            axarr[i].axis('off')\n",
    "            i += 1\n",
    "        else:\n",
    "            break  # Stop after displaying 5 images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmetmzNHTWzU"
   },
   "source": [
    "# 2) CLASSIFICATION SPEED Model Building - MobNetV3Small Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48RHLVshdX5L"
   },
   "source": [
    "### 2a) Set up model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aHjqXG1jSnCr"
   },
   "outputs": [],
   "source": [
    "dropoutrate = 0.2\n",
    "input_shape = (224,224,3)\n",
    "num_classes = 1 # we're only predicting the prob of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ZPso3wBuN9L3",
    "outputId": "ef11a9ff-7117-4836-dd78-9bcadac15995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 576)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               147712    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130097 (4.31 MB)\n",
      "Trainable params: 190977 (746.00 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mobnetv3small.trainable = False  # freeze mobnetv3small layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Ca0JFQuuN8oI"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) train the model with the mobnetv3small layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WPTNOtr7WjLS",
    "outputId": "63cf5fac-6100-43db-dadd-47da686c9712",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714/714 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7587 - auc: 0.5304\n",
      "Epoch 1: val_loss improved from inf to 0.54628, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 1, Loss: 0.5554, Val Loss: 0.5463\n",
      "714/714 [==============================] - 73s 89ms/step - loss: 0.5554 - accuracy: 0.7587 - auc: 0.5304 - val_loss: 0.5463 - val_accuracy: 0.7547 - val_auc: 0.7527\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apyba3/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714/714 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7592 - auc: 0.5995\n",
      "Epoch 2: val_loss improved from 0.54628 to 0.54185, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 2, Loss: 0.5409, Val Loss: 0.5418\n",
      "714/714 [==============================] - 70s 88ms/step - loss: 0.5409 - accuracy: 0.7592 - auc: 0.5995 - val_loss: 0.5418 - val_accuracy: 0.7607 - val_auc: 0.7913\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.7604 - auc: 0.6598\n",
      "Epoch 3: val_loss improved from 0.54185 to 0.50551, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 3, Loss: 0.5232, Val Loss: 0.5055\n",
      "714/714 [==============================] - 67s 86ms/step - loss: 0.5232 - accuracy: 0.7604 - auc: 0.6598 - val_loss: 0.5055 - val_accuracy: 0.7614 - val_auc: 0.8287\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.7644 - auc: 0.6895\n",
      "Epoch 4: val_loss improved from 0.50551 to 0.49109, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 4, Loss: 0.5103, Val Loss: 0.4911\n",
      "714/714 [==============================] - 67s 85ms/step - loss: 0.5103 - accuracy: 0.7644 - auc: 0.6895 - val_loss: 0.4911 - val_accuracy: 0.7593 - val_auc: 0.8331\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.7637 - auc: 0.6974\n",
      "Epoch 5: val_loss improved from 0.49109 to 0.48171, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 5, Loss: 0.5078, Val Loss: 0.4817\n",
      "714/714 [==============================] - 67s 86ms/step - loss: 0.5078 - accuracy: 0.7637 - auc: 0.6974 - val_loss: 0.4817 - val_accuracy: 0.7624 - val_auc: 0.8459\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory and create it if it doesn't exist\n",
    "checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint filepath with h5 extension (more compatible with TF 2.13)\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'mobnetv3smallcheckpoint.h5')\n",
    "\n",
    "# Create ModelCheckpoint with compatible parameters for TF 2.13\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    "    # No save_weights_only parameter\n",
    ")\n",
    "\n",
    "# Define epoch callback\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/models/mobnetv3small_classification_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_path = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/models/mobnetv3small_classification_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiHy6opSP2sQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/frozentraining_classification_mobnetv3small.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpLHyw20P93U"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #Clear keras session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENHbUvQdvyFe"
   },
   "source": [
    "### 2d) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ek_ytyw0KB"
   },
   "source": [
    "rebuild model after clearing keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> \n",
       "\n",
       " global_average_pooling2d         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)             \u001b[38;5;34m939,120\u001b[0m \n",
       "\n",
       " global_average_pooling2d         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                         \u001b[38;5;34m577\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,697</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m939,697\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">927,585</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m927,585\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,112</span> (47.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,112\u001b[0m (47.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mobnetv3small.trainable = True  # UNfreeze mobnetv3small layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # deliberately smaller learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load in the learned weights from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oAenzEiP-C-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apyba3/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/frozentraining_classification_mobnetv3small.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDtRxBow89t"
   },
   "source": [
    "Initiate fine-tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.8811 - auc: 0.8638 - loss: 1.4939  \n",
      "Epoch 1: val_loss improved from inf to 4.26729, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 1, Loss: 0.6943, Val Loss: 4.2673\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 435ms/step - accuracy: 0.8811 - auc: 0.8639 - loss: 1.4928 - val_accuracy: 0.2455 - val_auc: 0.5288 - val_loss: 4.2673\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9601 - auc: 0.9798 - loss: 0.1693  \n",
      "Epoch 2: val_loss improved from 4.26729 to 2.47814, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 2, Loss: 0.1417, Val Loss: 2.4781\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9601 - auc: 0.9798 - loss: 0.1692 - val_accuracy: 0.2459 - val_auc: 0.5861 - val_loss: 2.4781\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9696 - auc: 0.9926 - loss: 0.0851  \n",
      "Epoch 3: val_loss did not improve from 2.47814\n",
      "Completed Epoch 3, Loss: 0.0857, Val Loss: 3.0127\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9696 - auc: 0.9926 - loss: 0.0851 - val_accuracy: 0.7562 - val_auc: 0.5526 - val_loss: 3.0127\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9799 - auc: 0.9965 - loss: 0.0550  \n",
      "Epoch 4: val_loss improved from 2.47814 to 0.80931, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 4, Loss: 0.0566, Val Loss: 0.8093\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9799 - auc: 0.9965 - loss: 0.0550 - val_accuracy: 0.7510 - val_auc: 0.6593 - val_loss: 0.8093\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9869 - auc: 0.9976 - loss: 0.0395  \n",
      "Epoch 5: val_loss did not improve from 0.80931\n",
      "Completed Epoch 5, Loss: 0.0406, Val Loss: 2.4576\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9869 - auc: 0.9976 - loss: 0.0395 - val_accuracy: 0.5366 - val_auc: 0.7188 - val_loss: 2.4576\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9866 - auc: 0.9972 - loss: 0.0369  \n",
      "Epoch 6: val_loss did not improve from 0.80931\n",
      "Completed Epoch 6, Loss: 0.0388, Val Loss: 7.5126\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9866 - auc: 0.9972 - loss: 0.0369 - val_accuracy: 0.7618 - val_auc: 0.5070 - val_loss: 7.5126\n",
      "\n",
      "Starting Epoch 7...\n",
      "Epoch 7/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9895 - auc: 0.9987 - loss: 0.0267  \n",
      "Epoch 7: val_loss did not improve from 0.80931\n",
      "Completed Epoch 7, Loss: 0.0296, Val Loss: 1.9602\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 430ms/step - accuracy: 0.9895 - auc: 0.9987 - loss: 0.0267 - val_accuracy: 0.7909 - val_auc: 0.7166 - val_loss: 1.9602\n",
      "\n",
      "Starting Epoch 8...\n",
      "Epoch 8/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9909 - auc: 0.9986 - loss: 0.0257  \n",
      "Epoch 8: val_loss improved from 0.80931 to 0.59846, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 8, Loss: 0.0260, Val Loss: 0.5985\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9909 - auc: 0.9986 - loss: 0.0257 - val_accuracy: 0.8722 - val_auc: 0.9108 - val_loss: 0.5985\n",
      "\n",
      "Starting Epoch 9...\n",
      "Epoch 9/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9937 - auc: 0.9992 - loss: 0.0182  \n",
      "Epoch 9: val_loss improved from 0.59846 to 0.40243, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 9, Loss: 0.0219, Val Loss: 0.4024\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9937 - auc: 0.9992 - loss: 0.0182 - val_accuracy: 0.9306 - val_auc: 0.9315 - val_loss: 0.4024\n",
      "\n",
      "Starting Epoch 10...\n",
      "Epoch 10/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9925 - auc: 0.9988 - loss: 0.0220  \n",
      "Epoch 10: val_loss did not improve from 0.40243\n",
      "Completed Epoch 10, Loss: 0.0233, Val Loss: 1.2107\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9925 - auc: 0.9988 - loss: 0.0220 - val_accuracy: 0.7191 - val_auc: 0.7763 - val_loss: 1.2107\n",
      "\n",
      "Starting Epoch 11...\n",
      "Epoch 11/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0162  \n",
      "Epoch 11: val_loss did not improve from 0.40243\n",
      "Completed Epoch 11, Loss: 0.0177, Val Loss: 1.6019\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0162 - val_accuracy: 0.8294 - val_auc: 0.7638 - val_loss: 1.6019\n",
      "\n",
      "Starting Epoch 12...\n",
      "Epoch 12/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9943 - auc: 0.9993 - loss: 0.0149  \n",
      "Epoch 12: val_loss did not improve from 0.40243\n",
      "Completed Epoch 12, Loss: 0.0131, Val Loss: 0.5371\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9943 - auc: 0.9993 - loss: 0.0149 - val_accuracy: 0.9282 - val_auc: 0.9276 - val_loss: 0.5371\n",
      "\n",
      "Starting Epoch 13...\n",
      "Epoch 13/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9945 - auc: 0.9990 - loss: 0.0164  \n",
      "Epoch 13: val_loss did not improve from 0.40243\n",
      "Completed Epoch 13, Loss: 0.0171, Val Loss: 1.5137\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9945 - auc: 0.9990 - loss: 0.0164 - val_accuracy: 0.7138 - val_auc: 0.5557 - val_loss: 1.5137\n",
      "\n",
      "Starting Epoch 14...\n",
      "Epoch 14/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9953 - auc: 0.9994 - loss: 0.0127  \n",
      "Epoch 14: val_loss did not improve from 0.40243\n",
      "Completed Epoch 14, Loss: 0.0134, Val Loss: 1.2607\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9953 - auc: 0.9994 - loss: 0.0127 - val_accuracy: 0.8802 - val_auc: 0.8340 - val_loss: 1.2607\n",
      "\n",
      "Starting Epoch 15...\n",
      "Epoch 15/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9959 - auc: 0.9992 - loss: 0.0102  \n",
      "Epoch 15: val_loss did not improve from 0.40243\n",
      "Completed Epoch 15, Loss: 0.0120, Val Loss: 0.8182\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9959 - auc: 0.9992 - loss: 0.0102 - val_accuracy: 0.7968 - val_auc: 0.7786 - val_loss: 0.8182\n",
      "\n",
      "Starting Epoch 16...\n",
      "Epoch 16/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9951 - auc: 0.9987 - loss: 0.0150  \n",
      "Epoch 16: val_loss did not improve from 0.40243\n",
      "Completed Epoch 16, Loss: 0.0136, Val Loss: 0.6490\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9951 - auc: 0.9987 - loss: 0.0150 - val_accuracy: 0.8900 - val_auc: 0.8970 - val_loss: 0.6490\n",
      "\n",
      "Starting Epoch 17...\n",
      "Epoch 17/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9956 - auc: 0.9993 - loss: 0.0091       \n",
      "Epoch 17: val_loss improved from 0.40243 to 0.38619, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 17, Loss: 0.0103, Val Loss: 0.3862\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9956 - auc: 0.9993 - loss: 0.0091 - val_accuracy: 0.9229 - val_auc: 0.9459 - val_loss: 0.3862\n",
      "\n",
      "Starting Epoch 18...\n",
      "Epoch 18/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9976 - auc: 0.9996 - loss: 0.0066      \n",
      "Epoch 18: val_loss did not improve from 0.38619\n",
      "Completed Epoch 18, Loss: 0.0090, Val Loss: 0.5164\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9976 - auc: 0.9996 - loss: 0.0067 - val_accuracy: 0.9275 - val_auc: 0.9251 - val_loss: 0.5164\n",
      "\n",
      "Starting Epoch 19...\n",
      "Epoch 19/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0081  \n",
      "Epoch 19: val_loss did not improve from 0.38619\n",
      "Completed Epoch 19, Loss: 0.0094, Val Loss: 40.4116\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0081 - val_accuracy: 0.2378 - val_auc: 0.4755 - val_loss: 40.4116\n",
      "\n",
      "Starting Epoch 20...\n",
      "Epoch 20/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0048  \n",
      "Epoch 20: val_loss did not improve from 0.38619\n",
      "Completed Epoch 20, Loss: -0.0018, Val Loss: 7.1915\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0048 - val_accuracy: 0.2294 - val_auc: 0.5123 - val_loss: 7.1915\n",
      "\n",
      "Starting Epoch 21...\n",
      "Epoch 21/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0097  \n",
      "Epoch 21: val_loss did not improve from 0.38619\n",
      "Completed Epoch 21, Loss: 0.0082, Val Loss: 0.5847\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0097 - val_accuracy: 0.9054 - val_auc: 0.9022 - val_loss: 0.5847\n",
      "\n",
      "Starting Epoch 22...\n",
      "Epoch 22/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9952 - auc: 0.9989 - loss: 0.0247       \n",
      "Epoch 22: val_loss did not improve from 0.38619\n",
      "Completed Epoch 22, Loss: 0.0343, Val Loss: 2.1453\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9952 - auc: 0.9989 - loss: 0.0247 - val_accuracy: 0.8018 - val_auc: 0.7095 - val_loss: 2.1453\n",
      "\n",
      "Starting Epoch 23...\n",
      "Epoch 23/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065  \n",
      "Epoch 23: val_loss improved from 0.38619 to 0.13223, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 23, Loss: 0.0068, Val Loss: 0.1322\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065 - val_accuracy: 0.9713 - val_auc: 0.9789 - val_loss: 0.1322\n",
      "\n",
      "Starting Epoch 24...\n",
      "Epoch 24/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9979 - auc: 0.9996 - loss: 0.0044      \n",
      "Epoch 24: val_loss did not improve from 0.13223\n",
      "Completed Epoch 24, Loss: 0.0053, Val Loss: 0.1820\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9979 - auc: 0.9996 - loss: 0.0044 - val_accuracy: 0.9643 - val_auc: 0.9754 - val_loss: 0.1820\n",
      "\n",
      "Starting Epoch 25...\n",
      "Epoch 25/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0025  \n",
      "Epoch 25: val_loss improved from 0.13223 to 0.04328, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 25, Loss: 0.0009, Val Loss: 0.0433\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0025 - val_accuracy: 0.9874 - val_auc: 0.9954 - val_loss: 0.0433\n",
      "\n",
      "Starting Epoch 26...\n",
      "Epoch 26/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9969 - auc: 0.9997 - loss: 0.0035  \n",
      "Epoch 26: val_loss did not improve from 0.04328\n",
      "Completed Epoch 26, Loss: 0.0069, Val Loss: 0.8471\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9969 - auc: 0.9997 - loss: 0.0035 - val_accuracy: 0.8588 - val_auc: 0.8604 - val_loss: 0.8471\n",
      "\n",
      "Starting Epoch 27...\n",
      "Epoch 27/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: -0.0016     \n",
      "Epoch 27: val_loss did not improve from 0.04328\n",
      "Completed Epoch 27, Loss: -0.0265, Val Loss: 0.2404\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: -0.0017 - val_accuracy: 0.9363 - val_auc: 0.9655 - val_loss: 0.2404\n",
      "\n",
      "Starting Epoch 28...\n",
      "Epoch 28/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9964 - auc: 0.9998 - loss: -0.0059     \n",
      "Epoch 28: val_loss did not improve from 0.04328\n",
      "Completed Epoch 28, Loss: -0.0834, Val Loss: 0.9364\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9964 - auc: 0.9998 - loss: -0.0060 - val_accuracy: 0.8109 - val_auc: 0.8263 - val_loss: 0.9364\n",
      "\n",
      "Starting Epoch 29...\n",
      "Epoch 29/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9906 - auc: 0.9977 - loss: -0.3116  \n",
      "Epoch 29: val_loss did not improve from 0.04328\n",
      "Completed Epoch 29, Loss: -0.0647, Val Loss: 0.2320\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9906 - auc: 0.9977 - loss: -0.3112 - val_accuracy: 0.9762 - val_auc: 0.9795 - val_loss: 0.2320\n",
      "\n",
      "Starting Epoch 30...\n",
      "Epoch 30/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9980 - auc: 0.9997 - loss: -0.0182      \n",
      "Epoch 30: val_loss did not improve from 0.04328\n",
      "Completed Epoch 30, Loss: -0.0500, Val Loss: 1.1723\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9980 - auc: 0.9997 - loss: -0.0182 - val_accuracy: 0.6953 - val_auc: 0.7438 - val_loss: 1.1723\n",
      "\n",
      "Starting Epoch 31...\n",
      "Epoch 31/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9957 - auc: 0.9989 - loss: -0.1872      \n",
      "Epoch 31: val_loss did not improve from 0.04328\n",
      "Completed Epoch 31, Loss: -0.1766, Val Loss: 1.9273\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9957 - auc: 0.9989 - loss: -0.1872 - val_accuracy: 0.8452 - val_auc: 0.7814 - val_loss: 1.9273\n",
      "\n",
      "Starting Epoch 32...\n",
      "Epoch 32/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: -0.0068     \n",
      "Epoch 32: val_loss did not improve from 0.04328\n",
      "Completed Epoch 32, Loss: -0.1089, Val Loss: 0.0945\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: -0.0070 - val_accuracy: 0.9769 - val_auc: 0.9892 - val_loss: 0.0945\n",
      "\n",
      "Starting Epoch 33...\n",
      "Epoch 33/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: -1.3343e-04\n",
      "Epoch 33: val_loss did not improve from 0.04328\n",
      "Completed Epoch 33, Loss: -0.2302, Val Loss: 2.1854\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: -4.5514e-04 - val_accuracy: 0.8946 - val_auc: 0.8579 - val_loss: 2.1854\n",
      "\n",
      "Starting Epoch 34...\n",
      "Epoch 34/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9953 - auc: 0.9993 - loss: -0.1793      \n",
      "Epoch 34: val_loss did not improve from 0.04328\n",
      "Completed Epoch 34, Loss: -0.2358, Val Loss: 3.8558\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9953 - auc: 0.9993 - loss: -0.1794 - val_accuracy: 0.8914 - val_auc: 0.8258 - val_loss: 3.8558\n",
      "\n",
      "Starting Epoch 35...\n",
      "Epoch 35/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9967 - auc: 0.9996 - loss: -0.5885  \n",
      "Epoch 35: val_loss did not improve from 0.04328\n",
      "Completed Epoch 35, Loss: -0.1810, Val Loss: 0.3616\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9967 - auc: 0.9996 - loss: -0.5880 - val_accuracy: 0.9268 - val_auc: 0.9490 - val_loss: 0.3616\n",
      "\n",
      "Starting Epoch 36...\n",
      "Epoch 36/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9959 - auc: 0.9996 - loss: -0.0151     \n",
      "Epoch 36: val_loss did not improve from 0.04328\n",
      "Completed Epoch 36, Loss: -0.0689, Val Loss: 0.2879\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9959 - auc: 0.9996 - loss: -0.0152 - val_accuracy: 0.9331 - val_auc: 0.9594 - val_loss: 0.2879\n",
      "\n",
      "Starting Epoch 37...\n",
      "Epoch 37/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: -0.1316      \n",
      "Epoch 37: val_loss did not improve from 0.04328\n",
      "Completed Epoch 37, Loss: -0.3001, Val Loss: 0.1031\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: -0.1318 - val_accuracy: 0.9797 - val_auc: 0.9905 - val_loss: 0.1031\n",
      "\n",
      "Starting Epoch 38...\n",
      "Epoch 38/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9974 - auc: 0.9996 - loss: -0.4719  \n",
      "Epoch 38: val_loss did not improve from 0.04328\n",
      "Completed Epoch 38, Loss: -0.3166, Val Loss: 0.0737\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9974 - auc: 0.9996 - loss: -0.4717 - val_accuracy: 0.9877 - val_auc: 0.9940 - val_loss: 0.0737\n",
      "\n",
      "Starting Epoch 39...\n",
      "Epoch 39/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: -0.0460      \n",
      "Epoch 39: val_loss improved from 0.04328 to 0.00723, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 39, Loss: -0.1596, Val Loss: 0.0072\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 437ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: -0.0461 - val_accuracy: 0.9982 - val_auc: 1.0000 - val_loss: 0.0072\n",
      "\n",
      "Starting Epoch 40...\n",
      "Epoch 40/50\n",
      "\u001b[1m145/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:02\u001b[0m 426ms/step - accuracy: 0.9963 - auc: 0.9995 - loss: -0.5429"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m epoch_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(\n\u001b[1;32m     14\u001b[0m     on_epoch_begin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Training loop with added callback\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include both callbacks\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define ModelCheckpoint callback\n",
    "checkpoint_filepath = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.keras'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define a callback to print epoch tracking info\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training loop with added callback\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]  # Include both callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the weights learned from fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O14u6175RLjA"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/fintuning_training_mobnetv3small_classification_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCbo4VcLxLgQ"
   },
   "source": [
    "# 3) CLASSIFICATION SPEED Test-Set Predictions\n",
    "\n",
    "a) load in test data\n",
    "\n",
    "b) convert test images to numerical RGB feature maps\n",
    "\n",
    "c) generate predictions on the test set\n",
    "\n",
    "d) correctly format the predictions into a pandas dataframe\n",
    "\n",
    "e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnygDJsKxYhA"
   },
   "source": [
    "### 3a) load in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "W-e59lQQRXKK",
    "outputId": "aa8566ec-e472-47a6-c7a0-92266b567a62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              image_file_paths\n",
       "image_id                                                                                      \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data'\n",
    "# image_folder_path = '/home/ppyt13/machine-learning-in-science-ii-2025/test_data/test_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'\n",
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-9i5trTyDTf"
   },
   "source": [
    "### 3b) convert test images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3hT_c1s5TAR-"
   },
   "outputs": [],
   "source": [
    "def process_image_no_label(image_path, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Use decode_png for PNG images\n",
    "    image = tf.image.resize(image, resized_shape)  # Resize to uniform shape\n",
    "    image = image / 255.0  # Normalize pixel values to [0,1]\n",
    "    return image\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((imagefilepaths_df[\"image_file_paths\"]))\n",
    "\n",
    "test_dataset = test_dataset.map(process_image_no_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gobnK7PhyLa2"
   },
   "source": [
    "### 3c) generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtqcOFr7TAXa",
    "outputId": "73b4c96b-51bf-4e1c-e1b6-e8cde1321984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT1LJxHTPeQT"
   },
   "source": [
    "### 3d) correctly format the predictions into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pFVWGi04fza7"
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OnO0K1rReHOT",
    "outputId": "d9cebb2e-3d36-4c7a-b024-eabb646e3bbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.370914e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.999998e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.998439e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          speed\n",
       "0  1.370914e-25\n",
       "1  1.000000e+00\n",
       "2  9.999998e-01\n",
       "3  1.000000e+00\n",
       "4  9.998439e-01"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[predictions_df['speed'] > 0.5] = 1\n",
    "predictions_df[predictions_df['speed'] < 0.5] = 0\n",
    "\n",
    "predictions_df['speed'] = predictions_df['speed'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed\n",
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CcRKL9KTAfs",
    "outputId": "277533cd-06aa-4709-d44e-9027cc7e9438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1    516\n",
       "0    504\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['speed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU-PhskZPaHD"
   },
   "source": [
    "### 3e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "deXjPTO0TAiL"
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv('/home/apyba3/mobnetv3small_speedclassification_withvalidation_withpetrudata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
