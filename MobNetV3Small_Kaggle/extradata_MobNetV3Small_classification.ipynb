{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fhwRSFoj6C_"
   },
   "source": [
    "# SWITCH TO **`T4 GPU`** OR THE **`HPC`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4V83PflfFkL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kP6UczzNe1l2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O24_U-m8q-xv",
    "outputId": "f2298893-2e7e-4b8f-cc38-0caeb1a6a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IF_vPVifaU9V"
   },
   "outputs": [],
   "source": [
    "# makes it so pd dfs aren't truncated\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eocC68amnhEI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_MvRvYnfIM5"
   },
   "source": [
    "# 1) DATA PRE-PROCESSING\n",
    "\n",
    "a) Load in kaggle data labels + image file paths\n",
    "\n",
    "b) combine kaggle data labels and image file paths into one dataframe\n",
    "\n",
    "c) load in the extra 486 image file paths\n",
    "\n",
    "d) extract the speed and angle labels from the file path names\n",
    "\n",
    "e) store that extra data in a pandas df and do the value normalisation\n",
    "\n",
    "f) merge the kaggle and extra data dfs\n",
    "\n",
    "g) EDA\n",
    "\n",
    "h) convert the images to numerical RGB feature maps\n",
    "\n",
    "i) split data into training-validation sets\n",
    "\n",
    "j) data augmentation applied to training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU3TvBZ5hfhX"
   },
   "source": [
    "### 1a) load in kaggle data labels + image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZiNf_BxOfEH-"
   },
   "outputs": [],
   "source": [
    "# labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path\n",
    "labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)\n",
    "# labels_file_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_norm.csv' # tyler hpc file path (mlis2 cluster)\n",
    "labels_df = pd.read_csv(labels_file_path, index_col='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nOXmN--gb-Q9"
   },
   "outputs": [],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data' # OG data ben hpc file path (mlis2 cluster)\n",
    "# image_folder_path = '/home/ppytr13/machine-learning-in-science-ii-2025//training_data/training_data'\n",
    "# image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oeuvmeZaGSC"
   },
   "source": [
    "Checking labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pi13TZ2aFhO",
    "outputId": "fc675bb2-271b-48fd-a6c3-43834afb4500"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed\n",
       "image_id               \n",
       "1         0.4375    0.0\n",
       "2         0.8125    1.0\n",
       "3         0.4375    1.0\n",
       "4         0.6250    1.0\n",
       "5         0.5000    0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puEjGoOJaRS4"
   },
   "source": [
    "Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1suFSK7aWKH",
    "outputId": "c3cc2d29-d759-48ff-b92c-77dbd178f295"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      image_file_paths\n",
       "image_id                                                                                              \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjDdyYd6cMBE"
   },
   "source": [
    "### 1b) Combine the kaggle labels and image file paths into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6NdbonzPcLKB"
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')\n",
    "kaggle_df['speed'] = kaggle_df['speed'].round(6) # to get rid of floating point errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VstirIAdAZi",
    "outputId": "c03ff707-9e8d-4c3a-8965-f795919ace21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13796</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13794     0.6250    1.0   \n",
       "13795     0.4375    1.0   \n",
       "13796     0.5625    0.0   \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13794     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png  \n",
       "13795     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png  \n",
       "13796     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png  \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MgNoL8nfBm2",
    "outputId": "924e7562-25a4-4223-8305-c3fd02452846"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          angle  speed  \\\n",
       "image_id                 \n",
       "3139      0.750    1.0   \n",
       "3140      0.875    1.0   \n",
       "3142      0.625    0.0   \n",
       "3143      0.625    1.0   \n",
       "\n",
       "                                                                                         image_file_paths  \n",
       "image_id                                                                                                   \n",
       "3139      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png  \n",
       "3140      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png  \n",
       "3142      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png  \n",
       "3143      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.loc[3139:3143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7PCxqJbmXE6"
   },
   "source": [
    "The above cell shows that:\n",
    "\n",
    " 1) the image files and labels match (see image_id and the number at the end of the file path)\n",
    "\n",
    " 2) the missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOEWqBUYX6DL"
   },
   "source": [
    "### 1c) load in the extra 486 labels image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wvsDiCCLOvvs"
   },
   "outputs": [],
   "source": [
    "extradata_folder_path = '/home/apyba3/petru_data'\n",
    "\n",
    "extradata_file_paths = [\n",
    "    os.path.join(extradata_folder_path, f)\n",
    "    for f in os.listdir(extradata_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ofcGILO4et"
   },
   "source": [
    "### 1d) extract the speed and angle labels from the file path names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFsEI4MBRf2l"
   },
   "source": [
    "image file path name follows the pattern: `randomnumber_angle_speed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mY5-HDp-PJY9"
   },
   "outputs": [],
   "source": [
    "# Regex pattern to extract angle and speed values\n",
    "pattern = r'(\\d+)_([\\d]+)_([\\d]+)\\.png'\n",
    "\n",
    "angle_value = []\n",
    "speed_value = []\n",
    "\n",
    "# Loop through file paths and extract angle and speed values\n",
    "for file_path in extradata_file_paths:\n",
    "    match = re.search(pattern, file_path)\n",
    "    if match:\n",
    "        # Extract random number, angle, and speed values\n",
    "        random_number = match.group(1)\n",
    "        angle_value.append(int(match.group(2)))\n",
    "        speed_value.append(int(match.group(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8qIQJ8Y3t8"
   },
   "source": [
    "checking it has stored the labels correctly (check if the angle_value order matches that of the file path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf1bChw_OvsT",
    "outputId": "bdf648d9-3ab3-403e-c977-0c938ae1bf18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 100, 80]\n",
      "['/home/apyba3/petru_data/1712918428740_95_0.png', '/home/apyba3/petru_data/1712923220525_100_50.png', '/home/apyba3/petru_data/1712923068961_80_35.png']\n"
     ]
    }
   ],
   "source": [
    "print(angle_value[:3])\n",
    "print(extradata_file_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyvljUTBZP0E"
   },
   "source": [
    "### 1e) store that extra data in a pandas df and do the value normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tse95lu1OvnY",
    "outputId": "90ed60a7-5f9c-4901-f7ed-7442d739ccfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13801</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923068961_80_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13802</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712921566265_105_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13803</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712915924250_70_35.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed                                  image_file_paths\n",
       "image_id                                                                 \n",
       "13799     0.5625      0    /home/apyba3/petru_data/1712918428740_95_0.png\n",
       "13800     0.6250      1  /home/apyba3/petru_data/1712923220525_100_50.png\n",
       "13801     0.3750      1   /home/apyba3/petru_data/1712923068961_80_35.png\n",
       "13802     0.6875      0   /home/apyba3/petru_data/1712921566265_105_0.png\n",
       "13803     0.2500      1   /home/apyba3/petru_data/1712915924250_70_35.png"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extradata_df = pd.DataFrame({\n",
    "    'angle': angle_value,\n",
    "    'speed': speed_value,\n",
    "    'image_file_paths': extradata_file_paths\n",
    "})\n",
    "\n",
    "# conversions (see kaggle data section)\n",
    "extradata_df.loc[extradata_df['speed'] > 0, 'speed'] = 1\n",
    "extradata_df['speed'] = pd.to_numeric(extradata_df['speed'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "extradata_df['angle'] = (extradata_df['angle'] - 50)/80\n",
    "\n",
    "extradata_df.index = pd.RangeIndex(start=13799, stop=13799 + len(extradata_df), step=1)\n",
    "extradata_df.index.name = 'image_id'\n",
    "\n",
    "extradata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv0MwDKsbOef"
   },
   "source": [
    "### 1f) merge the kaggle and extra data dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZMZPUn4b3Kc",
    "outputId": "86bd34db-0b48-442e-b5b2-5ff322d0764b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "13799     0.5625    0.0   \n",
       "13800     0.6250    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  \n",
       "13799                                                       /home/apyba3/petru_data/1712918428740_95_0.png  \n",
       "13800                                                     /home/apyba3/petru_data/1712923220525_100_50.png  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([kaggle_df, extradata_df])\n",
    "merged_df.loc[13797:13800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3OKLcn9u0Pz"
   },
   "source": [
    "### 1g) EDA - speed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWQCQrR-oCps",
    "outputId": "88bb4558-2c8a-482b-de5d-8f7876ed9bc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.000000    10840\n",
       "0.000000     3438\n",
       "1.428571        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4pZ65pYvdqb"
   },
   "source": [
    "note: imbalance datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMZq41-RkLz0"
   },
   "source": [
    "we want to remove the row containing the erroneous 1.428571 speed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TDMqIiOLSKGX"
   },
   "outputs": [],
   "source": [
    "cleaned_df = merged_df[merged_df['speed'] != 1.428571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1.0    10840\n",
       "0.0     3438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.value_counts('speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di6F6km_DBmj"
   },
   "source": [
    "### 1h) convert images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oeeBTruNCQ96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 22:28:07.898719: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, label, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, resized_shape)\n",
    "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((cleaned_df[\"image_file_paths\"], cleaned_df[\"speed\"])) # Convert pd df into a tf ds\n",
    "\n",
    "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(len(cleaned_df))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOlsWQeVlyC"
   },
   "source": [
    "lets check and see if what we have done works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBTNjNhMVk2g",
    "outputId": "b00f1443-c179-43a2-e6fd-7cc90ff698f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md6U_i84SiK5"
   },
   "source": [
    "### 1i) Splitting data into training and validation sets (test set is already provided in kaggle data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yYlssPh5dxaO"
   },
   "outputs": [],
   "source": [
    "# 80-20 split\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPUE6rd8cgQN",
    "outputId": "a418b177-e08d-481c-d272-b9b7494882d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 357, validation size: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_size}, validation size: {dataset_size - train_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ujsjhMPSw4f"
   },
   "source": [
    "### 1j) Data Augmentation applied to training set\n",
    "\n",
    "- Random Brightness Adjustment\n",
    "- Random Contrast Adjustment\n",
    "- Random Hue Adjustment\n",
    "- Random Saturation Adjustment\n",
    "- Random Horizontal Flip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T9r811eWsYfe"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "  seed = (6, 9)\n",
    "  image = tf.image.stateless_random_brightness(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_contrast(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_saturation(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "  return image, label\n",
    "\n",
    "# Create a dataset of augmented images from the original train_dataset\n",
    "augmented_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "train_dataset = train_dataset.concatenate(augmented_dataset)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0     5536\n",
      "1.0    17312\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "angle_list = []\n",
    "\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    angle_list.extend(label_batch.numpy())  # add all 32 values from the batch\n",
    "\n",
    "angle_distribution = pd.Series(angle_list).value_counts().sort_index()\n",
    "print(angle_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOqizFg7rvKq"
   },
   "source": [
    "count how many images are in the training set - 22016 with no extradata and 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjlyfjAxLsrC",
    "outputId": "14dc79ee-e1b4-4c37-bfb1-b6525bc586c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train_dataset: 22848\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "for image_batch, _ in train_dataset:\n",
    "    total_images += image_batch.shape[0]  # Add the batch size\n",
    "\n",
    "print(f\"Total number of images in train_dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEdi-dUCTND1"
   },
   "source": [
    "checking to see if whats been done was successful or needs debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeboVhsQKGFS",
    "outputId": "b9c6bb08-d7ce-4951-b621-6775a6ee3bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 0\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n",
      "image shape:  (224, 224, 3)\n",
      "label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAA+CAYAAAC2oBgNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACJ30lEQVR4nOz9d5hlx3XeC/8q7L1P7Nw9OUfkQWYASZEgQIpZJEVRVhZFWfbV5ygr0LKkK9ny4+/6ypIsW/eTrUDlRMoKlEiRkpgAAiQBEnnyYIAJmJmemc4n7F1V3x+r9j5nAKJ7aPt57h+awoOZnu7T5+y9q2qtd73rXatUCCFwbVwb18a1cW1cG9fG39uh/9++gGvj2rg2ro1r49q4Nv7fHdfAwLVxbVwb18a1cW38PR/XwMC1cW1cG9fGtXFt/D0f18DAtXFtXBvXxrVxbfw9H9fAwLVxbVwb18a1cW38PR/XwMC1cW1cG9fGtXFt/D0f18DAtXFtXBvXxrVxbfw9H9fAwLVxbVwb18a1cW38PR/XwMC1cW1cG9fGtXFt/D0f9mpf+PZ3v4ef/pl/y5OPPc7/8Y//IUWRo1CMjo/ziz//S+zctZvOSoeV5RXOvXCWv/rkx/nEX/8lebeHMZpGlpAmmsKBVgrvAt57gg4EFL1uH4o+O8ZHmJoyPHR8Ae8CReHQRtNOE2o1xeWe5vp1M0xvHuOC3gzGoABlEpTWKBSf/ZNff9n7+K0//iIr5y9z8Kt/yqOPP8GpF87z5te/kfe999uxJpX3UApQoCB4z+KlM5w5/RSq6OC1ZmJqE2l9DK00QSmsMugkRZkMYzO0sRhlUNqgtJL31BofFB5Nv7NAoz2KQoN2ZGkLhSIHrLUoa0lsyv69zVXn5F//6w+we9dWiiLn3NnzPPzQl3nu+TN0uo5EK6xRBK1RxstEG41GYdDV/YFHqYBGEQCvIADGa0LwBAIheLwHHwAf8E5ReE/fO4oC8gLyomC52wcPWapxLhACjLczdmwd428eOPGy9/H7v/6f0QGChgRLUAGFQrmAsoagPSqAdx5rLOjAyvIC9aROktW5eOESTx85Tr2Z8ad/+Qlu3rOLkKYYnTE90eLTn32Iyclp/uX3fxeoQJHnLC5dYnR0hIBcewgeozUhaDAalxd0u33SZo0AzC/MsmP7dbzmrd+y6pzs2byRdWNjWLXCsxfm2L9lM+MjNerrt7N+/VZGRluMjk4wvWE9xib44JmemsYkKSvzC1y6eIaxkQyP4nd/9w949vhZxsfH+Sf/9AfZd/0+pmemSbI2WmuU0gSAsoloCDx7+CA/+2//Dbff8WqeO/oUl8+f5/jzJzGFJlGWs7On+N4f+TFmptbx7ve8/2Xv46f//Y9gtcYoi9IajMJojVUWpQ1GG7TSoMGHwMXLPS5eWObc+VnmLp/GByfrOwRQGgJ4AkEFTFxtIQAenAIVAsE5nC/weU7wHu89zjvwUIQ+eFmjAeLalPX62KNfXXVOPvCP/hEvnD3D3n27WT89iTUWpUDF6zdovFJoPAojzxTPl778KJ/5/Jc589xxtm3bzFu/8X7SLCOxRm5Jw/kLFzHKsnnjeqwWG/TYE4/z1DNHOHPyOeYXFkDJpgohYI2m2W4xOj7Jtu1baKYNvvDwQ3Q7K4yPjnD67LmXvY9NUyPU6w1+5N/+HOOjE/zkv/pBdt+whz033MCJwyf4209/huA97dFR+v0+K8tLWGuJN0Sr2WZicoyTJ09SFA4QE+AJBC8v0towNTXJ6dOnMUkCQL/bQynYvnUPp88+hw8OYwyFKxgbH2d5aQmtFTs2T6J0SpI1eOSLX1l1Tr7je76TVqvNzPr11OpNlhYXGWmNMDI6Asrg8j69lS6XL54ltYHJqSm6eY+PfuTPuHhpnstz84yNtHn/e97LvhtuZu++XYy2G4xPrqNWa1Cv18R2+YD3Dm0MxhgW5hfp531CgP/+//uvHDv6DAtzc/iVDieOHSSvN1k/s47Cw/qNm3n64Yd5/PjJl72Pm2/eyeLcAtftGiUPni88eorCiW1VCpQHrxRKEZ+3AhUgyDeUB2UMlcuJ/sdoI/7Hyb4IWgEBZTRpkvCv/+n302pp5i7P8qVHnmK05hmfWU/A8iu/+ZfiN7XCOQdKobRicX5l1Tm5ajAwOT4ejeICPgTZSEBiLMoYzr1wilZrjH7e4/Spk3zpoQfxnS4bg+cCHuUNzTRhpefxAQonGz04j9YaHTxaxaVZaNLEspz3sNYSCCTGQPAopeT3C0+w0TQohVJOJl+tTnbk/R5F3sG7gsLlOO9IkhStNEZrjDGoxMaJgYBHLxl83scoDw76/S4FSxjkmgql0Fh0khAw2CTDmgSbJCRJRpJkWGMJKDAJLjiCDygNCkPhcxKbkihx0957gvdrzok1huA83nmKIHY3+CCGBwgoggIVNF4HTFCAQQVZrF55AVJB45SWxYvHeA1BEZSC4AhYTFCEAIXOwcta1hgUAaVkPWil8YTqb6UDzgdMuRNeboh1j3skyPURKHBYNCqADgoVLArQXqO8lWfoA5cXFzl8/CTNZko9zQBDZlNG2m0wloCnZgPWgNcQcifPVyGgLbWgNM7lFLkjwWBrKcoYAoGiKFBBVwZztdH1BUsrK7RsFxUgMYGgHcp4AvK8jVYYY/FegHGIwAcCOIcPgV6/Bz6Qakur2SBLa7LUUWhtXv4xQnz2hqTRoD49zeTSCufnFmmPjhHOP8exw0/zhte/ftX7MBhAg9agDVbJbKNlnrUsXlQJI4MmEA1Y/D7BiOFTiniXaDQWg8cTlAelMABawEEInmCSuJ8LjNIE5bFkhPhstDa4aFCVXZvcbLbbrFcz1LKMoIzYLzQEhwqWEGQtSFhSLscC13fyDJRGJwl5UaC1xXuP0g6tNHnfoRNDr9cjVzkoRa/fRweFzTImJicZGxsnBFhaXGRqYoq0ltILCqtTlhbmmJyc4PJly/nZy6vehw+B3Hl8KO2DJs8dy52O7Ect4NAHMDYhBDDaEEKQvW3k+WqjMT4QNDTqDQEEhYD+EAQg6GhH1dDftVaDNMnoFx1xWEqDChGYKk6cPI9NEmbWr19zTrSPsUgMDAlgjJHn6xyegAse5zxByc/zbiHP3geMttTTjFqa4rxDQ3T8IT4HRfABrRXWpoCAMR99SAkSarUGdhyWlKyJEMAmGTpoRsfG2H/DdaveR7vdYOP6CRpmmfmVAheUfEbcCGL5AiF+HYjrNgIFBbhQRJunUN4LyDdl0OwxIeA9oAKqCAQDxqbUa0266QpZWkOZHo2sCdairQEvflZpjbGWNSww8HWAgZGRNkQHtW/3XjZt2siWLVvZsXMnE2PjHDz4JPv2XU93ZYVTzz3L/Pxl2l6xZBRFXGDBB5z3GG2oZRYfDCEU9PqOwnvqRpGmltx5+v0iGkpZyMZorNEoB+DIQ4HHYTAEFfDKo52isMWq91F0C7r5PIUvCIVHEdi2axu6oXGqwIVcotAAVovjNNbKAgKajTqNrI5Oaugg8E8Zi04yjElRJkVHQ+q8o+j36PZWYoCkaLenCUVBUfQxNkGhCLkgQG0MJklItEaptcGAsQoXCgqf41xOCIHgQZ62PDsArwI6lI464HTJBIQYGShxxARQspBDGc2g0EFAggbiWkYTCKF8F/kcYVRChMTyPRcCXn1t51UOHTwhqJizCoPPCAElN0QI4Cmig1GoIK4vkPP86bN84YtfpJGleAUXzl/EeYcvnb0OvOP+11Nv1yhCjxBS9ILCeU/uBERaDIUqCD4QvGygJEnQxtLvrqA9uODWnJPceWo2peuXcAGUtmhtUEHT73XodVOKoof3BT4CHasMwYshLlxBcI5+t0fhHEF7EqsIwVHkOcvLS6AzkjQbPPMItkIIeFegA2gFY+1RavU6xfwi3aTOu971bv7TTz/G8sVZfv0Xf5Z//0u/v8qkaGG1FNgYyzsdSBQoZUCZCN4DErTo+LXGqAyFx2lhmHyMwK2XNRRUggkOjaMwoL2AAq812hvQAmLL6D8ojQqBAoUOHqMtOjpvm6ZrzolNDM2sXjlG7wVqSpSWy3UHYfkUheyJ4HHBY5QiSRJSayVQ0LKHlEKeSdEjWIP30UlHq7t5y3puu+1m+q5H0XWk1vLc2ReYn5vj8uIiWa1Js1nn7te8kr/6+CdZWVpkot1aY205cH2ee/YYX539MovdJbwHwUM+Mh0C0hKbgFGgFdpHO6UNfZ+Lo0SRpgl5kZP38wjuZO/6EDCJwQfQOq4vrbFpnVqtSX+5Swjla2V/GqXIUSyvdDh+7Nk158RrhbUWa1NCCBRFLsFZUaBR6BBZJe8ogsd7R7fXQTlPXuRYDToALmels0RneYlzp59nfHqRyckpWqMjNOoNavUaSQx0AAgBozRFyOk7mWulLYnNUEZXtkXAk8bpZNX7qI+0mRydxK08T0Gfioap/laoyhVHEICK+zZEID14tewfHUnbAMETUPjoewMO7RzaKLIsQ2lDLclQKgdf0KyPkGUZOtG4foHLA41Wk1ptdRsMXwcYuOuuu1mcn+dVr3w1r7jrFaRpDQDvA8vLS4w0m4L8lWZhcYl+7mhpxQKyuHyALEnoOfmaENBKkdoa9dRzOc8heCg8RQLaGLQWFO+cIxQFOYr1MxvJVIiRGvhIISuvAUGNq43CddAhJzEQlGfjpvXccuAWtAloNHiFK4Sx6K70CD7QWV5mpdvj6LNHUcGQNlponZDVmrTqdWr1Olm9yUh7nFq9Sa3WILV1tLGkKiVoizZxAeCppXX6/RV8P0QGoUaSZBWV6J2rjP1qQykrDqMI4BVKaQoFDrAlFRUUKv6sXJPBK5wJYpgxUEVEAdA45QnBCxAKBh8i6xJBg0SBHl9C2xIEEOmJaG4i0sCuAUudUBoyh8rjVYjzqYQxkHwNeCSCRAlVqRNAkdoElND98mA8RaSelQpsmB5namyElfllnC/odDp89aknyWkyOtFiw9QkqU1JU0uaGoqiH6lFK4bKeXxw+GJ1oAng+z0Wl+bRacAha8rlDryXjY4mMRnWpuTeU0Eg8VC4wuEKx8ryMqlSNFstlHMszl1gvlXj+eOHaI+NUauPktbqpFlGrdGk2WyRZTUCCmsT+nmXoBTaGJI0Zd1Ui77rgArsv/1uRjqLaywuD8oQtBLmR+sBWxANWqCaIvLgYuxvSEyDEByqBAdBRyZA2B+tNAErYD64ak6DjhGpLmTtWvBOIqHgA5YYdRmLjteV1dprzokxCcEmBO/wTqI3WZI6usUABjHCWldBQJKmoC21Vptu33Pi+QsYIxGsTS1Ga869cInJcWg1x0jTDJtovBcwbqymmTRY7C2SF8JGbdywjtr8MpcuLzB7aZbFziKtzDDdbrFjx8Tqa8sH+t0+v/pffkGcgwoS9QfwRqHSRBgJm2KNxRpLahOMNWS1GmNjY4yM1hgdaRKUo1Fvs7IkdiDN+pw/n7OwOEtWS2i3R1he6dJsNLHKkiQp9VqLLEsxK7qaewEMwh7VssgeurUDmbzfxRUFuEDQAaUUiU1Jk4wi7wvYD4HCFRgLzvXJ8xylNa1agyU6qOBYmD9PY7zF0sIlzp46Tr/fYXH2XImQsDbBpgmtkRHWr9885FM8Rd4XMBMCyktKVGuNioRW4R3trL7qfezdsZeFi+cIAXq9Ah+GwIBS4qdiQKuVpsRr3osdRkULoGRNpmlCe6RJrWaF9VlcodfpV2DGGENiDVpBoi1GW4yxhCJBWXmGSZqilSUPfYLvUWvVafzvBAM3HbidyxcuCOViZBEC1Q1s2LS1olnKHGwRApmGHNBGk6YJ00nKfK9Hp1dQt5Z1422Wez2WFpbo5IKCWtawe6LGct+x3HOs9AUFruQ5b7jjThZPHGZ6+3pGt9zIc2de4IWL8xR9RaECeo1DGDOr6Ye+GIPg2LZhC6zk9DoLKKVRXvLsnpgk94HQy1lcWOTowYNx8Ru8dxQ+5tgDOB8g5od0ktFqjzE+PsHMzAY2btzKhumNTIxPU2s2sCYh0w3JtyqPK3r0+x2sTcnSOoYU8nzNOUksuKKgKALBKRwhXpBE2kGBETYRpxwmaBwI3RucRKYIpetUjvKaEA0MkUkQaCHsgSe+WfCy6COiVUEWglZQBCAyDKBxIaxJ5UpULLBEqHuBKS64KuKUWZUPVMHhXJ+gMjSeLRvGeMOdNzI1Pkkjy2g3Mmr1FvV6jVqtxsREjUazQb+QnFme9zDKo8Iyj3/1DI9rz7qJcYlQk4xWs8n4xCiTUyOMtBoU/RzliU9rjTkxBmsCvbzAeYFFwoZLVBCUMEBWG5wLJNaijaaX9+i7HktLl3F+kdkLF3HAzMQYmQ50F2dZt/FWtuzcTJrWUTqR/eYKChforSwSvOPEoSfxLtDv98hdHxej7vUbNtBb7qKVoigKxmbWrXofWlmMUsJyKR1ZI0lrlXOhlAVy+QwXp10ZjEkpgtCbocSHGExwhCDP0SvZ69YbCu3RUU8TtJc0kFIVkyU0a0AVGuM9yiaS97eWemN8zTkpXAePpNOUJ1LHCalJ0MbgFbjgCYUSzYKXVOTGzduptw6SF20KDIePPSuJnuDxhaNwjjzvQTiJsU9gjaVeMxQux9iE+ZUe42PjhODABZyCxKY0Gh5UwszUBAvz81ycn6cX4OnnTq95LyCOUWlJzwYf8EqjyRhptmjUa6xfN83mrVsYa7cZG5tAGU2SGM5fOk/ue2zYuIGjF49w6NFzLPQLVGcO3VxhoWfpdAOd/qKkc9KMflGgbEJIUk6deYFO36FtQuGLCvsrbdBaC4uSWRq11R0ogNGaJE1JEiuQ2aTCzETwX/KN3jlJYRQFve4KXsHI+AhZJ0W5nHxlmTHr2LVrC7t3byFJM2Fcg7BXxDSHMZYsy3j+uZM8+dQXGR2dwuW5pFm9J3eF2KAguiqlJKlVq69+L3Nzy/SXFwk+Jy/cIMRHo6LtUqqk6yO7rSHvO3xkT2VfyWvSRsrIaI1amkoQ4hx5T9gcpRW1Wo1moyYAwkAIPZmDoAje01cOYxO0CvgMdK+PtQZl1rZdVw0GlIdWa0QchXMxoS4iQImYJBeljaHZajMyMgLeM1WrMVmv0TSaiVadAkfa61e0Wq/Mg9kE78RoXuoXnF8uaCSWsZplvAGZDiz3FSdPHEEvzrO5vp0927eyf89Oer0+L1y4xKGTz3HmwsVV7yP0Pa6Xk7scyBmpp5GmZBD9eo+s9Pi3cmRZwr6duzE1yUcbazAmkay5FkQQggLvKVxBkXtWuh3mTx/iuUNfYaHbwQdLsz3Khk1b2bVzHzu27WJiYoY0rYlwUAfyok/u+xhfW3tOlDw75zyFz/E4HAGP5I2NF1+qgxg3pzwmGMldewMovHJCW4VSjOYJQQ+lDiBQCgkFEKig8KiYtwViRKiUi9SXbAJJLgScXX2ZBe/xoSCQ4I0sbK98fP6uYjhKsUJQguwJAR8CWzZM863veqNEdEph4ubTaIpQ4Ho5odfBJjk+KGxYYcvMGMvdLod9j14PRhopY3WNC1CEHpfPPc8Lzzn6QWGNYWp6hHWbtq45J5dXuri+o95QoBOshTSxtOoJzSxgjACuft5HoTHW0Ol1KPo5zjlGWmME36fZaLF5y3YUjqLocez5M4w+/TTrNmxmbGyc1sgYtcyQ1GpItK4o8hyreyQJ+LzL4uICBoPv9cnSlNkLs5WIbWRsdSeqlI40uqwlrXV04g6LlRQMLupUFIVXBDRBG5SxGO/FqSqHCkEo3SDpL4+KKaZCNAOhQGNwFFSoMuiYlooiYxVQVvaoSVKsbZIkNRqNsTXnRKMik9dA2QwPdHJPr98HrUlsSpZl1Fo10jQhixqfHTt2cd2+/ZG6NehIIwcvwGIl79DvdOmu9JhbnOPy3DznL7zA7IVzXL40x7PHTnAoP0re7+IDJKllfHyMVnOUxNZwPrA8N093YZ5eXrBtcnXBsDIS/aENyhqU0TE/bZiameLHfuSHaI20I0vmyPM+tTRjcnyc0dFRsloNbSyFK/j4Z/+SRxfP0F3skNUa+KJNrZHTGh/DKGHEXGTEgg+Sg1aekfokS5cd3c4ioQS5RnQV/byPMilT62auYk7EaqCV6AKIImEChRcWJXjHysoy/W5OKArOnTtHr9+nMdqCAnRiubzU5ezlOULQNOqSPtPGYqwFLFpHDYFShJDTaii2blxHe2wdmfZ0lhfo5X2Kbg/vfSQUAkp5EmMIyVq2K2dhYYlGQ5EXnhLKRBqTmEnGGIWxqmIH5OcDYlVrWVcqMsPdfhGvQ4tGSMWAOkup1VLRU/gCpRUbN08yeyHg8z7eFWAlNa91yToWFG5tbc1Vg4EQAsYagncE7+j1VpidPc/s7Dl6nQ4Tk5OktRadzhK3HbiV3bt2EwpxTS4IfR+8Y2RslB179oJz9IseReHpdTosLS8zNzfP0oWzXFy4wPjZy5w/f55Lly6zsrRMw0o2b+/IGLOzp0lsIkIsbajVa+zavpndO7eystxZ9T5WurMszZ+hls9x57ZxdmwdIYScECKNogy6pG7LnKX3BFfgjSI1FptYUpuR1dtkSZtgREaFEkerQ5kKcRS+wBc5uSso8pzOSpe5hVm+9LljfOrjK5h6k23b93DgljvZtWMP9eYIVqcialpjeKzQyt7jovCEIK7bBy3CxiBUtAkKvCj1JUeqYn6zdPoKrx3GC3ugg8ermGYISioKAOWVLOOgKhRc4vgyi1/F8QERzqxO1uB9HwIUypF4HYFAiAxEKcYJeF8IZR1EIBO8wxUBXYgYNO93RECkIlXpXWRLAq1mE5O0UUpTr9fYvGEzl5aWqdfOoYxm9/YNZKbLcq9Dp1+QpZoiD4BluZezvDRLZ2V5zTmxaY3R8XHuufNGdu/YSnukTdZo0h6bpt4cI601ybIsOnExijYx+CSlntVoN5okVkDUTXfUKfI+wXnq9RqNRgPtFSvzKywvrKCMIksNFseDn/8C7bExrNIkqeFN938Dx08c4tiRY+Sz0Lk0z5Nf+TKJNuLs1khDCRAQkKi0iokkYm49DDQmESD6CMxAoXWKD7kAw6AFCEiiQPaWsgRVoL0h8gPxQ4WV01FgVZYblGJVZ4RqtbZJlrXIsgZptrZmwMc6AacSlKnRbjZoNRs0603SWkotySR/bcThG63QysR8OVEcq0QjYRRWy+u0FtFxKNenk5RSr+jT63aZn19g9sIFTp58lqeefpJPfvJTnD11mjS7RLM1ztziHBtGGySJpkVKolc32CMT47TabS7MXkBZSy1rELTBO8/27ZtYt36G9sgoI+0R2s02Wb2GtckgBVjS467g9Xffx/aZ3cxeuMDs7EXOnjvPmfMXubzcxeWS4MIHXFHgXBBNkgv0u0uMT06S2GmWlxZZ6XRiZRkQNQCXLiysOSdKibhRWEAv6zJLyYsc7wv6eY+lxXmW5uYgLNFdyOisrJDZBIOHDPCK85dm6T7eQeuP0B5tMz4+xcTkFOMTE4y0RkgtpNqAUfLMGjX233ALzhW87z1v46knH+XJp57m2eOnAbGl3gWhVI2HNXRC7bE2F1+Q6qput6jSo9EjRJZBY0wEAmVGVQXWrZtkYXGRTr9PwFMUnn7u6PTkfYJyFK6QdWYEdGkFzsv7F/0CrQ2dxT6u5/E1By7HaEWee3RMVQQHvr+23umqwUC/32VxYZ4L517g4sVZXFGQZDVGWm2azRF88BT9PtYktFptlLWEIiL6MoJ0jvb4KO32qIiTEGW0MkqQtyRWRAMA5EXO8tIys7MXOHvmFGfOneH6/fv5xB8tktYbgMI7J2VOMY/YbK2Orheee5ykN8d4PcOZJiPNOoqADy469BiBVrlRKFyfvOhT9Ds4q8mNQWuHIYiwSCcYrUEnBKvRRqMQA+JVQDlRAgdf4AvHuqLLrrxHnnfodVaYvXiZT/7F7/LnLrBxy07uuPNV7N5349qTooKAgVyMcSXkCyXulH/bMteuRGGqkbRBmeaXdIETyUak6Vw0xD4qooKEiGIgQsUVxIxr1AnE5xUiKyC/GNYENiJ8dJX+IyFBlUCgqqrweC8RZFCO4HKKvIvVCcEXErnEe3DlvSelXiJeZ6zgCEHRcX3++oEvc3HBo63hwlKX6zZNMjkha8D7QC/vsbzc5fLKRfYdeAWJW1sz8KEf/RAb1q2j3WqgtZTjRVkvxkoeV5VMkg8i8kI0MiYECBZtAkmWMTGxXpxt8DGtRZV/VChMLFvt5TmPf/mrnDtzmovnTrHSXeGX5i6zc/8+9t14HW5phZPPP4tzBWjF4uVLXB5ZfesLDlTVHigXS1AeHZIoqPXxhVHdHsWrXmvwFqVcVRki6SlheTQqVv34SgioQ1md4qhuNl6I0lLdIsZUY01CktYxWSIswhpjcaXD7q272bB+gzAAaYZNDEnUJdkokhM7BEtL8zQaDWHRooq78A6jFWma4pWUFvqiwPlAs9GqFPWJVZgko1HPGJ8YZ8f2rdxxxx3c98b7eOqZgzx38iz9Xk7eP8/yUsKFsw7vcrE9bnXbNTq+Dmtgx4597Nmzlxuvv54bb7qObdu20mqPYG1CmSl0ztHvF6ysdEWc512Zr8EHj00sO3bsYOf2XVF74Ol1e8xemuX0qdM8++wpTp45y7nZWVaW+3EdCtiZm31BFO/BVbl9h5c9U3RIGmu7FZNk2DTFA91+D+cKirxAKYcrpGpgbHyCsbvuwSqNTUwsCV4G58mLAmMVzVaDxBpmJqcJrofymssX57hwbpai6JLWa4w1W9RqCfV6gzTR1FLRJhXBcevdr+SVr3sjRw89zb/6pz9Me6xOs2lZXilIAqx0Vw8AXIDlXockUVdWQ0WBoFFaxO9o2lmD8bEJLl2cZbRVZ3J8jHqS0M1zYUZi5Uo9TTFWo3wgV106StZ53xUQAv28hy883W4PvOfiwiI+l+cWvFg/5woBA8jcFsUaERlfBxj4zN/9NTZJGB8dY2pqnYidnMfjxciomG8LIgDTSuG0kzpH50WRW6+jlOHcubNRMCEag7I8xShRwRqbVLX5xlhmNm1k07ZtWGPRWrNr936Koo8Lgc7KCp1eRyITHyof9HJjpl0naThSW6NRq9Gc2hRz5AIAdIggIEh9ZgiBfr/LxYsv0J27gOrOUdQy+ialu3yZ5dplrMkwypI1RlA2Rac1bFYjSWsickuMlA3GxRJ8WbvvCHnO5Lou2/MO3c4ys/OX+Mwn/5y//vif8fpv+MPVb8YpnA/khRRrmyDGoHTaQSkxDDrEnLcnaHCRDShDdh3FXyHmQyuH7suvRClUph884sDxKjr68qGXsCCmFiI1vFaVZFUS5AIhOHyVdsmFYi6BSABCEdMVIvJ0yqFUTqIlzy2XGisiQpAIFdEhWK1QylMgIGp5sU+/E7BJwaNPHGX7ujZJkqEAqy1J3TA2Nc4tO+9j885bSbPVlcUA23dsRQVFP+/jcifi7lg3rIzGWktSWIztA5AYjc3FubrCkRc90iwha2Z0VjqEWEqXpSlZPRONgTIQja/zAlC++QM/wNGnnuLDv/yf6HcLjjz2NEcee5K//INAgWaq0ZSyJx+o1w2pXUOgqmLFSCSAdEV/SoyvK65Anrf4hMgOKE8IIg5TPgYEFNLDgRBTB6I/8Ih+x8WIyamAwolWQcccv5LUEEoqL6xNRe1OoJ+vDdCOHTrCbTfeTLMhDIDRAR1THJQYRoPyhp5zzM9dpt5s0s/7EiXqgNWG7soKibFR4Crv3et3uDx3ifHRMYwRc+q9F3ZN6aoEL0003/td384n//bzPHXwGCsrHVye02waOitLdDs5h19YWvU+7rj7Nq7bu59XvvKVbN68mVarjQ+evHD0+z1Wul3yXk6/36fAEWKgpJTQ1aWAo0ztSi8HJ2Ac0UJNT04xPTHNLTfeTL/os7C0wPOnnufI0eMcPf4c52YLDBuFpg6OUDhc3pNgyRX4wlP0Vq9nB1BGURQ5oZ8zNjKKsZp6VkNrS3ABHwqKwlEUIp6rJQl9rUl7fXJVoBUYa2i0RsmyjMboBEZBmmTYNIlpLjEaea/DSneFi3PnuTz7AvNLl7k8v8iWdevYumkdrSwwe+Z5vHfceeAW3nbfazh85BgXl/o8d251HUfREyCfpilpVmeqPUaj2WSkNcJoq0m71WRsZISsmTHSbFOr12g226RpRlZv0Mt7OESsbEys3lGWWpJi0XTyZfJenyIvmF+8TL+Tc3H+IoeOn0DhKXyflV6Pdr2OT2SvpEqxEoTZVgr6rj+kZXj5cdVgYMv2nZJ3cYF+LiUgYqi0RMZx4aMC1hpCluALjXWy2IzWZLU6KEWn02VhcYE0sZIvT6xE9ygKI2IJ54NEG1pytokVwUySpCRZSprUqKUJrdFxFIo879FZXmS5szqSGx9NaCRtataSJQl947h4/gQ+lpkkWhTUOoi4SykIS7PsXT9KsnFUmAwF4GOeXeNdjzRRBGYplMU7S94xuG5K12tynRBMjSRtkWYNrK1jk0iR1QKJ99SCp+09o+u7bNvRo99dPd0B4LyPGyaQF04qUdA4RJleln6VEYEOAeO0gDWIoEdwUNAS7aOCaCiCUJYFHoNEBZIuKJ1srBJB3kicr4rACgi6MjJurYUYPCp4VKHwpi91t1GoiIv9ECoxmo+Ax8frKKgrQy3TeOfp+95AIaxEnGSV9Door9ZYhYkCNef7+FycTWal34QPnkL3Gd2wnqlNN9HL+3SWF3EuJa2NrHorFy5cwETVRN7vY43CaItNUrQ1wiRZi0kkD5hoYbRAnIg1MDE5hVKaXq9HCB5jraTnlMMnKWmaynsYQ6pTahCjU8P7f+Cfc/7sWc6efp7Tz53g3Onn6F6a48zlOVItLMOXv/AA/ev38I2r3UhsqFXRPZQ9PBQquJhWEwW0C2WawOOLwMYNoxx98igLc5dZXllgdLzNjt37Ofj0QdrtMdIkk9xqFHfpqO5XXnQeZemVjloIYjpLY0GlKJsKY9cXynut8Zb772ekPSLrTElpszEGo4W9U1qjTMBoxZnz55memsYaQxFEt6K0wlpD0RW6PE2yCHA0jVqTxKZcuHiB8dFxkjSVfRnXp4qamoDnpptuZv/+/Zx8/hSf+dyDPPrYQRr1Gnt3bcEaw4njz696H+94x1tpNjK2bt2K0pa5uQV6/Vg1ohW+cPTyPhfOnWH9xk1gk6jv8bH/iAB7jZRql6WElFvWxBQcUgljg6XRaDA9OcMtNx0g7/U5N3uew4cP8/gzT3Pq9CzdXiFBYdGjn/fodvu43tri5127r6dWq1NL08g6OIq8QBuxq5Ftj/MTf0kr0lqG8Qmu7zBGV3MYvCcYFSUnOgoH5d+21iJptKkVBdo2eO6LD/Nbv/kHBJ8zMzbGbbfsp9ZoUuSO2XPnuHj+NFs2T7Ena/LEFz6/+o34nDRrkDQmuPuum2iPjVBvNrAWssSS2RraxNRBUOSuIE0yEa76Du16A3RCYhNhxozCGE1mE5r1DGsV1mhEriLuul84ur2cTq/D4nKHpcVF5ucvszA/y6lz81y/cxPPP3+BcxcvMrN1E6dfOHdVoPmqwUDuCoy2VW4MtJTjqZg7s2X9t6gejYkZkxgtWJvgnOe5U8+RpglTk1PRKEAJW3SAEBxLS/PYpE4/75OmmVBI/RwfPCaxQKDRaEHc0EmWkSSWenuM5vjq5TljjYRUQfA9iryHKjLqMQJWpcHzRur8RQ9HI4Pa1FSkXpTUhgBlmgKlMAW4ooslgCqERlQ9lAkEbQh6Befn6S9B7hM6OgVbR9sRkqxBkmak2pKqFihNodbO8XjvcS5QuJyi6JOrXISC3kXH7HCFx+fC4CTWCLUMGCTizhCmwBNQQRTPDokgU62rJh1OSbSOFsGkULfENEMsy1GKxFq8k98PgA+Kbr+/+n0EB5HeQmtccLH3AOgokgGF9zkoiwsOn/fwHTAmENKMrjOIsj3qFGJtv1CnhVChLoDWURzlMDajUU8JBJppgyxJyeqGpJ0xs+MmPG06K0ts2rpLytlYW4Rz/twZMitlZ6BII9C1LsMYQ19BYhJMasAX8jNjK/p//aYtKGMoioJ+t8/Fyxex1jI5Pkm9lpElqTTJMqbqVhZiOWeSpmzatJmJyQl27d9H8IH5+Tlmz1/g5InjnDx2mPMvPI82TRK7ekmeDoqgSyApOg6DzLk0gSn1AgWu8BLNucCl2VNMT27m5PHHOXf2FARP/ZababRqHHnmy+DFFiitqNeb1OsttDFkaUZWr6GtoZY1MIlU9oDBewFrYuNFIV7kgV6/T+G6a87J5s2b4mdImZnVMj9KS2rAaotWil6vR5EXtNrybFQAh4sCMEOjNUKvsxxz9WVKTpGZGhvWbeDchXMkacJIa1SExREwK6UgGJJUYRPDvj172Lt7D2fOnOF//NUn+PJjhxhttXnFXQdWvY/R1gjt0SnmF5clsZfnaKulU2BkvZLUMjo2wdmzp1g3s5FavY4kPmN1VLQPLnZ49MHhY8AmL4nJvxAkgxWDCuMlKNu8aTPrZ9Zz1513ceHceZ45cpCDh45x5vwFVpYtmITcrh3ItNptEi2pGtmPElhaXaZ+pAIDozEmwyqDMinGpmJzyv4gNkFrS9/1SbBY58nzvjAMwaMT6cAYnABW56FWb7F1+y7mLl+ku7LEqWeeptZoYBV8+nNf5ODhE2zfup4brt/L2fOXVr+PkQa3rD/ASHtcmgFpI1U4XoCKBCYWF/qoQtHt5Djj8EbRLzrYpEvT1lhRgU70q4m1NJKMZj2lVq+RpYbUpmSJxxgdfSwondJoauq1JlNTM6D2Us9q+JCzODfPc88/y7PPPke7lvLc6ZfvbFmOqwYDeNBGHIGxunJEzhWUbRWUNgORSu4JvsAFET9Zazl09GkmRkeYWbdRJjuKLMpcN8GzPL9AvVZHJSlpllXCF40AjW63S7fboZ8LIkOB6XXQWpqD8DLd2cqRptJCJQSpUV8pPCqRnL8KGqVcBARCURX9Lj7PKXxB3uuiFSgjXQadLyj3WA5QdicLCuJzCWjRTOiAJtDAoFQf1AqEBXJ3mf6CpkOCrk+h0xZpYxSTNtaekhAogsf5gNMeEyzg6az0yXuuitKtko3c7xdYa9g0WWe6nTKa1hgrArpmcQSKEFgKnn6MMtpdT6IKci8L9YU8p1cE+l6RFzkUnsxaCqVZjpG2ScpoLjaO8aHy8y83ghPhi1MB42JHuKBRvqDQouPxLgfnyJNAimG8kYIvCGhyX+D7PZmPIDqHUtRoTYrWitwU0go3GJSK86SQ5hzBS84v84xv286mnXdx+tQLOLfCzj3XVQybUmuDgYsXztFuS+MPYxKUjgqK4CQC1oYQHEUvx7ucRCfoNOA1zGzcTLM9TvCBSxcvcfaFs2zYsJHJsXFq9bq0F/UOHUt3deym1+t3yYucIu+jjSLJMkyaSgc2q5ianmbn3n0sLLyKhYUFRtpNxsZWb3BTdaes9maZ0pP1XdL2QUmE70OOd47JdVOk9Rq7913HC6efIwDrNmyk0WrRHp9gbvYiRa8nkWBRSElW3sN3e2itcShskpFkCWmSUWuMkNbq1FstarUmSsd0CoF+dwXc6kATIK1nGPSQSDCmbaIA0EQa/dSp52PULUGMNrJ3RZ9hUNbS660IMFKGUqWjkDax69dv4Pz5c5w7f46ZmQ0QWbXKwgWNi4LKgGL7zu384A98gCeeeIo/+dO/5uOfenDV+6g3RqPKHpRWeFeQZI14/QqdWBIM9ZlpRsdGOXL4EBs2bmRkdEzaRmkBBcorjA8SNHhDMKLP8T72dPEigq66oEpchw5K/jcCaqanpxkbG+X2mw7wwvlzPPn0Mzxx8CDnLl5F91Rt0FajjDxrEwXh2kh62DkoCofSCc1GGw2xFC+KVMOg/FApwQ2hCDjXlUovrdDaYiEKugNFLqmMkYlxXn/vvXQ7HeYuz3Hq8JPURxvUF7v0/BKnT5/h2Inn+eRnv8xa1fnKyH3MXryIcQV151jur9DRkKBQxtOwCaHwuGPPMqZSet0Oi7vWUZucwvZzZh8/Rp44LkxMxH0m6ZvrtmxmpFnHBaSFsqlJaasO9DE4rRltjzPSGiXLapgA3c4SS4sXSaxmYnqM7Tv2sH3Pdfzh739k7TlZ8xXVkDIvo4Vm7ud9QuGxWug7EZI4aYYRld4EEdg16m2WO8tcvDTLjdfdGMVJJtLOoWo3CUZytkrqMX2Zl1ZlO1PN8eNH2b17L+3WiChpvXxmr3B0ev1B0P4yowhaWuR6hVPQ77lYhqKEitGJoGak+51OUikHcxqjLcH1RRUdFChp86uRSDl4BYWIqUQP4SPKRnpMo9AqNlEJHu0goYe1mhY5ivPk/UusdBW5HQXeuPqMxAoNodU9aFflBrUBFYVO17Wa3DjZkk1sLR6wIeDyHivdglOFpusVGItGelq3Q2BzljI5OYatCyLveI/rFSyiuawKlDdk/YJur8dst8P8fJduv89cL2exKCi8il34Vt9SwfcIQaF9TpCeqeQhJ/T6JHlX1NypxuoQdRJSQuN9QXAK+n2JVFXUDehQtcwOwaNIhE5WUUfhV1jp9Qh60HEsa7bZc8cbmZrZzhOPPkKtWWPP/huomgKpsv/B6qNWb1MqfZ3qowpDbg1F3sHZBJ9YrFIYa2Lv8IK8XzCxcQOjE9MEBRdmZ3nuuRPcdOMBSUtZU7K5URsTja1WXLw0S3tkNJa+iWFKgmzr3BfUVQPvA0YnFK2CpJYxMzOJXlPUCXiiAxH5X1AeLUoCdFAUFOgQhBInVG20jU0G3wNJaySWtNZgeuMI3nsWFy9CUJybu4wOnobS1GMnxqSmIOR0O10WFy6R93LJ9ac16q0RxsfXkTVH0dZU/U5WG9ZYoY6ttIEWZyFdDE3srtddWcEVfVqtNiK6UkhrrVhiGbUf2lh63Q7NZpuy9LLUzHgCM+s2cGl2lueff5bNm7eIsBCi/ohYLinryKuAUQm33HITe3bv5DOfWx0MJEZSQ0ZBojVBadJMei6I6FWqigiBNEnYu+86Dj7zFMudFTZu3IxBhNqe2K/fSTtzFxzK+0EZXnkuRJni9aIHcaFkF2KpqTVYEoJSbNi4kXXr1nHXnbdy+OixNefEhRyckZK7CGpFs0QssYPUGGytRr1Zr4SopfcPkdkkMhmiY4prLgScg+C6qH5vwPg6KXetNZqMTkxgF5dIs4zR0RZ5v8dXDh1joZuTKkWtZvAekjWWV215kYtHj/J8f4VNusHu7RuZ3LGRsekZakkNcCx3V3jk8HGeOXSakbalX29yqx6ltpSzePEScyEwWyhuuOcebL1N4fr08z7NXs5IEmjWW2hlSJO4l7QFozh76SLHzp+lt7LM2XPzdHt9fAFqpU+zOUI60sCRohWsrKzNoH0dYABAkecFvW4XX5RdRkycDDEUWquoikxQIdAaGUWjOXTkILu275Ba3Sg+UjGvI7piAE+rPcLF2QuMNxqysKP9DXEh7Ni+k9kLF2iPjgoVq8Aoi4109VotY/PcYUKBC46e83gXqeYABeB6XYlMVWyqEvPr2otYDWOiUEihnTTPCEbqpoMOaG3FcARpsVk28ClTIR6xCBoBOyEPIqZRBYoOiU0YMwlOrU5PyTCi9vfSOVErsFqRpQmJsSRWUUtSRhs10tEGaS2lnVmSrCEHAKmANZbNWklEFoWb3uUkKrDOGFLtpHrAKZK8oLCevNdhds6x0l2RtrLW0FIJd2+uU6vVyHPFpV6P88pzebFP2lyd5fAecMJy9Io5fL+HDdKpMUtrGKPxOArnCUUueVDv0EkSKVg5kEO7EGvVh3omWI/RCc5J2iREal17T6oUAYNTikarQa0+zWf/9u+YXj/Dnv03xkZBpUpYnMFaI0mstFFW4hidUwTnoXCo1KNdgU8tmVIkaQtloD0+xvT6bQIiipyvPvEor7n7HjlkRsWWyipG5t7jtdS5u16fXr/LCKOVlkJjRDzmBYT2ej0OHj3Mpo2bkLamIujt+bXyuiEaXUR/Ec2vlAdKE6pAiK1iY/UNsga1tvTznCIIW3T58hzTcwucP3tK2Asla9/YRCLsECh8IPee4Dydfk4vePoR5DViyqvfX0Q7x2LhefbokyIeW2NtyZwkVQrLWisBjFEVq6EUHDt+jB07tkvaRSmUD6hYXlg6c7Si2R7h2JGD7Nl7HYM2swwO1yIwMTmJtQknThxn67btZKk01NFICqvkW3SI4kQF7VaLt7z5vlXvQxuFTY2wTjbB+Z4EJE5sipy5EHW/SLvhm26+hWNHj3D44DPs3btfQGgEN+DxOnYZdQEXynbcsdZfa5TzOFVglTQrc4lE7iFqxgwJBkNfa3AOk0xw+21ja86JlG4aymoZkPbg0oacKLIOUh6YNWIaUmBXKPdjXKbS7TIGlkRxdkBYQo9UKgWpaEmyGi2bkNWbrLRXWF5cYGnpMmdOPVcd3uQHmCM2p375sdkvctNN+5jasZVGlkngGALzi5c4fPYo8yt9lhccJ+ZzLmI50+3z5PlFRmxCI7Fcnl9kvpMzcf0WXnXgBrbcdC/S2C3Q6aywsrwgjJkPaOUp8oKVxUVWFmbJfMZOVaPTmWfDxBR2dIKV5Q4PfeyTfPHjn+f6W/eysuU0Zy/PMzYxuuacfF19BrzzUsrmREVaRVrO4aPTlkkR8Ux7pE2W1lhaWebs2dPcfssBtDHRuAgkqJowqYDyhpAo+s4xNzfP+PjEoNY/5q5Hx8dIkoRnn32Wbdt3SWclOY0nlgauVTIlDsR7TeF7klv2CJyJIi2tynR4kNyf0lhjxKl7Hc9fiVKcSIlrFY2lEvmNiB1rGJWKwdGWpF4XgRFgTYpNUozJULZUwUv+UimpiV1zToDgRXzjQ8G2qTG+4fpdtGstxloN6jVNs9YgtZqaTaISXShGrcR0qWhEShGgyKQVhBgTKSAYZCvJg+kTeFs0GK6I/SK6OUWe0+v1WV5YYfbSPOHiPNp2mF9aXdSZ93v0Okso52L3wIakVoA8X8EVKgpuCnGKqKoDm0kSVKFQKpc9pKR8TdoJS49z7wt0sOK4col2R0aa7Nw+yaFjl9FGsXHjFH/1V3/OrQduY+fuPQTnB6V1sQFPWN0uAFIdkJpYeuY9NgIKG0V/SiFMlE1wzlFvt5letxXJjXuOnTjGzMQ4aZbhg5O1isMHyT9KlaiwHsZqnPP0+j28i5U7ZWkojhA8KyvLKKWZGJug1+8RVCBNM7RbgwCtgE/ZDnpQURCQ56xDLFOzisnJJgaYm+8SgsMaxfjEGCudFeqtmM6zBo/ojiQilWZf3nt6QD+WWIWVomSDUUqRl4dvKZjrLDOXF7i+9Klfvrx6kzGQlrQimrORPjZxT8qaX15Zot/tMjo6Xt22UoP5VmHQX15rxYYNm1laWqQ9MkJ1ukd1EI1wJyOjI9jEcuzoEXbu2k2j3oieTAIfFZms6LkJRsXzNl5+1GoNyhNVnStYXl5ivN4S1kirqu2zDSrqAeRqdu/Zy4Vz5/jSww9y4LY7RODmDZgImn082yX2BHFeukBab/DWo52Ic70qwDkcHhsMkBC0MJJZAO8t1iVSArfGkLNmLNYkBOVjm2pAa7SDHEeaSflheWgSKnIAkSmOiAqDjYe+aUnHRoARCNLF3IvfKnyOK3QUjCpMYmm0WozlU3ivqY+Ms9TtxTLMaPvWaJLy6vtehYls5KWlSxx59hTPHH2OyZkNTJsGu3qapcvnWfKBUxvX058eYeUrzzB3YQk1krDc69PYOMFbPvCtbL7u1XivKIo+2hjqjSb1VkvSQFFDRaxMKvsW5P2cXqdDUJ4kyyiKgrvuuJevfvkRTp08juqcZyJr8/yJq9gna74iDimPkkNHpPtoEFGH0iJe0XLAiEbhPNTrdVrNEYJXHD52hN07d5KldTEEQwfnCCUXc5QxYp6ZmmZ+YQFrdFSiBwi2KkMcGRtFWcvJk0cFEGjpMe7DMFb/2kNrEfbkzuGLWHpigX4XZRRaQ6GC9GFXguSLUEikIJ8ip2bFE7aUSUnqdRrNEbL6CFldmqFkaROTSSczZZK4gCXqCEoU5r7KqUuNPEoAhLJJbA60+hD9v8N4zx1b1rN380aMcSRo6QJntByvHIGUMkJVOm/lmF5XRGcr+UEXBRDB57L4SgERkSqOVSNypJb0JjBKUTcZNBN8pO/Wr1dsB26NLZvzNUQDobfCaJqQpu2IrEtE7+WI6sRitUapLBoAE8vZ5AwDpQO1rIUL0tPeaIUvJOctqQQxBEpJisuHgqnNO9juMw4f/yLrNs7QLzR333Iru/bupWxyFKJnKKsi1qyRRA6TMSrE/vuA0VJ6mkp/+DSpkVqDtQnGWNat24Q2FofH5TnPHHya++99YwWYVaztKyspSvW8jv3cO8vLZFmNJI0tYqNVVTGNNTo+xgvnz7O0vIhNErQ20ghrjTSBClFtHsozCGROsryD7i9Gowzaw8Sm7azbdjOu8Fy6eJFDR47z3d/7Xfjw7SwtL3HqhfNktsG/+9l/GzUTntz16fb75J0+Fy5c4OL5Wc6ePcup50/xwrlzdDodvBdA2iXQpTwSWWFcAUqTx3W95rCxAYwW5lLEdrFaQSueevop9u3bE8vw5FeE6/D4YMRJx0ObFMIsnbtwVrqsBqCc69KexVrzRr3Bvr37OHz4MFu3b2ekPRLjFl+lF+QxKwg6tl5++VEUfbRNSAQlQVBVf4PqN4PYKB0rDJAEIjPr1lNv1HnkkYfZsWM3W7ZsHWhgdEB7Ee7igwQLXsU0gY4C4kLWtCnInI4nNBK1N9KMznmHU076Nqwx5heXqJk0rkmFNVaYG6UJJpAkCa3WqEiTg6tAizxhV6UJFIjfUSbqQETALsxBZPW8HG6UFxpnHNrn5P3Bvkpswtbtu/nW7/ggZ06f5vSZU5w9c5qLF15gZWl+1fvwRvP86Wf56pFDnDo9h11K2b9pM3t8k+7ly1w6d575uQXSkRbjm9Yztm0adeQk55aXqeWadHqMt/3A+9lx95vp9hWzl06xY/sOtLEopBIk73bxIVDL6rIGYxn+xcsXxO80S+ZEkyQZW3fsZOPW7VW58vLCHLe/8ck15+SqwYB3njwU0nzDxk5dIdbSoqoco0M2zuj4OCAtKg8eeppvevs3leu3quWvyHMlR6aWmbdWsyWfl+ekWSpOL6Lz8pS2sdhO9cSxI+zYvTeifGANdB1iXi2PR8ZqpbAWStvnYpQcVEmNSuOIpF6jNjJCo96i2Z4U1FYfI8kaoliPjVFk85WiK/m3BFYDMIA2oE0VeUKMRLSu1NJchZELGGpacWD3JmZG6nQ7S3S6ffrdJbr9nLzo4PN4OEsRUyp5Dn2LCR4dHNZ5VG5IlOW06/LV87P0iyLqJpSUFvoQa/+VlABGRiGxkrtMTCaVJRaaaUKSWtLMkiUpjXrGpnVTq95HZqTM0qsCFSTN4pXH2owkSUjSBJxUElibikPxshbLqFUQpRxBnWaWUJMuhTponOvLWQbeY5uG7Te9lk1bb+O5M78jDIIy3PPq17J/z/6qNzpaVV0ay6Y/V5GeRtrtRudjJa8qZWySWvLOSWfHBMamJknSNCq5Pc8ceob10zNyWIt3g5PkotVy3supbUHU0lopNmzcQq/XwRWFMFxx3QiDIu+xa/dujhw+wobNm2g1G7Fnwxr7JMTjYpHctPWeRr5CVnTlcCElLamTiXHGNu4m7/dxLsSDfuTYYeccWS0jTTRZommPjcppkDaC43it11PWwUvjm3Nnz3L8xAnOnznL0vIKRhtccCwuLbGy0uXy5YssLSxKhdHa+AxrDBYJZnQUDpb5/uXlZbory0xMTMbIU1V0f+moFQwd6asYGRsjLwpWul2a9WZJmFVrRMpgZd7qpsENN9zI4cOH6M/kTE5ORJ1Kaf0CMa8Ia6wvbYShLFMMly6fY3JmvaQgSlsSPPiSpRiwm9ooxiYmeO1rv4GHH3qY2Qvnue3A7VV1DdGZeBOqszTKiFQ5AVHOeJRT+Nj8SauC3BXkqoBCHJRSAV2sbbu89+SqQBpVenKl6GsdW9prpqbXi/g3CMtFfKa+pIiIDKYuz8+AoCVIQ1EdryxiV1VVPMn7aFxMk8AgUGqPtdnd2MOWbdvp5R06yyvMz8+teh8f+buP8+jBCzx3doUdWnNH2zCyVONSb4GV+WWW5+fprHTxRcEKNfxig1ajxsnLc2wab/C2D76X3a/5RuqtDTz+xYe5+ZZbJF0b94bGopKUleU5TKMtgbQS5+kLYTyyxpWt6713WC3CaGMM7bEJbr7zVWvOyVWDgW6vR2LMwCETItUmm0uQqCzw0YlxkiSjcAWHjh1i/cw0zWYrTgSVk9WxB31Z2+rjSXlBK+bnL/PM0UPc+w33QaRoKyerhLIcHx3H+8Czx4+xc89uYSXWWIdKK1wuIhOtFEli6fe6grC1xWYZ9UaTkZEpWiPjNFqj1BtjpLUMm6QQN4Ifqs+trEGMonRE6iVPEVTZzVA2bYjRSZlnFcc/KFcMatD+dbWhg0OvXOapx57hiU6fWrBkpib1qt5hUDQINKijjaboe1Q/x/VXyIsehVN0+n1hJoLios/52KETrHQHNJm1ieSGi6JyIAFpUGJib3Rc7OxH3ITBx9pYxWSW8u0374QfWWVOlJJyQicG2KiMLGlg0hqUR83qDGVE0a0DYBK0TVCIkww+dlh0BXmuSVMrB6go6ZWfhISxjevYfv0rqdWl/LToe5Q27L1+F/v27adkqYLy4EsluTgCXxnu1YfPC+S0P0+Irfa9KTBKkL41FpNaRkbGqGdtyW36QJ73eerpp3jHN761Wjul0JOohtbey7URy0aRPOkDD32OG667SU4tjI1GVIwMvYfEWnbt2sUzB59h3/79hCysmfJwUVPgVKCFYzR0SLWDzEaWSxMaNTbfcA/e1FheWZHTNhOLw5HWaqi8T2+lg/d9es7Q6/YIqhMNtQAGG2vhA+XBVIqRsVFuPHAz5sAtaGvlWFutSYyFxKK0JeQFCwsLnH7+1JpzYuK+E9pXbtxEwP3UU0+we89e6WDKwD2rEB1sEDFhGWmKIFgxNz/H4vMnOXDgtqgDCIPVUdm4uCesZe/+6zh88BlckbNu/QZKdybnPQzSL6uNMi+OBmUNrfaEpBeI7c+Jp0KWuhilKtNUto9WJuWVr3w1jz32CH/z6U/y2te+njRNKUGJiWdBEHTUCjq8BVsYcl8QlBHtlJaUnYkHFDktrJXyCq3XLouOpyvH01xtFeRpDCOjY2T1RuW4A74CA8qH6kwU2ZtDLACSMrWYClxJjBoqJkbHdlk+iF0OWppPKS3VJkaJlsoHS6jXo7j85cfx+ToL/Yy5pQt8tdtDzVvqIWFyrE2v0yXv9+gXBanzvHDpCPOHj3Cp22N8rM393/0+dr/uftqTe7lw/oJ00G22IgM12KAqzWCRCO5jqlFp1q3fxIXZc3Q6y0xNzcierMClrwBsKMtE1xhXzwzEPu86sgCi/oz15V4JnUug1qzTqrcpvMflni8/+iXe/uZvxBNLq0p6SxERbaxdDkFK/uLxuZs2b8F7ARwl4jUI9SnlzkKFTk5NUBQ5zx4/zo6du0vp2MuOwnuKSLmKCCcwsW4z4xObGB2fptEaJc3q4mgiAlPKxE5sMfJX8tmlTqJ06JLSGTAYJRMgDX7KvNcwABCnL/s4tswNQU5DK3Jg9Vrwbceeo+gtoZzFkKAdBF9UghoVDL5wzLs5ySHmgTwv6OQrkp4I0uI3eLmOFeNoZQndXK5jQD+KCntwnZJu0VFHISV9MTJRYkT6DnCeC3mPZw6t0cXLy2mTKr6nsgnBWDn9DY1KNegkHhQTH1+QLnzKh9hDPkQxp8YHR95bwSYpSinShmHb9XczsW6fdO8LgcNHDvHoVx9FK83E5Hrk1kKM2lRV4eKj8S8ZnrVGVmtgtOwN5x29ok8gRakCbTSOhEZWo9keBVTVzvWJp59k04YN1Op1efaUDJWJxIcooR0O7ZW0bo6v2rp5C3Nzl5iZXgdlbpG4R6IDTLKMXTt3cuzYUa67/nrMGidJep9DgFE8E2nAGkBJ+sqQ4I1iau8dZI1RYZFqNVaKJc6dO03AMz9/mZH2KIUxbN20nTRN0UbTqDdEAKcMToEPRSQ/pAV2cEUUXopw1eU5AelH0u/3UYWjCLnQps06e27Yv+acGF3uyXhKp1bx/XqcPXOKu+66u8J5JSEnZbFIdBmBuo5BCQp27toVe/hHR66oyt3QoRIKlv3hlVZcd/11HD58hDwv2LxlCwMpdXlIz+o0h/SoF1GjCYr5uXnWbyKekxAdvzJoT6TXSzstTdQEGMh+ufW2Ozl58jgf+6s/4xtedy8TYwKQg1ax9JoItJEUqQkkyuKENqgEtUHrKHBzJM6S5zm9qwDN2sQUc3ReWh4TzVaTZntEVHyUQKB8LgEXS1t91GgJABJbpmOzIk/sEBk7mVZAghDnU0IvjbB/K90OL5w7x8XZWZzLSUxCVq+RZXWpNlllfPxjn8UTJD0YFOc9nL28QMtavM8JeSAPELxjJtMcd4qJsWk+9IH3sv81r2Zk/c2EoPjyo4/w2te8Bm1k3Qz+FHtba7RZmrvI2PT6ai1rBTNT65idPc+ZM6fYuHHzwGeVx+kiz8GsoaWDrwMMaBXVmvFhl07BF0GahyrQScL4+KSkEHzgxJETjLYarFu/USYpBOToXEkVlKIVcaKRkgpC4WkLW3fskO/FHgAlyo0emrLOdHr9evK84NTzz7Fp69bVb7hepznRZnpqHa32BI3GiDiNsvyojOJj5BCqaEJ6BlR+oTrAJf5cqZjDUpE9GDgQhbQFVmXTFkIFZgKexaUFzrxwmrn5OQiBLB69u37Tzavey6XjzyKHxHjJ+wdQPh4yEvUbykPfS5ew3HsKF8i9QwmbKIAggLUK5Q11YzA6sjTRMFSHBcV7Lcv2JEIk3nugnhqpo7aaVCk6zlEUntnu6rXgeVGQpVbAhZWjaZVWKK+waUKwVtTMuqRIPQQnqmYcwRURMSuc1iTakmBRFia2bmXzzttIMgFWzhd89tOf5ktf/TJjk5OcPTfH+OhoXFjigH0VVajK8FwFDpDHI7230N5U2pjCFahCEVYCSZIyNj4dVeySUuv3+zzx1OO85+3vuuI65KjpCLqDqYReoCuD7wls3LhFDsLq54AhhCKa2HgvXgxhvdVk86YtHDl6mH379q16H9oXzKQwkkTDGduDo+UEgdbO/YzO7CQQsB5smtBoNGmNtpm/NM/5c+fpmGUmxicYGx/HGItzubBnMQcstkoO5er3cubmLjM+Ps65F87QbDZpNBp0V7yozq2VgMAmKGPorKygbSK08FpzEpkBo6J4LKZxnnz6Sfbs3oM1ctql2P3o3gNSehrzhypCAkdMl6nAmTOn6Pa77N2zvww8KzNOBAESlMZ9Yiz79u/j0KHDnHruebZs2xL3jq4A0WqjPOKXGIhs3LJVLImWiohKQR9KNiC6SxXipUjnzfLrHdt2MdIe5W8+/QluO3AXu3bsjkeyR8sar11q/PWAoq6euYo9FxTCuxcYIL2KFKf3HkxAe2kK1sm7ZI2M5shILNuGWDBYcSZlkCUwIFQ2KZQpBB8BsmwOOY4e6cvgvLTN7xdObGHhOHb8KCdPPkuWpExPzbBn9x5qTTnzRtJyTpi+VUY7SVhxDu8F1M07z8WVDpuXO6RGgmjvCjoopkabtDfu5Ae/6fXsvfVGxrbeidaWZ0+eJEkMo2NjVfBbHpceCy5ptNo8f/IkoxMzsgfiz7W2TM+sZ2lpkaNHDrJj5x60MTIdFZkZrkpb83WAgXhCl5dKguACGKm71Uby5GPj4yQ2kT7zQfGlLz7Ea171Smxi5TxvwWwYhMXxXjrVxa2DHEIDKniM0hT9Pn/1ib/kTfd/I0mSRMfLFZvGRAX/xq1bOH7kCOfPnV31Pg7c+XpSm8XrtuJQlJauemEoYldQ6qdR4jjj8UPxZ1GENNT2csAI6OpayyUrcWwcQdHtrvDMkWc4eOQgzUaDfbv3c8P+m6jV6pXQba1xYXGRvitwKlC4iAbjJyqjpa97kPLMwqmBMEiJyCkEKDxy8IfXWONJQtyoIZAohfdleqAMmaJjjGg8xLbRQWl6TmhFgJ7zFUDqrHErkmM30iMAEaUpBTaRJjHGRGGXj9cUxX1RNipHuxrJB6dK40JO1qqxZd8BRkc3g07AO5wL/NEf/yEXLs3y1jfdx+/97m/R7fcZjVGRBIKx3bEStqFibuI9rzWCip05rZUW17FCJFEJxmimptZhY3MuT8B6w1eeeJzN6zYwOjIuVxHKatEgp03GDS39LcTwFToMXusCs+fOg4ZmS+rfq/MCypQeYsjHxsfodDucOPHsqvexrWFppPK5SmtpaKUDaEM2s47JLTeikRSSRxpD9XyP5088hweuu+kGsiyTPWKM0LveUOZtJZoTY5n3c/q9FUZaTVqtJrVtOyRKDlCr1aUZTvDUUmHyPFCrZRROUkRrjqgRcEoMnkaEnocOHeS9737vIAILZbBRBjthsGlVECCAQs5egLGxcT7+8b9kz669EZhSqb4VVLn4qmoq2o3de/dy/PBhThw7zo5duwbra409L2DMVPvvqWeeZNeOXazbsCl2RJQ78bGyKkSWq0x3qdIaKfnaKxifnOStb3kXn/rbT3DmhdPcefsrMamlTFQqFcBqkqDlRFPnwAm1bpVoYEr4U6anrgo4KwlUnJZyRqMNkxPrUMYMqjhUqBhjF+eksj8q7v1B7Tkh6iXK4lcdIqvm43kMzpP3exw+coivfOURNm7axJ133k2tVo8dCsU/6Qg4lJJTMlYbP/GWe3hhqcOvffEJZheWWfSOjgsElZN7zZIrWAkFzltcr+D9r7mN3dfvYHrva1G2iXeOT3/67/imd75L9CyUUFL+1OXMqcD2Xbs4c/oU27btqJ5xCZRarTabN2/j0MGn2LlrL2mtRhnYlKfOrjW+rj4D8rA02lgwvkK83jmyRoP26FgsUwucPvsCCwuX2bx1+6BJSolSqk1HpXiFeMiJl8n3GrIso9frc/zYMa677vpK3R50bJ8bI22NOKZdu3bx2OOPrXoPrZERjJF8skKONvXR4FfGQOlqEkp638Bgg5U91aPjp2Isyo3w0g1RNh+aX5zns1/4LMdOHufADTfztvvfSrM5Imh/iCa8mta3C72CFZdHSltjg2g3rJI7IwglqoJElCqKvqpjhb3CBE+OwkRarR6jCIgNg+LVlIANVd5LjDhDfA5Ss4hWkFg5lCm1mmatxkR99UNx0kTOgwhGToAUpydRgXPR6ZWCIRNNd/A4J3oTicCVNHyqwfqtO9mw9SYS24jzGlhZXOHDv/1hgoK3vOEN/PJ/+QWOnzxDvwh88aHPsXnjZhrNkQrUyVINL9p0a4OBklrVGqwVsKm9dH+bmJ6m0WyJw/DyvJa6yzz61Ud5/3u+WWKg0hCWf0ZEX+pppLxQUnPlYVAhBC7PX+bRrz7C29/6Tqo9JniGgJxvHlc56zeu58jhI6veR9uWoEgAt+R0NbpRZ92eu7FprQK9ygdOP3+Kg08/yYEDtzIxMyNtuuPva6WkHNcXkR0kRp1w+MmnSWt1du7aJaGtUqSpxXnRqZhSxOnFOXdXupw9d5Zut1uJFNeckxCrlYLkVB2egwcPsm3LpihQjmxACfLLPyNoVnEeVLyXcobGJ8f55vd+MyaxsTqh3MOy54LS1XtX4CKACYF9113HoYMHOXHiGDt37orR4Op7Xs5K0VGrpbnz9ldI2+bq10qQPGARVAyeShBQaSCic/UEsqzGW970Dp586nH+/C8+whvecD+TE1NExjnqEWTOcuNRRY6Kx+tKEBdLzD1YBVdzkiTBEbzCObBaMzYxTb3ellAxXmqZq/HlnghlWBUqZ12elwHClJmgKQ9pc7ECxQePzx3PPneSz3z+75ienOLNb34rWa0mJwP6ECtwTNQLQVlt4tXqTjRcnmW0s8J14ykPLvfo5j3OF47FXk4jS+iHQK4z5qzm7d/0Vu5+1Y1M7Hk1yjbQwDOHDtFqNpiYnIyzP+CWqgAsfjdNMhYW56/wLsMJhZH2KHv2Xs/TTz7B9l27aI+MVpY7XIU/uWow4HwRe3hLI2ETqwjKk7nWr9tUUf0B+PyDn+eWWw6QplmkXMW4lwd3qHIykYUpNbFRbV/RQ/CqV72a50+cjA7Yi/gOoVB9jAyFEpP6y5tuWZ1arzXEQfighJmIzrEEAbLhdYQaZZ5/IAgs6f8SnQ4edbnBSnqHob8D3W6XT332Uzzy1Ud43atey/d/5/dTi6WW5d2qod+5ClaHy05KHkGRxFIzr8uoX+NwWAJFUFilo4PQUoKpwBtxoiZ2UawbS1snpIlcT2oNNZtgjaFVT6hnTbJUMzbaoJm1aNQ1k2Oj1GoNRlqa0ZExarWEsbEm9VqLrJbQbNbEya4ytDJoK+jXF7mIFrVUdmglNdWlZ1FO5sIaREgW5EQ5tKIx2mbz3gOMjIrCmiBnTMxevMQv/df/zP7r9/Put76LBz/7MTr9nHq7QdtmnD5zgcLF41jR8flV2eAK6F0NGKjycyWdqSw2MzRbI4xNrgPEuLoQsCHwxa98iS3r1zM+OiY0py5FapJGqNZUpC3L89HdkEEnBLZs2crf/M0nWVxYpNWQo3BFoCuLyRCq2nOlNLv37l31Nmwi1Q8EEYtqpcEYpq67k3pjQvZCBLBPPfMUx44c5o333U+tUQOlcEYNcu4qpjWMGG5hxANL80vccOPNmMTGHLfkfaUhEWRpQuGkTa4hMPvCeb761a9y4y03S2lcKWBdY6jonORQG+FPH3nkS7z3m78Zq6MgUqnK9pZXWeakKRnDGLmXe10h56IM1goV4CjBZJVOHZqrEhTs2buHZ55+huPHjrFj5841N71IWgLlpfb7XXqLXabWr4t6rGqXCGNEhcdwUZcw9EZIxUB0J1Zx+4Hb2bRhI3/5V3/Gm+59Mxs3ba6CrZLjsFpJR8zosbV3FEqhvEM7TUGOYe0+A8GHKnWaphmN0ZGKVfDBR5gpQm0TgpRdKl31YohxZfQrSgCPl71bPQjEdiwtLfGpv/sEFy/N8obXvZGJyWmcK/Au4OPZE1GiIIDDRGZNhaGUyNcey5eXKfJlttYtDyeKPIeLznFmpWBXLWFmz1be865vJKk1mN42ycT+V1FvbwQgz3M+9pd/zvd97wcpw8/q+TAEBaLpsdayddN2jh55ht17rmP4x8R5b9Sb3HjTLXzlK4+yY9dOJqamYoXL2g7l6pmBSOnp2MtbKRW7UzkmZmbIallcOIqLly9z4vhR3nL/m2ILV1HQuxBJncgeEHsNqCC1qsQoh6DR3uOVZ8vmLWzZtJVyGwpgLykUOVinioKAZA2hRN/roTPvKxlJzA6UJTOlaLGk1KhsQumpr/heGUkyDATin97zlSe+wkf/4n9w0/U38i//j39Bo96unumwAKlUVQ9jw9VGL4ckRuUhbhyl1NDfFpcIlWtqKdZm2EyRNuIRmk1DozFOWrO0RjLatRavbRlMvU29VqfWNLTrcoBP0sjIGtINTCei0FcmQdkMmzXRJkGndbTJpK+CtpGeFXHRasNYhVKJVDVojQpykIqRRpCDCEcp+lFNTbCYuI68zVm/bS+btt2CNmmMMCUGPHzkKL/8//wyb3nbW7j39W/AYLjn9W9n13W3c+zEIZ47/TxbNm2n1Ri5YqKHGYFSWHg1w5qUUsmtQZgBa5mcmRnQgNEoLy4v8eijj/Dt3/qtg9XjQ6R1xZqrKAwrmw0FXzJRpakXw2qM5n3v/RaMTaqmNq4qrULWtYlAOwzRqy8z0iSNz1mDkpx6a+dexmf24pQI0hTw1GNPcvDgU3zTe74Zq0ykJaFsFydtSMSp+tiiOAC+cPzeb/823/m930VWKrZLQz9k8E0SYmtc+NIXH+ZVr3stYxPjlYG7CngWVfWR0dOK8+cukCaGdln3P7T/yi1eJS/DECBDXuur1Is0t/rjP/593vzmt8cDjuIVlQRIdYFi3ssDwYh7fP9113HomYOcOH6cnbt2r34fJQDzAaMDz58+w6HDT/Oed7+36s5XflKpJ1ERFKDKeiUlgJXBvUqQIz/atGET73zru/jon32EN3zDfezYvrN6T6rnouIBdSaq84v4XY+Px4ivNbwrYpmnYnRyCqNShGv2FbhSSipBBIeJjQ5K9BxlQ7SSdfFBAKNT0jPBq4D2cOzEUT7xqb/ihv3X88Z77xd75Fy1NpUbBKbDLlijcHqwV19uLC8tkONoW00j0cwpQw/Fxdxz/cYZ3vk972d0vIkz0Nh1gObI9up3v/rYY0xPTTM1OXXlM4us2dd6jkma8pXHvsqu3fuv1AGUQDZIau32O+7ki198iO3OsW79+mpPrTauvumQ0RUi9qWyX0urz6mpmaoVZsDz8Je+yLZtWxkZHaU8VU1OwYqiOR1pnlBGL3GjhVjjSlSpouPkh4gf5PdcdNwhRHqYQKGk7KQqP3yZ0SsEjZbGQSMlkSGCABU1AXroAVcPu4SjQ3/Jl+HKcD6+dm7hMh/+vd/i/IVzfN93fh/bt+wY/GogCtVU9SyuoCuvwspt2beDsfFR0qzGaDthZGSSel3RbDVImy0adUujNUaaJdQbGUmtQVKTI6O1TUWAldYxSR2MKPYxFmNr4lRNIsI8bdAmjWcMvGiFDl1rGT2EELh0+TKnT59ieXmRJEm4+45XvOx9hPjcpTLVEwopMVI4qQ8OPiqmDc4VWGNxFAQKRibG2HHd3bTb6+VSynXoFH/3wGf4oz/6Qz7wfR/gjgO3gxJjnCQJWzZvZ+PGLaACWidSGimh3RU3V8VE4cUL4msPa8t+GSE+O8X09HrStB6fkKmYpscef4LpyUkmJ6er6S55p0pkisdgoho88lCDcoH4uoD3gVq9IV00vWRTyznxMeVSgcwg7NFqQ1sTy2Al36/HRpnefqA6wlsBJ088y4MPfJ7v/r4PUK/XhkCL9CUJEdiU+1t7yhWC14Z3vutdjI6MIfn8QeQcsRwudpEzJpBYRaPRJEkT0iSNp8GpikJebYjkErlvDw899AVuu/32ik5GCaDSIVRVQ5pSQxDXUxQ7+xh8VEyAttSaIzz00Be47/43Rd0Rg/QCXAEkY7FXjIDlNTfcdCOPfPnLnDlzmttuvmWV+xAbWoKbmZlpLl2ciimJkimKzj+yauVrJXhS1bXElUQZ4Q+btsnJKd77Td/CR/70D7l06VZuO3C7sKURBHplCKqQiFqX6REBfDroNc8iiY8AXzhGZybJanVUPPeiFIfLNZa9AojzFGdG+QqMBoTdVd4j3QelAsHlBZ994DM8/uRjvPMd72Hjhk3kLicviljCGndETPUaQtURs9rvgTX3yeVOnwJPSC1jiWVe9+kHqK2b4k3f8V5GJ6Q3QLpxO2Prbo6shRwI9bG/+gs++D3fV5mVShA55EaqSYnz22w2ec+7v2UABK4IUgYhaZIk3HnX3Tz4wOfRSjOzfv2aU3LVYMBoU3VzUkgu1/mCjZs3obWOeSVPt9vl4S89zLe///3isMuFTyzXKi84DNC1gAwV1aqD14j9DQQfOHb0KNu2byNNsugwZdVLR7mADeCMk5zRqqPMMQ0tPKWl5CxujOF3KLHiFUuitMWqfPiq+r5sqsCTB5/kv/7KL3PXnXfx/d/9QRpZmb8euBgV7690AeUblAKktcaP/N8fIqnVsUkLnSbixG2GsSnKZBiTok0ijIe2qLJjZBkGDI0SGQ/u80V3/eI98TWAUqezwjMHn+bUqeeZmJhk585d7N+/X06TXGVIl0lpd5zEo0e981WeWhq0KDHYxgjTkAS27TvApi03YHVWOXKlFHlR8Ht/+Md8/qEH+NEf+mF2bt+B1GJfqaotDYEqgaQumxgN8sdUeVd/VeyALwTApDbBWEuz3aIxMkpAgK/kUwO9Xp8vPfww73j72+IvCuMh/TSiyJbY+CiU86MGQKBkBzwUwcfT7DxVOWFwcm5FGNDaZSvXq1lciaLSxoTMMnPdK8nSNmVTo5WVJX7v936XD3zvB2g1W1JKpzy21NhoU6nSTZCmYT4afIlUPTMb1mEi5Vw6YvlAYspgoMlRSnHbHXdw8KlneN3rXx+dXnjpuvwaQw0xO4tLi5w5fYq3vfVtlFU/VYpOD+3l+Ix1KOPpWPpKXDeqZCUCr37lq/ibT/2NfL90jqVtqJTxA8YphCAMSlxjKgQO3Horn/vcZ1e9D61EVxC3A5NT09zzmtdFcZ2AMPkipgAYzKE8xHCFbRm+1+r7MagZHxvjW9/3HfzJn32EM2fOcP99b8amSbRUYUCpAyjpNaDkOEZxqmsM5z31eoPRkfHK0sgeKS+ufFZlKricaw9Dr5dujvJAyp4dK90V/vTPPsrK0jLf/q3fzcjIqFQGOA9RHB3ifZTzFCCemVG2J5L0w1r+ZL7oE7xCu0BbOYzRTI6N8YM/+J2s3zgl79NqM7H1Fagh1vqpp58mtZaNGzdV7N2wJxiMlwZfvW6Xhx74PN/wxvuuZAfiHKsgjEotyXjVK1/N5z//ORKbsH56ZtV7ufqzCbRG2nIUeCeHVLTbbUbbE7GURab0iaefIkkMW7ftoIxiSrGHOHFfLaKgIgoKuuraJShcVTemIzr71N99kltvvZ277nolJV6I4vkqmnJKvfjRvWQMSuKGyv/KTarLyPJKg1nZm6HPC+Ufaug1KlDkBR/7xF/wx//jo3zwe7+Pe15xTzzutHyPwFBcUUV3V15jBRVWHRtvfhuKUsb54hsduvChaOB/ZrwEDDGIXgkwPz/HFx56kPMXznHgwG28+U1vjY1MKP3pqkPHKEEpi7OFnFtuhIJUyoO2eAoSm6BVoLVump3776LdnB7kSeMcdns9fuVXf5XDx47yUz/+k2xcvw7lS7mpgMCylW7Z82KwlhwEU90f0SiE2BQrXIVy3Ri5RmMSkjRlYmrdYL3ryuRx8NBhlAls27a9ch7lTOrInpWgUdgRFddeVDuHQdQfvK/+/dSTT5D7nFtuvDUaU19R9yr2gFAvWXEvHfPnj8m5H0ozceNdjE5uxWAqGPuJT3+W667bz9Zt26rfCUNfDVP4Hkd5QmnZt2FlZZn/+vO/yA996Eelw2T5+yEM3ZtQBCUnuWvvHh77yleYX5hnYmJCzg25CvqzAhkKHn/sMfbv20+SpPEzfSX+K2dd6tGHzXI8caBcMxDz3aCCZmpqhm95/7dUQYqP0zwQJQ4uQuazDN/jjwIYY7jnnteseh9ickIFkCCwOD9Pa2QEqfpUVQRpVFmZNXie5R4p92RlwiqAUDK2MneNWp33vftb+ZtPf4rf/N3f4N3veC+j4+PCyKKFFfCilg0RwpZiy7WGMZbRiSmxx+WzCcQ+AuWzi/uz0mxxRXQWSqFiZEZ0CMzNzfG7f/DbjI2O8g/e/+1kWRZPti3k7iqbX3LIqtIhlJ+lvQgTS9nyasNHJqvIC1pWMT3a5mf+yXdz3d6Nop9LLaO77sYkreq5eO/5yP/4CO96+ztio76rG6WdT7OMzzz8Oa67+SbWr98YfzbEC1RAFOpZjdfc8xoe/uLDa4KBq7+SQCWqCIBNLRs2bR4cqRocLi/4zGc/wyvvvltajg45ankPfwUHEnF/NfllXris+dWxqF9rw913vZIHHniA4CTyirIRQYVajhI26mvQ2C95oprqgCEGqL0S+Axud7AAr3yDwQvi3xFMs7yyzM//l1/gz/7yY/zEj/44r33l69B6CAiU7/Cib1xhNNQVf60xXgYIXPGSrwcIqKH/Xv79ytTP0tISH/3oH/E7v/tbbN68lW/71u/k5htvIUvTwe9fBcvR7eeiPsdDSDDWYK1Fp7ZqZqUVJHXLjhvv4Pqb76NRnxiUz8Wc4Uq3xy/98i9z+OgxfvJDP8GGmWmIbZVFGV6yOeWMx3hZhViWpAeqqxidyi+5qGW5ijBUazmr3WjGxifj8axRIhqBX14UfP7zn+PO2+/AWFuBKh/LGj2OsnY6eE+IhxAF52IPeDkpUMqlpMw3RMl3vVHnU5/8W/I8x8d6fhVKUalkkgex1cuP0cntjI5vpz29k037vyEe+SuVREVR8LnPfY573/BGOQXQJvF/OXjGRlbEWmmfmxj5WZIkZElKmqaMtNssLXeYuzhHLa1RSzJqSUotycjSjFqaUs8yGmmNLE1Jk4w0TXnjm97E5z/9GWxipdVxrbbqfQCVrSmc49FHH+HAgVvi/JftlgfVI0A8bKz8dxmgEJ2orAPtkcY2cbHIsgqx2Uv53RjghMHryoBHYhBVOZuAQtu16HVxuj4eJeyKgt/4zQ+zOHcZG89dMFpaXw8Ajq7saUV5V5qYMHCEpStRpcUWm2St4b577+eeV97Dr//Wf+PkyRNRMyb7Wiuw6KqrqjIaY9Z2K41mk3q9GZ+17I0SJFXAMQTKJnQlUCnTOJIe8tX1qwAXLpznv/3arzA9NcO7v+l9ZGkWnfXg0Caj5VwVjWYYAxBKYBQDhthzZi372i8CvcKTe9iUZfzEB97HPXfsRutAMFDfej2NkW1X2OEjR48yf/kiN95wYyVcHYaeaw1jDN/2/u94SdFG9R6l84qjltW451X3rPm+X0dpYag2gjKa9vgYxsg58WU3pKPHjnPhvESHpSsdrLVhRysIrQIKlXeMizAi7PIISnxg7959/OFH/ojTZ06xZfO2yqDpUG5rogZhrbuInxOG/h0vIahBzD48roiMS4MwjMWC4uLcRf7Df/wPrHS7/OxP/VvWT6+/IlIS5CpvUJUwhRIbhWhYymh0CBysOl6MHIavdLDI/meHGr7HIQPpvOORR77In33sY9x/3/284x3fJMftDl9LeNFzW2XkvsCYVIRBWmF1IrqUEI2TgcmNO9i66wC1elPAXEwEl334Ot0Ov/TL/5Xjzz7Lz/zk/8nkxDhSqVGuL1Wdskd5XSE25gkKKrWJRF6+un8iVemuIp4uo39Do9mi1qxTHu1NQMSAAY6dOMHluUvccOMN8r7BDBmiUtwXr0bJ50t5qNxDQPocDwyoRCc6wKYNm+n0Opw6c5otmzcLCKqArh4Sqa5usFVML7V27KHemKYMZlUIHD1ylEa9zsy6Mg8ZGDgQeUqx+LaKZAfC2Gi0TMI73v42OYBKxwr4+HhNKNmLaoMQlOiUZtbNsG/vfr7w2Qd47b1vWBsMl9eiAs8/f4p6ljE+PgmUTcIY2jLx2st9Ooi1ooMvV8jQ2g6VqoClhSXSWkaSZsO7cGCbQwkRiJ9fOuj4fNbc9KG6aqs1GI2ppSwtLzE9PVMB9cHnhCs+v2Korri68quhz64cvY4iPrjx+psYHRnlN37313nnW76Jvfuui0yqfJ5wN5F1u4oxOjlZ3pIAHB9AyRklZcVYCaA1Oq77kpEpS9XlK+UVZ184zYc//Kvsv+563v62d2JNPB0TKgBc8gtWmSg0LOdl8FwqsBwUa+kFQLqnFgpqxnL/W+/j/jfcJho0HzCTU4yuv/UKpsR7z0c++se86f43V+zU1z0U7Nm9t/xy1deVw6wJNL8OZkCHHIXHxB7h4+OThKGHHYC//fTfcNONNzI2Mj6ILmOpVAlbRM+iqyN0oRRxxE5cSoHy0p6yXPpK0Wo2ufd1b+Dc+fPyJlreW4gBaYhUqk1XfT5lcqb6NxEVvjTye8k7RfQ/2DqyvE6fPcWHfuLHwCh+5id+inXT66/8ndLQDX9u9c9o9NRAaV1d5xpDvehfilLrW0nFXgwS/yeGGvwfYHb2Aj/3C/+Rzz/4AP/sn/xzXvPq12KThCty0S+1NauO6fXSSQ2DnKzoPVoZXOiRNCx7D7yCXfteiU0bYjSqGwqAo9Pt8vP/+Rc5fPQoP/mhH2fd5NQQPeoksvelO4/z7CXakDrissNiXM9BFPfSQMbJexC49MKFNe/Faou1honxiaEyszIyDwTv+OznPsuNN95Iq1Wqz+WexOBFYx4GESaU+eUQI6Z43Lb3KFfuPjFjaZqwc9suHn/i8eoRqVh2RQRDVYe51e4jsSRjbSa3314B8hDkkK+//dSneM099whD5ImNpyKLEQOGUD1rJIAogVj12YHX33cvm7ZsFqFhya1TpkcGJYg+9lNwQVzNrXfeQbfT4eknH4dwNX0GNMorHvz857n77rsptUzDbpEIxoZClIpRKIGBCoPXDN0hJYD/xN/8NX/253+KgthYbTjeC9UOvaJleakziOe7rDa0NvH4WrF3Vms2TE1z8eKFKngo2Z+B6HrIxjG0noaubGAzFGUfA7ToqHTUGWll2LJ5Kx/4jg/ypx/7Hzz2+KPR/Q+lXWPp5kCnsNqk6AFrUqbAYmAXYl6/XLvEyF66L/r4v/w8hMDps8/zK//9/2HX7t289S1vk7bfwcVdGwRMU4K4uIeiG/DRh0maUJ6LgLRBeftqI0RwceB1r+C+t98nJdweQpYxtu1ulEoGvhA4+dxzHDz0DK9+1WtelO+/+lHOVbfbYWlp6SWpsvKcB6745LXH1acJCKChn/cYmxiXLn7ayEEV2nD+/HmeevJJXn3Xq4Qaj01oBghTVwK2cjMQm3pEfzlEmZSKfj04e9pY3vTmN3PnHXcRTKg2kHTlKktN9Jo5mHDFnyXtNIhAqgVQ+gwqoD30HoPeACefe5Yf/fEfZWrdND/+w/+asZHxgRGMo3znss0thKo5UygTeqHiRCLKvpoZGV6spXn7+kd40Tt9LQjhveehhx7gx3/yQ9x64Hb+yQ/+cyYnJq9AvV9rXM1SnNmwkyRrEJyKtL4D41i3ZRfX3XofY+NbJcKMBs5RUvuiEfjF//LLPHPoED/5oX/DpvXr4h3EiCWoAQAMnrjH5efSjxlCmSaKwLYyoFFMFEV85154ds17STPLxPQUOtbpDzsXBZw6fZbjx49z1513x+9IzrRcGiFIZOJLpxmdbeGHHbmu1mQZxcjXEim9+pWvohbr3wcxYjnHssrWdKG1hMnrXkGatuV5x2d/+eJFnnzqCW6/806o3h3KXvilE6rSHBVzJ+tJgIwALeccn/jYX/L8cycr7YOAjpjSCBFohCBAIMQcrYF7v/HNnDx2kkNPP7PmnIQQWFxa5OTJZ9m77zp56qE0+zBg2NTAMaAqkfOwIx8IzmI4oCSI0SjuuuNuHnjgQVY6K1WUXlL0lXBXD9lBxHmK013bdql4BoGpDqCBN73xPm64/qboHKM9iWu6hLehutryTgf3XXUMjPa3vA4VbbuKYsQY2bFh/UY++F0f5BOf+jgPffELVatyE9+rOm9mjaE8Q/McV0dMi7n4v/flupO9XHUhjNce8Fy4cJ7//qv/jW3btvG2t78LbY1UoQQGotoQKhDnggjfpVOq7Epf2s6oUQhIoy0T9JrBpUax7fq9vP873otNpVQ+N4Ha9ptJsokrHX4IfPRPPsLrXvtaOf76f3EcOXKYX/jFnxs09Ss/5iVfD0O/1e7lKkcIGu881hjGxqYoRXZlKvVLX/4SY2OjbI5nA+iyLISY/1cqOu6KHhA0GQYRmXzQADWXgh2l5XCOsrZVqaFeB/EmBYSrNbUrFZYPww/ryq/K9wsQjfRQfrl6tef4iaP88Id+mM1bt/Cj//xHaDVbQIzI1WAaXvI5JdAIX3viwot+4+XvpQwHrvz/JUhxzXda5TOCNEz69Q//Gr/1O7/Nj/3Ij3Pv698otNOL3lhRPrSv70Nr7RE2br0OpRX9XofG6Bj7D7yBbTvvILHNajOXZ1uUkUO/cPzKr/46D3/5S/z4j32IrZs2VCDNU/a/j79HWU40gE3VDIXYg7IUZ1UgTVWRifJw/PCxNe8ly2rUGy0EoarKWRBTDw88+CDbt21h3bp1ssh8jHxjHFPmnoGYvI6GPUjlwKB5TQliB3dcRuBbtmzhDa+/V/ZZdKIhOlIRf8Fa66u5ZQejU7tITEJirJw3ohSf/rtPs3vXbsYnJ+XRqaHpVoOIswIhQz9XQw44IEFC3zl+49d+I7YlH6THFOVJg/FcAa0wCqxWpEqTpSnf+La38vQTa5/T7lXg8ccfZ9fOHXIQFKW9kH1aCnbLa64i1BJDRtw4XC6ooj3TsXcBGjZt2czE5ARPPvGEBEA6RslVMFSWL5f6KGJVQqnlWH1O1NB+D3H+R0fHyWq1mBsPVQqzcq5q+PfLz6QCAfI9uUZ01YlgUFpdph4YgIaZdev5gQ/8Yx78wgP87d99UlgqVf6eYs2CLqqYCB+kI2QJdF3JapQthOP3iWvdBQEJwTsuXbrIr/7qf2NifJxvfu/7SGwSGSqoQGrJkJT73cfUdezgOfA/EXDKyhT7rSQNvdpobFrP9/7Ad9BuSetttMKu20ZrYu9LIv8LFy7w0EMP8Kb73nw1GsvVh4Jdu3bz7LPPcvbsmf/FN5Nx1WCgCAEfFOPTM0L9RxTvg6Pb7/CFh77ArQcOyMEylBQrXOEdhihEIiqU+5InIxswDHI1auCUS/IwlMBCDZDokBbwKtIEcSEO5YOG4UgYur6Bhy3N2+C1x44f5Yf/9YfYun07P/ovf4Rmoy2OpKIGB9RUFUuEYZAQidAh5/1S+nH18VKgMRhDru4q3ufFdJJcfwiBi5dm+amf/gmeP/U8//5n/wPbtm4b0JwvevfqqsPQJ18FMDBKMzazge033M6+u76BGw68kUZrgvLQqHIhlGBMTgkL/MFHP8rHPv5X/NA/+6fs2bGjWizlJh9wLeUzKiNrYjvfMnGgKuGX8mUcFZUoEXX7EDh26uiaz3J0fGLQ1S0+BwmUApcvXebxxx/jla94ZdRghAHgrK5dVYLGAQNVbgRVAYhBpEx1DwP6P0QcUO6cK9dIqYBebcxsv0tadhsVz5g35M7xqb/5FG94473x9DyhrHUUjWlj5BwCHRvSmPjzeLKdNgpjFCaeN2Gs4XXf8DqeOXiIM2dOS28DbarPLSl0FRudmZheNBGyJUnCu7/lfWvOSfCBhx96iLvuvlu+ocr+8yWAvTIgGbYPJZgrHR0RBFR0ulJCpWtDYi33vv6NHHzmoPyuBqVDiRXEToUBuBgODIIPa0rXvQ/SpnkY3BGGTNXwpKqBDqUMvmIAVm3KCh3EL+N6rZ5DBS6gWq3xnxPjk/zAB/4xzxw8yEf+9I/J877Y/VgBttZwpf2v0gFxxw2nAohMQXBxvcuNOwJzi/P82q/9KkrBt/2DbydNa9X1lT0qilBUjJV0nPWREQhRhB7btKOqigpZCgN2ay0FxPf8o+9i/cYJwMv2bLQY3XT7S1IlIQQ+9vGPsWfPXjZt2sT/Wogmo9Focscdd/DQFx68Yuq/zlisGlcNBhQaZTStkTGcj2pnLyVPzz57krNnz3LrbbcNEGkoXaCsVM+gvnPw8xIVD7tdJXXlQw9TEQ0YZb9A2Z6VSrfczCqs+RCqmDBuqCHmvtJZl2s+MFTVWqGSwIlnj/OhH/8Q69bP8KF/9WOMNEcqkDCMxAdOqHz/ENNdpdOMUQYDRzfofLb2nFwBGiKKH14JVwcpXua9Q+D4iWP8ix/6Z2zfsYMf++EPMdIeYRjSDu6tBE9D1/J1fLaxCa32GBu3Xsf0zA7KbpRQzo+LNeoqHlOq+fwXHuI3PvxbfM93fSd33n4r8VVxnQld7igk5RDnL3hdzbUryzcjBTkAqOXfgzuQDoTw1ne+f817sTYDJSmCUpCqoiv58iOP0Gg22btnT/mQhsqXyvhQGAw1HN2FgSLax/tU8WceKqakpOMHpyyqofsYrGFZ96ubOZO0B45GfpujRw6zvLLMdddfP0SBq8iAlJS3Qmtivf1wZKyG1ufAOY2Nj/LOt7+No0cPD+Bl5YhLSK1iZK4qh1yygHaN8+YBLl2cZXFxgU1btg6i8lACtrJEtnrigDBCurJhZdirKgX3AKOqijHQaF59z2v4tu/4TogVN+Koy69j3tvL0dsuhNhYyUdbs/qchOAozxUokOCiKAqeeOwx+v1edOoDG3CFSSgrJuIT9VWJ84s+g+gEqzSPr86TqMKbIG/cGhnh+773+7l06RK//pu/xkqnI3buKjyRpkzBRkYgeKl+KVkZfGyeFa/HU0X13U6H3/6t3+Ty3GW+8zu+h1ZrrEozlYewUeKMIH04Cu8JLjIL8T4jRxOZw9JolscjDwD6auOGfVvl2G0CmIT2ttswtjl4v/hZCwsL/MXH/oJvete7r05TcRVDKcV3fNt3c999b3rJz/5nLP/VMwMuZ3x8ApRBOeJDlcXyhS88wMYN69i4cXOlbC5/RqCa0EAZ8Qw7qsEDV+Vro8UaFuCUKtpAwBeewrlqc5WbWtDdWo9hUCp0ZTxAXOeD6Kq0nOX1euV57tRzfOjHf5TGSJOf/NBPMDY6HnNNpeG/4qOGgM/we70IKMTXDsQ0V1MJTrllqsV9BYr5XxghBL7y1Uf4oR/5l7zlLW/ng9/7QdIsvQIIvMwFxaFe5vtfezTaIyQ2peRTghpEPHI75fOVcfjoUX7uP/0cr3/da3jHW94OxANighsI0UIgDPeJ94Oawaoj39D1llFAUCHmM6mMeSnI2rF119rPTsUOmXogyArASqfDw198mDtuuzWWww2Asao+q7zhMjIZiEqrtRV7CvhoPImGzCO1bhLYlPcY5yIMGJUSLKwFm8u9G/8BIfDXH/9r7r7zLpoxHVZuJF1F1sPONO6aMFiQpfG9gtFSmn/wnd/OvffeL79Z6YliBBsdv67ahJeCtfL7a3uexx57nN17dpOkaSUUDnH+yyDFl9car1eefAkUy6Y3JaAagEfty9ROnA/lsdbEfLUETBWQC54iOHyku5UPkfKOQHaNW5HUA1FPJc+q1+/z33/7w5x74Wz1TL4mCxDEhlbMURjMWAkA5L5dNecleBiYlYHTlOPmA/Vane/5ru+j0Wjwn3/p57l4afaqwtIrQLf3lTYmeNEnhcg6lzmuEBwFjn7R48//7E84euQw/+Dbvp2p6XW4UFTP1PuAd6I5CN7FJkMevItsgeyZkjUs19ywuLUMGIaZkJcdKlYZaYuZ3kqtveVr3Sx/95lP06jXuOnGm/+nhYNfazSajXj08f/6e119NYFSjLTHCM5TUKJGWFxc5JFHHuHuu+7C2qSiZYjo0gUVRUCl0nhQyu2HFmO0DJShyBCDxeAYTpm+z/zt3/I7v/ObuOAoa07LdKRf46kMPm/I3V7p+68MbNXg6/PnzvFvfvLH8UrzMz/xM0xPTg/fwRWLaJiypfr30IcNfXioNuqgnvZKyu/l76V6o5cFAV8fMnDO8Ym//jj/5if/DR/8wD/kfe95n5zyWMVog1F9PVxJQGl+KnpiTdsgLUSJjyUaMz2cXR6YovOXZ/nZ//v/Ympmhn/4ge8jsbEzZtDVugrxzYTmj45eDZz6wCGU3ysflXzf6Zi+Kh9tlaK4imc5BEiVHpTaPXPwIIsLC9x62+0C9sKA1i8FTz5cKZwq2TABMAyitFJYxcDxiyEt67Mj4xAjrQHoUIPHuVae4IoJVSzML/DAgw/whnvfINqfochzoNcZiv51SfMz9P2BBqA6v6EUrJVpgfL7lO87YAjUUKirlQjprsaAPfzQQ9x++x0MWJaSJvZXPJ/Y6YIhFBplG8NRshfKOs6TCw4fez+4ISdWiT2rKosy9031mXKIlGwQg8KsVe6pDUaJZsIaSVc0anXGxyc4debMQABYcqflc+fKwEeYoRIsUNXqVz0QwkBQO7zk1dAfqmRPUKSJ5f3v+wfcestt/Nwv/F+cfu7UmnMyqIqRNV02zioBgsNHFrWsnAngAg8/+BCf+9xneds73s7OnbtwIReAXMQmVd5R+CLOC/gICEpgrcrKlxKsMtC5KF8+hyFaYY2htUVbi2o1aG+4UZ7Ni3xQp9PhD/7w93nn299Jlq1+guv/rvFiW30146rBwPT0BlDmCkTnCTz11FMsLy1x8823UEVzIRCGInVZiKqKYgfdn4YwQOX9ywUc871XrEl5j4n1Uzzw4BeYn1sogTqlCV6rI9mgxG/grITeH/xedfZzBRACl+Yu8VM//VNcuDjLz/zE/8mm9Zsoo4sKRQ478ooJGRIKhhIRD2Kj8un42I534KLWHgKGrgQjL777rwcKFEXB7/7e7/CffvHn+dEf+RBvvv/NYvhfZlRRRvmNKggZiiqu6j5E3SvPq4x25dpVKGvjYaXb5z/9wi8xe2GWf/Uv/hkjoyNxfQTKhlYlwCzXKISh7nexQjlImsApYbdUKFkshavWXElWDjcMWntWKhOr4soPgcLlPPjgA+zauYOpqSlKhDT8dAYVBPFEhuDEgPkyPeCqnKqLCuzhYtSSfXPxGlSAJ776OL/9m79JUeRDybjI1q21T6roW77+8pe/RC3L2LNnb/QHsUPccGoqvrai8mP6oHTi5d9ecQVwqI4Fj9/zaiAeFunlAHnIr+grJBRrjfMvnGPr1m2Vky/FlhUgLBWWIYoHS0dVAS9ZXj6KUasoNIIw5504n2pOyvn0FRtTalgGrEesn49gyCi9dpMURfX68u8kSdm8dQsXLl6MgsbyJFldrT81+PVq7YXqP4b2kPy47NkShn632k8hVAAwRDG4QmGN5U33fSPveec38x9//v+79qQEOT7+SoYqeoogDEEZVHkUjsCJ48f56J98hAMHbuXuV7xK9rsvU20hig5lHsvWzD4QAUfpl6ow4crmRgFC9G8VQ13a61WnRBOspbHlAMY2XwIEQgh84eEvMDd/mde//t7/LRH8i0cIctbBi6sK5Pquflw1GGiOjVAdDlJulsLx4IMPsG3bNmbWbajQcGmkKp1zhcKg9JVl97jS0Aoy90MTEN8hhumBuABR7Ny5G2stTz31RLU4ywMg16YNKw9/xTUN9kLczCViDbC4NM/P/vt/x8Ejh/mJf/OT7Nq5h9LN+iEqtyqlCuXvDlGPVZnSUJRZoc/Scfhq06k1FiEQa7eHn/VLb7MCSWu8V1EU/M7v/ia//4e/z7/76X/HPa+6pwIC4SX/D5zQUBxMSSle8VCvapSb0lFF6rHpR2nsXeH41Q9/mAe/+BD/9P/zj9m1Y4dcmxqo5StqO5QGmCoik2+7CDpKQ1A6UHHCsr7LvGXFUFZU1lqGQe5E/tND03v69FmOnzjGXXffLS1jK/ASf2nI8IbgpGqgij4ja+ClFC/8/9u70t+qjiv+m/ueFxZDoaylFAjGKKyxES0QKFgJVGqVQKuwtPnYRhSRVumSSlXb/yBbKUVNxYe2NCIJSxMFgkiTAmFJYmOJLYABYeGEtKVJQWAbY787px/mnDPjV+P7UuiHyPOTDPZb7nszc+ac35zt2lQ9BKQnUfmi4BCHM0JVVUPQdPwYPvnkqiM6FMRgs+LTvJw5uLu87Xl9DxYsXIDKgQP8GOHbxIo3yBHrIiUkxh9Q4+FPqkyMCLh18ybar99gxwKn7SVc3qVkI9FseL14BmbdNwsVXGop8W+vBqwaHe+9c7AQF7+8hrv/qQ4LvDcS3mP56+zowKnjx2HT1N3kJ3HGU75/jnukgBvskOFcqT6QgxRjkU9gTAzGjB6NS60tbooSSbrlUJiRZkCysPC6Ljz9aqMCeFmC31cE0jwDzX2QnW9yAJOTuXO+grXfXZu5JoS0RwKkI7oF9raEoTCnF65fu4oXtm7B0KFD8PDyFcjlyiDBL0d4rF8f68LYjqClPBOhnuZ9T6naMkB0BT9v2S5leNBSkyIZ8QVUDv5ir+7/QqGAHTu24YH6egwfNhyf/ryeDSLC7zZuwMGDB0rSUbdDyWTAuS6lrMMRgn9euYLTZ09j7ty5MLl84BXw5SLq8mT3jEilChon1EkcV4xhSt7gSIUAsatw4MCBmDF9OhoaG9x1VRUVM81exhEy4qIfPTmQ7xJ481YHnt7wHA6/cwRPPvlTzKmdAzHrMhwdt7iflOSIquS/PTdXpem+kzsNa8UEKU3pEzpHpNSpDxN8++ulaYodf9mGbTu345e/+BXm1M3BHVPY4ItkjYQs+fkU5cOhJcCiYLux7dVX8fKOHXh0zRosXvRVSGq2rr1JkM+7expI7wirKYXiJjQAXLMTF2JneYN8llPopgcBcGSi0NWFq//ObjoEyOldmqQAjY2NqKqqwtSaqTwtPrMlTCK3LHsWQgKYOIZx50DGbI/1dzd8EvkxZDBm7FhUVlTi7Lmz0FMw+fBcJvgE+I+P/o4Tx09gSf0S9giQeg3kZB9W8fjTerBvQc4Y6izJ87ybDLD/rX34yRM/RmdXp3oWYAySRDL4xesg5XnFrKN3LF5SzzxVvHCO+KW87j43wydpeg8fH9jFblKQZkgh0SYtU0xMgvaODjz1zFO42NLCY0k02dKV8IW6yiix6AtcSKIyauDIWvWEiRhUWaFGTfSggfdyAeihj0SHhd8fcnjRx0nfKw2XlDzwdayB3ldDQrXTZ8zOXhRKgNA+oCcBSEUnWkJ3Vzd27NyOKx//CytXrsagIUMRqFm3F2QIgHpykArh55Qh68VFD1Jsb0iv4bwQ4o3wuTe3GUZFBapGz+w1KZCIcLb5LE6cPoVvfP2h/4tXAHDEePCw4dj5yisuLFL8PEqjIKX3GZDEDnIxGJMCTU2NABFmzpytLhqJeRIzvBSpxkJTjv/YYuYHckqQLApIAxepCUiId8tZItxXV4fms824du2qEwBlmKU42EXU5XcR/KDGnwy6urvw/ObfY/eu3fj+2rVYuuRB/1plMvACBXFrEfyJnR20AbkR6ZP3hLWu7nU22HQZi8fKHQgEPfwjYPC9XdFai917duEPf/oj1q97HPO+PC8gAv9NLYR2Fc/lbVGCJAppdCdiSepitz0RDh5+D89v3oz7F8zDd1atdi5V3viGLBKTIFeeQy5vkOQT5POuEZY0dpG5dl0FU54zyeBmueHHvKFM+fWOrH1w6SJ+9vMf9j0QyDW0cAnX227g2LFjmDV7FgYMHAhX5eDWV/4PCZ2SQXIJq7pPgjwBcXkaacjDcuOcaH6tyysrUV1djZMnTzI5QUBw+l4Ur7gIhw4ewojhwzC5mqsgTNAPgEQe+F+TuNvYipRwCICMGECoEZdQgXSxq5s7B+cvXEBTQ6Pz8CU98wRg3H3qQydUKZh4z6QeBlxIGkuQGw+4ERHPU0IcppSQhbxfftjwgbufSlllLsnD5BKMHDUS0+6djrffPuBzHnhOXHfVRMmZkLs0Y0R+3/mTPsFixr0z8Nj31iGfD26+BNFw3nirpmM5CqMSRMSZ+PwW3TMii6TrKQScAj1hDMEnLmYj5f1NEP3On8NJg0QW3RwuO3LoIJqajmLpA0sxafIUNuSpGvMUKXccDImzhHI4rGP8/UXcvrfqnRWd68mFURuUZrQkrhw/HfmyoehNyRERtu3cjmlTp6F68pReX3M3YIzB/Pnzcab5DD4q6jnQk/D1jZLJQHfajYLWfHbjVlcnGhrexaR7JmHYsOFKBsRYp1wEbSy4oQiUZxJJqaAk2UhNN7xBIDGlUpHgFs5ap8xrqqcCAE69f8pv7lJcJKSEv8hQOhFRz4Qt4MXtW7Hlz1vwrUdWYOUjqxz7U2Xq/QtOgaeQG2qo/MimEhPDgqmfGlRWqGIIBDQLIe0JP7IkOkRu47zx5l5s2rQJ317zKJY9+LWMzey9Kr0912P+Szy16XcXUqjZ1U4xXLjYgqd/82uMHz8OT6xfj8qKcgjNIkpBxiIpc4ctCzmduVp2d+Mcd9MgI5qPXHGeXxMJWQUnFGemfQMUMujs7MLHV29ljsW7jN0cnz79Pm603UBdbS00Yx2AeIGEyDmiK4aB5Yi9E1bb/ELJpttDRmOi7ulUwwCu2ifFlJopaLl4EW1tbawkZX/1LSUiw93dBbzx172Yd/98DKgc4M6c5M+Xcuc4NdrwiYAaw0aQDMrehjCxTWRgxJhRWFy/GFtf2IruQpfv0wDv9/PvkVbl2SpMyZYQLN3tBgiqC7z0Gt8HANCyRiUhnBSZJEnQZyGPMu7a5x7PYeGiRWhoaEB7eztEm3ljRIG3yuu3vuAlVw4Ybt1zSQ75XFkgw0KUe86BH5LxY1bvmPFVLToX1GPuCMSVKBZW+wmw94fnR/ZgJowNPssJpU3deDR50BJaLl7Aa7teQ83UGiyprwfUa+ZtDaxxW8b6UIHYD8PEwljvPQD5vS/ypV5JfjwkuX2h6nNTb+tF/fDyZezb/zeseHh5SSWwd4IJX5qAUSNG4uChQyXZjt7wKcIE1t0x0BIKltD6YStaW1tRW1sLSgzHMJkIpG7BCvyjXaTIu16UlekCehWtLiIgWAu3qJaV/eChQ1BTU4PG9951MSaOo2UZIHH/+1OYnMSFpDhB2X/oADb+diPq6mqx7rF1KMuV+9cLhSQRIkk2U++yvz4Cr4F+CZK3eyMYeA7ES5EFE/zmfQvwpyaV5d6F+sg7h/Hcs89g2bJlWLNqNXK5HH9+iVBW0/d3zbqeDdyz6rAlixsd7Xh24wa0tbXhR4//ACM+P5KVFs+9IS7rJJAFEut60KcmVQ+RMYab35Qhl8vB5AOXLX+6NH/ipmMQz42UX1kQOtMCYLJv9gHrx1CwKY42NWHc2DEYO3acxquFDBOBkxelOyJ5ow8gBTjQIfvC75uUEwpVHsk6ciAzzteYOHkybt7sROsHlzhhUmQ9K2fAyczly5fRfKYZCxct0lYbMDL3xe8B1OIYLx7FyX42uI487sLnCZZ/cwWONjbh3Llz/rrBNaQzacm+T0BdwIZ5lifjpCFh791zn0hkVVf4LeTGJQl6SfjDnYVczN6dnqfPmInOW504f+G8DkJ0nMinD1cQl2j2MQ4rmkXPtbzuFpB+/jJjxFLDsqz7Ssp24Y8y4iMr+ACJPm6LHhO5Eo+Ub58uYyJkiJbOt+HvqSd9AyUnhgg329rx8ksvgSjFQ8tXoEyq1USnsnctJdkTjrhL/wY3t9bn4Ki3AO7cZ9kboiSA32NkRpBtWI3Pqyoe3+t7d2PQgAFYMH/BXS0n7A3l5eVYuHAh3tr3JgqFwv90DUN3knEQERERERER8ZnH3WmFFBEREREREfGZRSQDERERERER/RyRDERERERERPRzRDIQERERERHRzxHJQERERERERD9HJAMRERERERH9HJEMRERERERE9HNEMhAREREREdHPEclAREREREREP8d/AINKvHsmQmeTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1,10)\n",
    "\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset.take(1):  # Take one batch\n",
    "    for image in image_batch:  # Iterate through images in the batch\n",
    "        if i < 10:  # Only display the first 5 images\n",
    "            print('image shape: ', np.shape(image))\n",
    "            tf.print('label:', label_batch[i])  # Print label for the corresponding image\n",
    "            axarr[i].imshow(image)\n",
    "            axarr[i].axis('off')\n",
    "            i += 1\n",
    "        else:\n",
    "            break  # Stop after displaying 5 images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmetmzNHTWzU"
   },
   "source": [
    "# 2) CLASSIFICATION SPEED Model Building - MobNetV3Small Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48RHLVshdX5L"
   },
   "source": [
    "### 2a) Set up model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aHjqXG1jSnCr"
   },
   "outputs": [],
   "source": [
    "dropoutrate = 0.2\n",
    "input_shape = (224,224,3)\n",
    "num_classes = 1 # we're only predicting the prob of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ZPso3wBuN9L3",
    "outputId": "ef11a9ff-7117-4836-dd78-9bcadac15995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 576)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               147712    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1130097 (4.31 MB)\n",
      "Trainable params: 190977 (746.00 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mobnetv3small.trainable = False  # freeze mobnetv3small layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ca0JFQuuN8oI"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) train the model with the mobnetv3small layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WPTNOtr7WjLS",
    "outputId": "63cf5fac-6100-43db-dadd-47da686c9712",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714/714 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7587 - auc: 0.5304\n",
      "Epoch 1: val_loss improved from inf to 0.54628, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 1, Loss: 0.5554, Val Loss: 0.5463\n",
      "714/714 [==============================] - 73s 89ms/step - loss: 0.5554 - accuracy: 0.7587 - auc: 0.5304 - val_loss: 0.5463 - val_accuracy: 0.7547 - val_auc: 0.7527\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apyba3/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714/714 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7592 - auc: 0.5995\n",
      "Epoch 2: val_loss improved from 0.54628 to 0.54185, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 2, Loss: 0.5409, Val Loss: 0.5418\n",
      "714/714 [==============================] - 70s 88ms/step - loss: 0.5409 - accuracy: 0.7592 - auc: 0.5995 - val_loss: 0.5418 - val_accuracy: 0.7607 - val_auc: 0.7913\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.7604 - auc: 0.6598\n",
      "Epoch 3: val_loss improved from 0.54185 to 0.50551, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 3, Loss: 0.5232, Val Loss: 0.5055\n",
      "714/714 [==============================] - 67s 86ms/step - loss: 0.5232 - accuracy: 0.7604 - auc: 0.6598 - val_loss: 0.5055 - val_accuracy: 0.7614 - val_auc: 0.8287\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.7644 - auc: 0.6895\n",
      "Epoch 4: val_loss improved from 0.50551 to 0.49109, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 4, Loss: 0.5103, Val Loss: 0.4911\n",
      "714/714 [==============================] - 67s 85ms/step - loss: 0.5103 - accuracy: 0.7644 - auc: 0.6895 - val_loss: 0.4911 - val_accuracy: 0.7593 - val_auc: 0.8331\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.7637 - auc: 0.6974\n",
      "Epoch 5: val_loss improved from 0.49109 to 0.48171, saving model to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/mobnetv3smallcheckpoint.h5\n",
      "Completed Epoch 5, Loss: 0.5078, Val Loss: 0.4817\n",
      "714/714 [==============================] - 67s 86ms/step - loss: 0.5078 - accuracy: 0.7637 - auc: 0.6974 - val_loss: 0.4817 - val_accuracy: 0.7624 - val_auc: 0.8459\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory and create it if it doesn't exist\n",
    "checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint filepath with keras extension (more compatible with TF 2.13)\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, '50epochs_frozen_mobnetv3smallcheckpoint.keras')\n",
    "\n",
    "# Create ModelCheckpoint with compatible parameters for TF 2.13\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    "    # No save_weights_only parameter\n",
    ")\n",
    "\n",
    "# Define epoch callback\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/models/mobnetv3small_classification_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_path = '/home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/50epochs_frozen_mobnetv3small_classification_model.keras'\n",
    "model.save(model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiHy6opSP2sQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot-1/MobNetV3Small_Kaggle/weights/50epochs_frozentraining_classification_mobnetv3small.weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpLHyw20P93U"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #Clear keras session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENHbUvQdvyFe"
   },
   "source": [
    "### 2d) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ek_ytyw0KB"
   },
   "source": [
    "rebuild model after clearing keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> \n",
       "\n",
       " global_average_pooling2d         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)             \u001b[38;5;34m939,120\u001b[0m \n",
       "\n",
       " global_average_pooling2d         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                         \u001b[38;5;34m577\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,697</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m939,697\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">927,585</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m927,585\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,112</span> (47.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,112\u001b[0m (47.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mobnetv3small = tf.keras.applications.MobileNetV3Small(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  mobnetv3small,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mobnetv3small.trainable = True  # UNfreeze mobnetv3small layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # deliberately smaller learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load in the learned weights from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oAenzEiP-C-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apyba3/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('/home/apyba3/PICAR-autopilot-1/MobNetV3Small_Kaggle/weights/50epochs_frozentraining_classification_mobnetv3small.weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDtRxBow89t"
   },
   "source": [
    "Initiate fine-tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.8811 - auc: 0.8638 - loss: 1.4939  \n",
      "Epoch 1: val_loss improved from inf to 4.26729, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 1, Loss: 0.6943, Val Loss: 4.2673\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 435ms/step - accuracy: 0.8811 - auc: 0.8639 - loss: 1.4928 - val_accuracy: 0.2455 - val_auc: 0.5288 - val_loss: 4.2673\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9601 - auc: 0.9798 - loss: 0.1693  \n",
      "Epoch 2: val_loss improved from 4.26729 to 2.47814, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 2, Loss: 0.1417, Val Loss: 2.4781\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9601 - auc: 0.9798 - loss: 0.1692 - val_accuracy: 0.2459 - val_auc: 0.5861 - val_loss: 2.4781\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9696 - auc: 0.9926 - loss: 0.0851  \n",
      "Epoch 3: val_loss did not improve from 2.47814\n",
      "Completed Epoch 3, Loss: 0.0857, Val Loss: 3.0127\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9696 - auc: 0.9926 - loss: 0.0851 - val_accuracy: 0.7562 - val_auc: 0.5526 - val_loss: 3.0127\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9799 - auc: 0.9965 - loss: 0.0550  \n",
      "Epoch 4: val_loss improved from 2.47814 to 0.80931, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 4, Loss: 0.0566, Val Loss: 0.8093\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9799 - auc: 0.9965 - loss: 0.0550 - val_accuracy: 0.7510 - val_auc: 0.6593 - val_loss: 0.8093\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9869 - auc: 0.9976 - loss: 0.0395  \n",
      "Epoch 5: val_loss did not improve from 0.80931\n",
      "Completed Epoch 5, Loss: 0.0406, Val Loss: 2.4576\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9869 - auc: 0.9976 - loss: 0.0395 - val_accuracy: 0.5366 - val_auc: 0.7188 - val_loss: 2.4576\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9866 - auc: 0.9972 - loss: 0.0369  \n",
      "Epoch 6: val_loss did not improve from 0.80931\n",
      "Completed Epoch 6, Loss: 0.0388, Val Loss: 7.5126\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9866 - auc: 0.9972 - loss: 0.0369 - val_accuracy: 0.7618 - val_auc: 0.5070 - val_loss: 7.5126\n",
      "\n",
      "Starting Epoch 7...\n",
      "Epoch 7/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9895 - auc: 0.9987 - loss: 0.0267  \n",
      "Epoch 7: val_loss did not improve from 0.80931\n",
      "Completed Epoch 7, Loss: 0.0296, Val Loss: 1.9602\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 430ms/step - accuracy: 0.9895 - auc: 0.9987 - loss: 0.0267 - val_accuracy: 0.7909 - val_auc: 0.7166 - val_loss: 1.9602\n",
      "\n",
      "Starting Epoch 8...\n",
      "Epoch 8/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9909 - auc: 0.9986 - loss: 0.0257  \n",
      "Epoch 8: val_loss improved from 0.80931 to 0.59846, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 8, Loss: 0.0260, Val Loss: 0.5985\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9909 - auc: 0.9986 - loss: 0.0257 - val_accuracy: 0.8722 - val_auc: 0.9108 - val_loss: 0.5985\n",
      "\n",
      "Starting Epoch 9...\n",
      "Epoch 9/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9937 - auc: 0.9992 - loss: 0.0182  \n",
      "Epoch 9: val_loss improved from 0.59846 to 0.40243, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 9, Loss: 0.0219, Val Loss: 0.4024\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9937 - auc: 0.9992 - loss: 0.0182 - val_accuracy: 0.9306 - val_auc: 0.9315 - val_loss: 0.4024\n",
      "\n",
      "Starting Epoch 10...\n",
      "Epoch 10/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9925 - auc: 0.9988 - loss: 0.0220  \n",
      "Epoch 10: val_loss did not improve from 0.40243\n",
      "Completed Epoch 10, Loss: 0.0233, Val Loss: 1.2107\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9925 - auc: 0.9988 - loss: 0.0220 - val_accuracy: 0.7191 - val_auc: 0.7763 - val_loss: 1.2107\n",
      "\n",
      "Starting Epoch 11...\n",
      "Epoch 11/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0162  \n",
      "Epoch 11: val_loss did not improve from 0.40243\n",
      "Completed Epoch 11, Loss: 0.0177, Val Loss: 1.6019\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0162 - val_accuracy: 0.8294 - val_auc: 0.7638 - val_loss: 1.6019\n",
      "\n",
      "Starting Epoch 12...\n",
      "Epoch 12/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9943 - auc: 0.9993 - loss: 0.0149  \n",
      "Epoch 12: val_loss did not improve from 0.40243\n",
      "Completed Epoch 12, Loss: 0.0131, Val Loss: 0.5371\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9943 - auc: 0.9993 - loss: 0.0149 - val_accuracy: 0.9282 - val_auc: 0.9276 - val_loss: 0.5371\n",
      "\n",
      "Starting Epoch 13...\n",
      "Epoch 13/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9945 - auc: 0.9990 - loss: 0.0164  \n",
      "Epoch 13: val_loss did not improve from 0.40243\n",
      "Completed Epoch 13, Loss: 0.0171, Val Loss: 1.5137\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9945 - auc: 0.9990 - loss: 0.0164 - val_accuracy: 0.7138 - val_auc: 0.5557 - val_loss: 1.5137\n",
      "\n",
      "Starting Epoch 14...\n",
      "Epoch 14/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9953 - auc: 0.9994 - loss: 0.0127  \n",
      "Epoch 14: val_loss did not improve from 0.40243\n",
      "Completed Epoch 14, Loss: 0.0134, Val Loss: 1.2607\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9953 - auc: 0.9994 - loss: 0.0127 - val_accuracy: 0.8802 - val_auc: 0.8340 - val_loss: 1.2607\n",
      "\n",
      "Starting Epoch 15...\n",
      "Epoch 15/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9959 - auc: 0.9992 - loss: 0.0102  \n",
      "Epoch 15: val_loss did not improve from 0.40243\n",
      "Completed Epoch 15, Loss: 0.0120, Val Loss: 0.8182\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9959 - auc: 0.9992 - loss: 0.0102 - val_accuracy: 0.7968 - val_auc: 0.7786 - val_loss: 0.8182\n",
      "\n",
      "Starting Epoch 16...\n",
      "Epoch 16/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9951 - auc: 0.9987 - loss: 0.0150  \n",
      "Epoch 16: val_loss did not improve from 0.40243\n",
      "Completed Epoch 16, Loss: 0.0136, Val Loss: 0.6490\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9951 - auc: 0.9987 - loss: 0.0150 - val_accuracy: 0.8900 - val_auc: 0.8970 - val_loss: 0.6490\n",
      "\n",
      "Starting Epoch 17...\n",
      "Epoch 17/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9956 - auc: 0.9993 - loss: 0.0091       \n",
      "Epoch 17: val_loss improved from 0.40243 to 0.38619, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 17, Loss: 0.0103, Val Loss: 0.3862\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9956 - auc: 0.9993 - loss: 0.0091 - val_accuracy: 0.9229 - val_auc: 0.9459 - val_loss: 0.3862\n",
      "\n",
      "Starting Epoch 18...\n",
      "Epoch 18/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9976 - auc: 0.9996 - loss: 0.0066      \n",
      "Epoch 18: val_loss did not improve from 0.38619\n",
      "Completed Epoch 18, Loss: 0.0090, Val Loss: 0.5164\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9976 - auc: 0.9996 - loss: 0.0067 - val_accuracy: 0.9275 - val_auc: 0.9251 - val_loss: 0.5164\n",
      "\n",
      "Starting Epoch 19...\n",
      "Epoch 19/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0081  \n",
      "Epoch 19: val_loss did not improve from 0.38619\n",
      "Completed Epoch 19, Loss: 0.0094, Val Loss: 40.4116\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0081 - val_accuracy: 0.2378 - val_auc: 0.4755 - val_loss: 40.4116\n",
      "\n",
      "Starting Epoch 20...\n",
      "Epoch 20/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0048  \n",
      "Epoch 20: val_loss did not improve from 0.38619\n",
      "Completed Epoch 20, Loss: -0.0018, Val Loss: 7.1915\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: 0.0048 - val_accuracy: 0.2294 - val_auc: 0.5123 - val_loss: 7.1915\n",
      "\n",
      "Starting Epoch 21...\n",
      "Epoch 21/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0097  \n",
      "Epoch 21: val_loss did not improve from 0.38619\n",
      "Completed Epoch 21, Loss: 0.0082, Val Loss: 0.5847\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0097 - val_accuracy: 0.9054 - val_auc: 0.9022 - val_loss: 0.5847\n",
      "\n",
      "Starting Epoch 22...\n",
      "Epoch 22/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9952 - auc: 0.9989 - loss: 0.0247       \n",
      "Epoch 22: val_loss did not improve from 0.38619\n",
      "Completed Epoch 22, Loss: 0.0343, Val Loss: 2.1453\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9952 - auc: 0.9989 - loss: 0.0247 - val_accuracy: 0.8018 - val_auc: 0.7095 - val_loss: 2.1453\n",
      "\n",
      "Starting Epoch 23...\n",
      "Epoch 23/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065  \n",
      "Epoch 23: val_loss improved from 0.38619 to 0.13223, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 23, Loss: 0.0068, Val Loss: 0.1322\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065 - val_accuracy: 0.9713 - val_auc: 0.9789 - val_loss: 0.1322\n",
      "\n",
      "Starting Epoch 24...\n",
      "Epoch 24/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9979 - auc: 0.9996 - loss: 0.0044      \n",
      "Epoch 24: val_loss did not improve from 0.13223\n",
      "Completed Epoch 24, Loss: 0.0053, Val Loss: 0.1820\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9979 - auc: 0.9996 - loss: 0.0044 - val_accuracy: 0.9643 - val_auc: 0.9754 - val_loss: 0.1820\n",
      "\n",
      "Starting Epoch 25...\n",
      "Epoch 25/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0025  \n",
      "Epoch 25: val_loss improved from 0.13223 to 0.04328, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 25, Loss: 0.0009, Val Loss: 0.0433\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0025 - val_accuracy: 0.9874 - val_auc: 0.9954 - val_loss: 0.0433\n",
      "\n",
      "Starting Epoch 26...\n",
      "Epoch 26/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9969 - auc: 0.9997 - loss: 0.0035  \n",
      "Epoch 26: val_loss did not improve from 0.04328\n",
      "Completed Epoch 26, Loss: 0.0069, Val Loss: 0.8471\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9969 - auc: 0.9997 - loss: 0.0035 - val_accuracy: 0.8588 - val_auc: 0.8604 - val_loss: 0.8471\n",
      "\n",
      "Starting Epoch 27...\n",
      "Epoch 27/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: -0.0016     \n",
      "Epoch 27: val_loss did not improve from 0.04328\n",
      "Completed Epoch 27, Loss: -0.0265, Val Loss: 0.2404\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: -0.0017 - val_accuracy: 0.9363 - val_auc: 0.9655 - val_loss: 0.2404\n",
      "\n",
      "Starting Epoch 28...\n",
      "Epoch 28/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9964 - auc: 0.9998 - loss: -0.0059     \n",
      "Epoch 28: val_loss did not improve from 0.04328\n",
      "Completed Epoch 28, Loss: -0.0834, Val Loss: 0.9364\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9964 - auc: 0.9998 - loss: -0.0060 - val_accuracy: 0.8109 - val_auc: 0.8263 - val_loss: 0.9364\n",
      "\n",
      "Starting Epoch 29...\n",
      "Epoch 29/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9906 - auc: 0.9977 - loss: -0.3116  \n",
      "Epoch 29: val_loss did not improve from 0.04328\n",
      "Completed Epoch 29, Loss: -0.0647, Val Loss: 0.2320\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9906 - auc: 0.9977 - loss: -0.3112 - val_accuracy: 0.9762 - val_auc: 0.9795 - val_loss: 0.2320\n",
      "\n",
      "Starting Epoch 30...\n",
      "Epoch 30/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9980 - auc: 0.9997 - loss: -0.0182      \n",
      "Epoch 30: val_loss did not improve from 0.04328\n",
      "Completed Epoch 30, Loss: -0.0500, Val Loss: 1.1723\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9980 - auc: 0.9997 - loss: -0.0182 - val_accuracy: 0.6953 - val_auc: 0.7438 - val_loss: 1.1723\n",
      "\n",
      "Starting Epoch 31...\n",
      "Epoch 31/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9957 - auc: 0.9989 - loss: -0.1872      \n",
      "Epoch 31: val_loss did not improve from 0.04328\n",
      "Completed Epoch 31, Loss: -0.1766, Val Loss: 1.9273\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9957 - auc: 0.9989 - loss: -0.1872 - val_accuracy: 0.8452 - val_auc: 0.7814 - val_loss: 1.9273\n",
      "\n",
      "Starting Epoch 32...\n",
      "Epoch 32/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: -0.0068     \n",
      "Epoch 32: val_loss did not improve from 0.04328\n",
      "Completed Epoch 32, Loss: -0.1089, Val Loss: 0.0945\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: -0.0070 - val_accuracy: 0.9769 - val_auc: 0.9892 - val_loss: 0.0945\n",
      "\n",
      "Starting Epoch 33...\n",
      "Epoch 33/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: -1.3343e-04\n",
      "Epoch 33: val_loss did not improve from 0.04328\n",
      "Completed Epoch 33, Loss: -0.2302, Val Loss: 2.1854\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: -4.5514e-04 - val_accuracy: 0.8946 - val_auc: 0.8579 - val_loss: 2.1854\n",
      "\n",
      "Starting Epoch 34...\n",
      "Epoch 34/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9953 - auc: 0.9993 - loss: -0.1793      \n",
      "Epoch 34: val_loss did not improve from 0.04328\n",
      "Completed Epoch 34, Loss: -0.2358, Val Loss: 3.8558\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 430ms/step - accuracy: 0.9953 - auc: 0.9993 - loss: -0.1794 - val_accuracy: 0.8914 - val_auc: 0.8258 - val_loss: 3.8558\n",
      "\n",
      "Starting Epoch 35...\n",
      "Epoch 35/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9967 - auc: 0.9996 - loss: -0.5885  \n",
      "Epoch 35: val_loss did not improve from 0.04328\n",
      "Completed Epoch 35, Loss: -0.1810, Val Loss: 0.3616\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9967 - auc: 0.9996 - loss: -0.5880 - val_accuracy: 0.9268 - val_auc: 0.9490 - val_loss: 0.3616\n",
      "\n",
      "Starting Epoch 36...\n",
      "Epoch 36/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9959 - auc: 0.9996 - loss: -0.0151     \n",
      "Epoch 36: val_loss did not improve from 0.04328\n",
      "Completed Epoch 36, Loss: -0.0689, Val Loss: 0.2879\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9959 - auc: 0.9996 - loss: -0.0152 - val_accuracy: 0.9331 - val_auc: 0.9594 - val_loss: 0.2879\n",
      "\n",
      "Starting Epoch 37...\n",
      "Epoch 37/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: -0.1316      \n",
      "Epoch 37: val_loss did not improve from 0.04328\n",
      "Completed Epoch 37, Loss: -0.3001, Val Loss: 0.1031\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 431ms/step - accuracy: 0.9971 - auc: 0.9998 - loss: -0.1318 - val_accuracy: 0.9797 - val_auc: 0.9905 - val_loss: 0.1031\n",
      "\n",
      "Starting Epoch 38...\n",
      "Epoch 38/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9974 - auc: 0.9996 - loss: -0.4719  \n",
      "Epoch 38: val_loss did not improve from 0.04328\n",
      "Completed Epoch 38, Loss: -0.3166, Val Loss: 0.0737\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 431ms/step - accuracy: 0.9974 - auc: 0.9996 - loss: -0.4717 - val_accuracy: 0.9877 - val_auc: 0.9940 - val_loss: 0.0737\n",
      "\n",
      "Starting Epoch 39...\n",
      "Epoch 39/50\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: -0.0460      \n",
      "Epoch 39: val_loss improved from 0.04328 to 0.00723, saving model to /home/apyba3/MobNetV3Small/mobnetv3smallcheckpoint.keras\n",
      "Completed Epoch 39, Loss: -0.1596, Val Loss: 0.0072\n",
      "\u001b[1m714/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 437ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: -0.0461 - val_accuracy: 0.9982 - val_auc: 1.0000 - val_loss: 0.0072\n",
      "\n",
      "Starting Epoch 40...\n",
      "Epoch 40/50\n",
      "\u001b[1m145/714\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4:02\u001b[0m 426ms/step - accuracy: 0.9963 - auc: 0.9995 - loss: -0.5429"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m epoch_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(\n\u001b[1;32m     14\u001b[0m     on_epoch_begin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Training loop with added callback\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include both callbacks\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory and create it if it doesn't exist\n",
    "checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint filepath with keras extension (more compatible with TF 2.13)\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'finetuned_50epochs_mobnetv3smallcheckpoint.keras')\n",
    "\n",
    "# Create ModelCheckpoint with compatible parameters for TF 2.13\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    "    # No save_weights_only parameter\n",
    ")\n",
    "\n",
    "# Define epoch callback\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_path = '/home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_MLiSards/finetuned_50epochs_mobnetv3small_classification_model.keras'\n",
    "model.save(model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the weights learned from fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O14u6175RLjA"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/weights/fintuning_training_mobnetv3small_classification_weights.weights.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCbo4VcLxLgQ"
   },
   "source": [
    "# 3) CLASSIFICATION SPEED Test-Set Predictions\n",
    "\n",
    "a) load in test data\n",
    "\n",
    "b) convert test images to numerical RGB feature maps\n",
    "\n",
    "c) generate predictions on the test set\n",
    "\n",
    "d) correctly format the predictions into a pandas dataframe\n",
    "\n",
    "e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnygDJsKxYhA"
   },
   "source": [
    "### 3a) load in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "W-e59lQQRXKK",
    "outputId": "aa8566ec-e472-47a6-c7a0-92266b567a62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              image_file_paths\n",
       "image_id                                                                                      \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data/5.png"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data'\n",
    "# image_folder_path = '/home/ppyt13/machine-learning-in-science-ii-2025/test_data/test_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'\n",
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-9i5trTyDTf"
   },
   "source": [
    "### 3b) convert test images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3hT_c1s5TAR-"
   },
   "outputs": [],
   "source": [
    "def process_image_no_label(image_path, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Use decode_png for PNG images\n",
    "    image = tf.image.resize(image, resized_shape)  # Resize to uniform shape\n",
    "    image = image / 255.0  # Normalize pixel values to [0,1]\n",
    "    return image\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((imagefilepaths_df[\"image_file_paths\"]))\n",
    "\n",
    "test_dataset = test_dataset.map(process_image_no_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gobnK7PhyLa2"
   },
   "source": [
    "### 3c) generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtqcOFr7TAXa",
    "outputId": "73b4c96b-51bf-4e1c-e1b6-e8cde1321984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT1LJxHTPeQT"
   },
   "source": [
    "### 3d) correctly format the predictions into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pFVWGi04fza7"
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OnO0K1rReHOT",
    "outputId": "d9cebb2e-3d36-4c7a-b024-eabb646e3bbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.370914e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.999998e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.998439e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          speed\n",
       "0  1.370914e-25\n",
       "1  1.000000e+00\n",
       "2  9.999998e-01\n",
       "3  1.000000e+00\n",
       "4  9.998439e-01"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[predictions_df['speed'] > 0.5] = 1\n",
    "predictions_df[predictions_df['speed'] < 0.5] = 0\n",
    "\n",
    "predictions_df['speed'] = predictions_df['speed'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed\n",
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CcRKL9KTAfs",
    "outputId": "277533cd-06aa-4709-d44e-9027cc7e9438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speed\n",
       "1    516\n",
       "0    504\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['speed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU-PhskZPaHD"
   },
   "source": [
    "### 3e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "deXjPTO0TAiL"
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv('/home/apyba3/mobnetv3small_speedclassification_withvalidation_withpetrudata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
