# -*- coding: utf-8 -*-
"""MobNetV3_DUAL_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/tylaar1/PICAR-autopilot/blob/main/MobNetV3_DUAL_model.ipynb

# SWITCH TO **`T4 GPU`** OR THE **`HPC`**

# Imports
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.preprocessing.image import load_img, img_to_array
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
import matplotlib.pyplot as plt

# makes it so pd dfs aren't truncated

pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

#from google.colab import drive
#drive.mount('/content/drive')

"""# 1) DATA PRE-PROCESSING

a) Load in labels + image file paths

b) combine them into one dataframe

c) EDA - spotted and removed erroneous label (speed = 1.42...)

- `cleaned_df` is the cleaned df with a) b) c) completed

d) convert images to numerical RGB feature maps - ML algorithms only understand numerical data

e) Splitting data into training and validation sets

f) data augmentation applied to training set

### 1a) load in labels + image file paths
"""

#labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path
labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)
#labels_file_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_norm.csv' # tyler hpc file path (mlis2 cluster)
labels_df = pd.read_csv(labels_file_path, index_col='image_id')

image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data' # bens hpc file path
#image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path
#image_folder_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_data/training_data' # tyler hpc file path
image_file_paths = [
    os.path.join(image_folder_path, f)
    for f in os.listdir(image_folder_path)
    if f.lower().endswith(('.png', '.jpg', '.jpeg'))
]

image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)

imagefilepaths_df = pd.DataFrame(
    image_file_paths,
    columns=['image_file_paths'],
    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]
)

imagefilepaths_df.index.name = 'image_id'

"""Checking labels dataframe"""

labels_df.head()

"""Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"""

imagefilepaths_df.head()

"""### 1b) Combine labels and image file paths into one dataframe"""

kaggle_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')
kaggle_df['speed'] = kaggle_df['speed'].round(6)  # to get rid of floating point errors



"""The above cell shows that:

 1) the image files and labels match (see image_id and the number at the end of the file path)

 2) the missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of

### 1c) EDA
"""



"""note: imbalance datset

identifying the row with the erroneous speed value
"""



petrudata_folder_path = '/home/apyba3/petru_data'
bendata_folder_path = '/home/apyba3/bendata_1st_set'

def extract_filepaths(extradata_folderpath):
    extradata_file_paths = [
    os.path.join(extradata_folderpath, f)
    for f in os.listdir(extradata_folderpath)
    if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ]
    return extradata_file_paths

def loadin_extradata(extradata_folderpath):
    extracted_filepaths = extract_filepaths(extradata_folderpath)
    # Regex pattern to extract angle and speed values
    pattern = r'(\d+)_(\d+)_(\d+)\.png'  # Fixed pattern to capture groups correctly

    angle_value = []
    speed_value = []

    # Loop through file paths and extract angle and speed values
    for file_path in extracted_filepaths:
        match = re.search(pattern, file_path)
        if match:
            # Extract random number, angle, and speed values
            random_number = match.group(1)
            angle_value.append(int(match.group(2)))
            speed_value.append(int(match.group(3)))
        else:
            print(f"No match found for file: {file_path}")

    return angle_value, speed_value

def create_df_and_normalisation(folder_path):
    max_image_id = len(loadin_extradata(folder_path)[0])
    df = pd.DataFrame(
        {
            'angle': loadin_extradata(folder_path)[0],
            'speed': loadin_extradata(folder_path)[1],
            'image_file_paths': extract_filepaths(folder_path)
        }
    )

    df.loc[df['speed'] > 0, 'speed'] = 1
    df['angle'] = (df['angle'] - 50)/80
    return df

def combine_dfs_add_imageid(df1, df2):
    combined_df = pd.concat([df1, df2], ignore_index=True)
    combined_df.index = pd.RangeIndex(start=13799, stop=13799 + len(combined_df), step=1)
    combined_df.index.name = 'image_id'
    return combined_df

bendata_df = create_df_and_normalisation(bendata_folder_path)
petrudata_df = create_df_and_normalisation(petrudata_folder_path)

combinedextradata_df = combine_dfs_add_imageid(bendata_df, petrudata_df)
display(combinedextradata_df.tail())

merged_df = pd.concat([kaggle_df, combinedextradata_df])

def validate_image_files(dataframe):
    """
    Check each image file in the dataframe and return a new dataframe with only valid files.
    Also prints information about invalid files.
    """
    valid_rows = []
    invalid_files = []
    
    print(f"Checking {len(dataframe)} image files...")
    
    for index, row in dataframe.iterrows():
        path = row['image_file_paths']
        
        # # Print every 100 files to show progress
        # if len(valid_rows) % 100 == 0 and len(valid_rows) > 0:
        #     print(f"Processed {len(valid_rows) + len(invalid_files)} files, {len(valid_rows)} valid...")
        
        try:
            # Basic file checks
            if not os.path.exists(path):
                # print(f"File does not exist: {path}")
                invalid_files.append(path)
                continue
                
            if os.path.getsize(path) == 0:
                # print(f"File is empty: {path}")
                invalid_files.append(path)
                continue
            
            # Try to open the image with PIL to verify it's valid
            with Image.open(path) as img:
                # Just accessing img.size will verify the image can be read
                width, height = img.size
            
            # If we get this far, the image is valid
            valid_rows.append(row)
                
        except Exception as e:
            print(f"Error with {path}: {e}")
            invalid_files.append(path)
    
    # Create a new dataframe with only valid rows
    valid_df = pd.DataFrame(valid_rows)
    
    print(f"\nValidation complete: {len(valid_df)} valid files, {len(invalid_files)} invalid files")
    
    # if invalid_files:
    #     print("\nFirst few invalid files:")
    #     for path in invalid_files[:5]:
    #         print(f"  - {path}")
    
    return valid_df

# Run the validation
valid_df = validate_image_files(merged_df)

cleaned_df = valid_df[valid_df['speed'] != 1.428571]

"""### 1d) convert images to numerical RGB feature maps"""
BATCH_SIZE = 32

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

# Start with dataframe columns
image_paths = cleaned_df["image_file_paths"].values
speed_labels = cleaned_df["speed"].values

# Perform stratified split on the raw data
train_paths, val_paths, train_labels, val_labels = train_test_split(
    image_paths, 
    speed_labels,
    test_size=0.15,
    random_state=42,
    stratify=speed_labels  # This ensures same distribution
)

# Process function remains the same
def process_image(image_path, label, resized_shape=(224, 224)):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, resized_shape)
    image = image / 255.0
    return image, label

# Create separate datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
train_dataset = train_dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(len(train_paths))
train_dataset = train_dataset.batch(BATCH_SIZE)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))
val_dataset = val_dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.cache()
val_dataset = val_dataset.batch(BATCH_SIZE)
val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)

def augment_image(image, label):
  seed = (6, 9)
  image = tf.image.stateless_random_brightness(image, 0.2, seed)
  image = tf.image.stateless_random_contrast(image, 0.8, 1.2, seed)
  image = tf.image.stateless_random_hue(image, 0.2, seed)
  image = tf.image.stateless_random_saturation(image, 0.8, 1.2, seed)
  return image, label

augmented_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.concatenate(augmented_dataset)
train_dataset = train_dataset.shuffle(buffer_size=len(cleaned_df))

"""# 2) Model Building - MobileNetV3Large Transfer Learning

a) Set up model architecture

b) define training step

c) training the model on the training set

d) fine-tuning

### 2a) Set up model architecture

- MobileNetV2 to learn lower level features
- global average pooling layer
- drop out layer
- dense layer with sigmoid activation
"""

dropoutrate = 0.2
num_classes = 1 # we're only predicting the prob of the positive class with a sigmoid
input_shape = (224,224,3)

mbnet = tf.keras.applications.MobileNetV3Small(
    input_shape=input_shape,
    include_top=False,
    weights='imagenet',
    minimalistic=False
)

# Create input layer
input_layer = tf.keras.Input(shape=(224, 224, 3), name="input")

# Shared backbone
x = mbnet(input_layer)
x = tf.keras.layers.GlobalAveragePooling2D(name="global_avg_pool")(x)
x = tf.keras.layers.Dropout(dropoutrate, name="dropout_shared")(x)

# Classification branch
y = tf.keras.layers.Dense(64, activation='relu', name="dense_class_1")(x)
y = tf.keras.layers.Dropout(dropoutrate, name="dropout_class")(y)
y = tf.keras.layers.Dense(32, activation='relu', name="dense_class_2")(y)
classification_output = tf.keras.layers.Dense(num_classes, activation='sigmoid', name="classification")(y)

# Regression branch (completely separate path)
z = tf.keras.layers.Dense(64, activation='relu', name="dense_reg_1")(x)
z = tf.keras.layers.Dropout(dropoutrate, name="dropout_reg")(z)
z = tf.keras.layers.Dense(32, activation='relu', name="dense_reg_2")(z)
regression_output = tf.keras.layers.Dense(1, activation='linear', name="regression")(z)

# Combine outputs in model
model = tf.keras.Model(inputs=input_layer, outputs=[classification_output, regression_output])

# Compile with separate loss functions
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss={'classification': 'binary_crossentropy', 'regression': 'mse'},
              metrics={'classification': 'accuracy', 'regression': 'mse'})

# below code can be used to adjust the loss weights for classification and regression tasks
'''
classification_weight = 3.0
regression_weight = 1.0  # Adjust these weights to balance importance

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss={
        'classification': 'binary_crossentropy', 
        'regression': 'mse'
    },
    loss_weights={
        'classification': classification_weight, 
        'regression': regression_weight
    },
    metrics={'classification': 'accuracy', 'regression': 'mse'}
)
'''
class CustomModelCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min'):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.verbose = verbose
        self.save_best_only = save_best_only
        
        # Initialize best value based on mode
        self.mode = mode
        if mode == 'min':
            self.best = float('inf')
            self.monitor_op = lambda current, best: current < best
        else:  # 'max'
            self.best = float('-inf')
            self.monitor_op = lambda current, best: current > best
            
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        current = logs.get(self.monitor)
        
        if current is None:
            if self.verbose > 0:
                print(f"Warning: {self.monitor} not found in logs")
            return
            
        if self.save_best_only:
            if self.monitor_op(current, self.best):
                if self.verbose > 0:
                    print(f"\nEpoch {epoch+1}: {self.monitor} improved from {self.best:.4f} to {current:.4f}")
                    print(f"Saving model to {self.filepath}")
                self.best = current
                
                # Use direct model.save() without extra parameters
                try:
                    self.model.save(self.filepath)
                except Exception as e:
                    print(f"Error saving model: {e}")
        else:
            if self.verbose > 0:
                print(f"\nEpoch {epoch+1}: saving model to {self.filepath}")
            try:
                self.model.save(self.filepath)
            except Exception as e:
                print(f"Error saving model: {e}")
                
checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_kaggle/tyler_checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

# Define checkpoint filepath for the finetuned model
checkpoint_filepath = os.path.join(checkpoint_dir, 'stratifiedsplit_frozentrain_50epochs_mobnetv3small_classif_checkpoint.keras')

# Use the existing CustomModelCheckpoint class (no need to redefine it)
# Just create a new instance with the updated filepath
custom_checkpoint = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# Define epoch callback 
epoch_callback = LambdaCallback(
    on_epoch_begin=lambda epoch, logs: print(f"\nStarting Epoch {epoch + 1}..."),
    on_epoch_end=lambda epoch, logs: print(f"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}")
)

from keras.callbacks import EarlyStopping

# Define early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    min_delta=0.001,
    mode='min',
    restore_best_weights=True,  # This loads the best weights into model memory when stopping
    verbose=1
)

# Train with both callbacks
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=100,
    callbacks=[custom_checkpoint, epoch_callback, early_stopping]
)

import matplotlib.pyplot as plt
import numpy as np

def plot_loss_curves(history, save_path=None, fontsize=14):
    """
    Create a professional-looking loss curve plot with training and validation curves.

    Args:
        history: The history object returned by model.fit()
        save_path: Path to save the plot (without extension)
        fontsize: Font size for labels, ticks, and legend
    """
    # Create figure with appropriate size
    plt.figure(figsize=(10, 6), dpi=300)
    
    # Epochs on X-axis
    epochs = range(1, len(history.history['loss']) + 1)
    
    # Plot training and validation loss
    plt.plot(epochs, history.history['loss'], 'o-', color='blue', linewidth=2, 
             markersize=4, label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 's-', color='red', linewidth=2, 
             markersize=4, label='Validation Loss')
    
    # Labels
    plt.xlabel('Epoch', fontsize=fontsize)
    plt.ylabel('Loss', fontsize=fontsize)
    
    # Integer ticks for epochs
    plt.xticks(ticks=np.arange(0, len(epochs) + 1, 2), fontsize=fontsize)

    plt.yticks(fontsize=fontsize)
    
    # Grid, legend, and ticks
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=16, loc='upper right')
    
    plt.minorticks_on()
    plt.tick_params(axis='both', which='minor', length=4, color='gray', labelsize=fontsize)
    plt.tick_params(axis='both', which='major', length=6, color='black', labelsize=fontsize)
    plt.tick_params(top=True, right=True, direction='in', length=6)
    plt.tick_params(which='minor', top=True, right=True, direction='in', length=4)
    
    # Layout and save
    plt.tight_layout()
    if save_path:
        plt.savefig(f"{save_path}.pdf", bbox_inches='tight')
        plt.savefig(f"{save_path}.png", bbox_inches='tight', dpi=300)
        print(f"Loss curves saved to {save_path}.pdf and {save_path}.png")
    
    plt.show()

model.save_weights('/home/apyba3/car_frozen_regression.weights.h5')
#model.save_weights('/home/ppytr13/car_frozen_regression.weights.h5')
loss_plot_path = 'stratifiedsplit_earlystop_mbnetv3small_frozentrain_losscurves'
plot_loss_curves(history, save_path=loss_plot_path, fontsize=22)
tf.keras.backend.clear_session() #Clear keras session

"""### 2d) fine-tuning

rebuild model after clearing keras session
"""

dropoutrate = 0.2
num_classes = 1 # we're only predicting the prob of the positive class with a sigmoid
input_shape = (224,224,3)

mbnet = tf.keras.applications.MobileNetV3Small(
    input_shape=input_shape,
    include_top=False,
    weights='imagenet',
    minimalistic=False
)

# Create input layer
input_layer = tf.keras.Input(shape=(224, 224, 3), name="input")

# Shared backbone
x = mbnet(input_layer)
x = tf.keras.layers.GlobalAveragePooling2D(name="global_avg_pool")(x)
x = tf.keras.layers.Dropout(dropoutrate, name="dropout_shared")(x)

# Classification branch
y = tf.keras.layers.Dense(64, activation='relu', name="dense_class_1")(x)
y = tf.keras.layers.Dropout(dropoutrate, name="dropout_class")(y)
y = tf.keras.layers.Dense(32, activation='relu', name="dense_class_2")(y)
classification_output = tf.keras.layers.Dense(num_classes, activation='sigmoid', name="classification")(y)

# Regression branch (completely separate path)
z = tf.keras.layers.Dense(64, activation='relu', name="dense_reg_1")(x)
z = tf.keras.layers.Dropout(dropoutrate, name="dropout_reg")(z)
z = tf.keras.layers.Dense(32, activation='relu', name="dense_reg_2")(z)
regression_output = tf.keras.layers.Dense(1, activation='linear', name="regression")(z)

# Combine outputs in model
model = tf.keras.Model(inputs=input_layer, outputs=[classification_output, regression_output])



# Compile with separate loss functions
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss={'classification': 'binary_crossentropy', 'regression': 'mse'},
              metrics={'classification': 'accuracy', 'regression': 'mse'})

model.summary()

model.build(input_layer)

mbnet.trainable = True

model.summary()

model.load_weights('/home/apyba3/car_frozen_regression.weights.h5')
#model.load_weights('/home/ppytr13/car_frozen_regression.weights.h5')

"""Set up fine-tuning training"""

checkpoint_dir = '/home/apyba3/PICAR-autopilot/MobNetV3Small_Kaggle/checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

# Define checkpoint filepath for the finetuned model
checkpoint_filepath = os.path.join(checkpoint_dir, 'stratifiedsplitfinetuned_mobnetv3small_classif_checkpoint.keras')

# Use the existing CustomModelCheckpoint class (no need to redefine it)
# Just create a new instance with the updated filepath
custom_checkpoint = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# Define epoch callback 
epoch_callback = LambdaCallback(
    on_epoch_begin=lambda epoch, logs: print(f"\nStarting Epoch {epoch + 1}..."),
    on_epoch_end=lambda epoch, logs: print(f"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}")
)

# Define early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=8,
    min_delta=0.001,
    mode='min',
    restore_best_weights=True,  # This loads the best weights into model memory when stopping
    verbose=1
)

# Train with both callbacks
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,
    callbacks=[custom_checkpoint, epoch_callback, early_stopping]
)

loss_plot_path = 'stratifiedsplit_extrabendata_finetune_earlystop_mbnetv3small_losscurves'
plot_loss_curves(history, save_path=loss_plot_path, fontsize=22)

model_path = '/home/apyba3/PICAR-autopilot/autopilot/models/models/BenTyler_Dual_head/mobnet.keras'
model.save(model_path)
print(f"Final model saved to {model_path}")

model_path = '/home/apyba3/PICAR-autopilot/autopilot/models/models/BenTyler_Dual_head/mobnet.h5'
model.save(model_path)
print(f"Final model saved to {model_path}")


"""## instead - convert to tf lite (chatgpt code - not tested yet)"""

import tensorflow as tf

# Define the converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Enable default optimizations
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Specify fixed input shape
converter._experimental_fixed_input_shape = {"serving_default_input": [1, 224, 224, 3]}  # Batch size 1

# Use FP16 for smaller model size and faster inference
converter.target_spec.supported_types = [tf.float16]

# Convert the model
tflite_model = converter.convert()

# Save the model as a TFLite file
tflite_model_path = '/home/apyba3/PICAR-autopilot-1/autopilot/models/BenTyler_Dual_head/mobnet.tflite'
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

print("Optimized TFLite model saved at:", tflite_model_path)