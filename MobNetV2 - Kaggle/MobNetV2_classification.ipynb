{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylaar1/PICAR-autopilot/blob/main/classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhwRSFoj6C_"
      },
      "source": [
        "# SWITCH TO **`T4 GPU`** OR THE **`HPC`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4V83PflfFkL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kP6UczzNe1l2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from keras.callbacks import ModelCheckpoint, LambdaCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linux\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "print(platform.system())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IF_vPVifaU9V"
      },
      "outputs": [],
      "source": [
        "# makes it so pd dfs aren't truncated\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_MvRvYnfIM5"
      },
      "source": [
        "# 1) DATA PRE-PROCESSING\n",
        "\n",
        "a) Load in labels + image file paths\n",
        "\n",
        "b) combine them into one dataframe\n",
        "\n",
        "c) EDA - spotted and removed erroneous label (speed = 1.42...)\n",
        "\n",
        "- `cleaned_df` is the cleaned df with a) b) c) completed\n",
        "\n",
        "d) convert images to numerical RGB feature maps - ML algorithms only understand numerical data\n",
        "\n",
        "e) Splitting data into training and validation sets\n",
        "\n",
        "f) data augmentation applied to training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU3TvBZ5hfhX"
      },
      "source": [
        "### 1a) load in labels + image file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZiNf_BxOfEH-"
      },
      "outputs": [],
      "source": [
        "# labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path\n",
        "labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)\n",
        "\n",
        "labels_df = pd.read_csv(labels_file_path, index_col='image_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nOXmN--gb-Q9"
      },
      "outputs": [],
      "source": [
        "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data' # ben hpc file path (mlis2 cluster)\n",
        "# image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path\n",
        "\n",
        "image_file_paths = [\n",
        "    os.path.join(image_folder_path, f)\n",
        "    for f in os.listdir(image_folder_path)\n",
        "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "]\n",
        "\n",
        "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
        "\n",
        "imagefilepaths_df = pd.DataFrame(\n",
        "    image_file_paths,\n",
        "    columns=['image_file_paths'],\n",
        "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
        ")\n",
        "\n",
        "imagefilepaths_df.index.name = 'image_id'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oeuvmeZaGSC"
      },
      "source": [
        "Checking labels dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "2pi13TZ2aFhO",
        "outputId": "61b057ab-c7d6-4d69-f37d-7474876cfea0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>angle</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.8125</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           angle  speed\n",
              "image_id               \n",
              "1         0.4375    0.0\n",
              "2         0.8125    1.0\n",
              "3         0.4375    1.0\n",
              "4         0.6250    1.0\n",
              "5         0.5000    0.0"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puEjGoOJaRS4"
      },
      "source": [
        "Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "a1suFSK7aWKH",
        "outputId": "881d4ab5-f4af-4bdb-cb3d-6abac8ed221e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_file_paths</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/2.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/4.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/5.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                       image_file_paths\n",
              "image_id                                                                                               \n",
              "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/1.png\n",
              "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/2.png\n",
              "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3.png\n",
              "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/4.png\n",
              "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/5.png"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imagefilepaths_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjDdyYd6cMBE"
      },
      "source": [
        "### 1b) Combine labels and image file paths into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6NdbonzPcLKB"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')\n",
        "merged_df['speed'] = merged_df['speed'].round(6) # to get rid of floating point errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "-VstirIAdAZi",
        "outputId": "4027d38a-52b6-45e9-9682-d12b39741baf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>angle</th>\n",
              "      <th>speed</th>\n",
              "      <th>image_file_paths</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13794</th>\n",
              "      <td>0.6250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13794.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13795</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13795.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13796</th>\n",
              "      <td>0.5625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13796.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13797</th>\n",
              "      <td>0.6250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13797.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13798</th>\n",
              "      <td>0.6875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13798.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           angle  speed  \\\n",
              "image_id                  \n",
              "13794     0.6250    1.0   \n",
              "13795     0.4375    1.0   \n",
              "13796     0.5625    0.0   \n",
              "13797     0.6250    0.0   \n",
              "13798     0.6875    1.0   \n",
              "\n",
              "                                                                                           image_file_paths  \n",
              "image_id                                                                                                     \n",
              "13794     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13794.png  \n",
              "13795     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13795.png  \n",
              "13796     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13796.png  \n",
              "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13797.png  \n",
              "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/13798.png  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PCxqJbmXE6"
      },
      "source": [
        "The cell below shows that:\n",
        "\n",
        " 1) the image files and labels match (see image_id and the number at the end of the file path)\n",
        "\n",
        " 2) the 5 missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8MgNoL8nfBm2",
        "outputId": "ab41b9fc-dc38-4cc2-eb43-a97346da057c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>angle</th>\n",
              "      <th>speed</th>\n",
              "      <th>image_file_paths</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3139</th>\n",
              "      <td>0.750</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3139.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3140</th>\n",
              "      <td>0.875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3140.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3142</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3142.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3143</th>\n",
              "      <td>0.625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3143.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          angle  speed  \\\n",
              "image_id                 \n",
              "3139      0.750    1.0   \n",
              "3140      0.875    1.0   \n",
              "3142      0.625    0.0   \n",
              "3143      0.625    1.0   \n",
              "\n",
              "                                                                                          image_file_paths  \n",
              "image_id                                                                                                    \n",
              "3139      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3139.png  \n",
              "3140      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3140.png  \n",
              "3142      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3142.png  \n",
              "3143      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3143.png  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.loc[3139:3143]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3OKLcn9u0Pz"
      },
      "source": [
        "### 1c) EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "IWQCQrR-oCps",
        "outputId": "51433e90-a832-4ad4-d468-0dd826eea044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "speed\n",
              "0.000000     3390\n",
              "1.000000    10402\n",
              "1.428571        1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.value_counts('speed').sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4pZ65pYvdqb"
      },
      "source": [
        "note: imbalance datset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJmG7jmNkE0k"
      },
      "source": [
        "identifying the row with the erroneous speed value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "wAQnbLLeiqy2",
        "outputId": "a02da7d8-c04f-4908-da19-ae3b41d3c99b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>angle</th>\n",
              "      <th>speed</th>\n",
              "      <th>image_file_paths</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3884</th>\n",
              "      <td>0.4375</td>\n",
              "      <td>1.428571</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3884.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           angle     speed  \\\n",
              "image_id                     \n",
              "3884      0.4375  1.428571   \n",
              "\n",
              "                                                                                          image_file_paths  \n",
              "image_id                                                                                                    \n",
              "3884      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3884.png  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df[merged_df['speed'] == 1.428571]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMZq41-RkLz0"
      },
      "source": [
        "we want to remove this row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TDMqIiOLSKGX",
        "outputId": "1d3ae6d4-3412-43e9-deae-dca9de9b718f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>angle</th>\n",
              "      <th>speed</th>\n",
              "      <th>image_file_paths</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3882</th>\n",
              "      <td>0.5625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3882.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3883</th>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3883.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3885</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3885.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3886</th>\n",
              "      <td>0.7500</td>\n",
              "      <td>1.0</td>\n",
              "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3886.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           angle  speed  \\\n",
              "image_id                  \n",
              "3882      0.5625    1.0   \n",
              "3883      0.3750    0.0   \n",
              "3885      0.0000    1.0   \n",
              "3886      0.7500    1.0   \n",
              "\n",
              "                                                                                          image_file_paths  \n",
              "image_id                                                                                                    \n",
              "3882      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3882.png  \n",
              "3883      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3883.png  \n",
              "3885      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3885.png  \n",
              "3886      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025//training_data/training_data/3886.png  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_df = merged_df[merged_df['speed'] != 1.428571]\n",
        "cleaned_df.loc[3882:3886]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di6F6km_DBmj"
      },
      "source": [
        "### 1d) convert images to numerical RGB feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "oeeBTruNCQ96"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def process_image(image_path, label, resized_shape=(224, 224)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, resized_shape)\n",
        "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
        "    return image, label\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((cleaned_df[\"image_file_paths\"], cleaned_df[\"speed\"])) # Convert pd df into a tf ds\n",
        "\n",
        "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(len(cleaned_df))\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUOlsWQeVlyC"
      },
      "source": [
        "lets check and see if what we have done works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jBTNjNhMVk2g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-29 15:17:03.291953: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2373 of 13792\n",
            "2025-04-29 15:17:13.240054: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 5510 of 13792\n",
            "2025-04-29 15:17:23.343778: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 7682 of 13792\n",
            "2025-04-29 15:17:33.293932: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 9327 of 13792\n",
            "2025-04-29 15:17:43.254389: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 11772 of 13792\n",
            "2025-04-29 15:17:52.425977: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 224, 224, 3) (32,)\n"
          ]
        }
      ],
      "source": [
        "for images, labels in dataset.take(1):\n",
        "    print(images.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md6U_i84SiK5"
      },
      "source": [
        "### 1e) Splitting data into training and validation sets (test set is already provided in kaggle data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "yYlssPh5dxaO"
      },
      "outputs": [],
      "source": [
        "# 80-20 split\n",
        "\n",
        "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train_size = int(0.8 * dataset_size)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qPUE6rd8cgQN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 344, Test size: 87\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train size: {train_size}, Test size: {dataset_size - train_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ujsjhMPSw4f"
      },
      "source": [
        "### 1f) Speed classification model's version: Data Augmentation applied to training set\n",
        "\n",
        "- Random Horizontal Flip\n",
        "- Random Brightness Adjustment\n",
        "- Random Contrast Adjustment\n",
        "- Random Hue Adjustment\n",
        "- Random Saturation Adjustment\n",
        "- Random Vertical Flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "T9r811eWsYfe"
      },
      "outputs": [],
      "source": [
        "#only performing augmentation on training data as want validation/test data to reflect kaggle test set\n",
        "seed = (6,9)\n",
        "train_dataset = train_dataset.map(\n",
        "      lambda image, label: (tf.image.stateless_random_flip_left_right(image,seed),label)\n",
        ").map(\n",
        "      lambda image, label: (tf.image.stateless_random_brightness(image, 0.2,seed),label)\n",
        ").map(\n",
        "      lambda image, label: (tf.image.stateless_random_contrast(image, 0.8, 1.2,seed),label)\n",
        ").map(\n",
        "      lambda image, label: (tf.image.stateless_random_hue(image, 0.2, seed),label)\n",
        ").map(\n",
        "      lambda image, label: (tf.image.stateless_random_saturation(image, 0.8, 1.2,seed),label)\n",
        ").map(\n",
        "      lambda image, label: (tf.image.stateless_random_flip_up_down(image,seed),label)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdi-dUCTND1"
      },
      "source": [
        "checking to see if whats been done was successful or needs debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "collapsed": true,
        "id": "OeboVhsQKGFS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image shape:  (224, 224, 3)\n",
            "label: 0\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 0\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n",
            "image shape:  (224, 224, 3)\n",
            "label: 0\n",
            "image shape:  (224, 224, 3)\n",
            "label: 1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "f, axarr = plt.subplots(1,10)\n",
        "\n",
        "i = 0\n",
        "for image_batch, label_batch in dataset.take(1):  # Take one batch\n",
        "    for image in image_batch:  # Iterate through images in the batch\n",
        "        if i < 10:  # Only display the first 5 images\n",
        "            print('image shape: ', np.shape(image))\n",
        "            tf.print('label:', label_batch[i])  # Print label for the corresponding image\n",
        "            axarr[i].imshow(image)\n",
        "            axarr[i].axis('off')\n",
        "            i += 1\n",
        "        else:\n",
        "            break  # Stop after displaying 5 images\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmetmzNHTWzU"
      },
      "source": [
        "# 2) Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48RHLVshdX5L"
      },
      "source": [
        "### 2a) Set up model architecture\n",
        "\n",
        "- MobileNetV2 to learn lower level features\n",
        "- global average pooling layer\n",
        "- drop out layer\n",
        "- dense layer with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "dropoutrate = 0.2\n",
        "input_shape = (224,224,3)\n",
        "num_classes = 1 # we're only predicting the prob of the positive class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
            " tional)                                                         \n",
            "                                                                 \n",
            " global_average_pooling2d_4  (None, 1280)              0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               327936    \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2629185 (10.03 MB)\n",
            "Trainable params: 371201 (1.42 MB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "mobnetv2 =  tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  mobnetv2,\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mobnetv2.trainable = False  # freeze mobnetv3small layers\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b) Train classification model with MobNetV2 layers frozen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Epoch 1...\n",
            "Epoch 1/45\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "344/344 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9675 - auc: 0.9921\n",
            "Epoch 1: val_loss improved from inf to 0.07419, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 1, Loss: 0.0888, Val Loss: 0.0742\n",
            "344/344 [==============================] - 218s 634ms/step - loss: 0.0888 - accuracy: 0.9675 - auc: 0.9921 - val_loss: 0.0742 - val_accuracy: 0.9698 - val_auc: 0.9951\n",
            "\n",
            "Starting Epoch 2...\n",
            "Epoch 2/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9715 - auc: 0.9944\n",
            "Epoch 2: val_loss improved from 0.07419 to 0.05709, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 2, Loss: 0.0747, Val Loss: 0.0571\n",
            "344/344 [==============================] - 255s 743ms/step - loss: 0.0747 - accuracy: 0.9715 - auc: 0.9944 - val_loss: 0.0571 - val_accuracy: 0.9824 - val_auc: 0.9982\n",
            "\n",
            "Starting Epoch 3...\n",
            "Epoch 3/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9713 - auc: 0.9939\n",
            "Epoch 3: val_loss improved from 0.05709 to 0.04856, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 3, Loss: 0.0761, Val Loss: 0.0486\n",
            "344/344 [==============================] - 208s 604ms/step - loss: 0.0761 - accuracy: 0.9713 - auc: 0.9939 - val_loss: 0.0486 - val_accuracy: 0.9810 - val_auc: 0.9982\n",
            "\n",
            "Starting Epoch 4...\n",
            "Epoch 4/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9763 - auc: 0.9956\n",
            "Epoch 4: val_loss did not improve from 0.04856\n",
            "Completed Epoch 4, Loss: 0.0643, Val Loss: 0.0510\n",
            "344/344 [==============================] - 207s 601ms/step - loss: 0.0643 - accuracy: 0.9763 - auc: 0.9956 - val_loss: 0.0510 - val_accuracy: 0.9820 - val_auc: 0.9981\n",
            "\n",
            "Starting Epoch 5...\n",
            "Epoch 5/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9767 - auc: 0.9955\n",
            "Epoch 5: val_loss improved from 0.04856 to 0.04657, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 5, Loss: 0.0638, Val Loss: 0.0466\n",
            "344/344 [==============================] - 209s 608ms/step - loss: 0.0638 - accuracy: 0.9767 - auc: 0.9955 - val_loss: 0.0466 - val_accuracy: 0.9820 - val_auc: 0.9986\n",
            "\n",
            "Starting Epoch 6...\n",
            "Epoch 6/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9809 - auc: 0.9969\n",
            "Epoch 6: val_loss improved from 0.04657 to 0.03719, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 6, Loss: 0.0526, Val Loss: 0.0372\n",
            "344/344 [==============================] - 144s 418ms/step - loss: 0.0526 - accuracy: 0.9809 - auc: 0.9969 - val_loss: 0.0372 - val_accuracy: 0.9871 - val_auc: 0.9992\n",
            "\n",
            "Starting Epoch 7...\n",
            "Epoch 7/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9797 - auc: 0.9963\n",
            "Epoch 7: val_loss did not improve from 0.03719\n",
            "Completed Epoch 7, Loss: 0.0550, Val Loss: 0.0430\n",
            "344/344 [==============================] - 127s 370ms/step - loss: 0.0550 - accuracy: 0.9797 - auc: 0.9963 - val_loss: 0.0430 - val_accuracy: 0.9831 - val_auc: 0.9984\n",
            "\n",
            "Starting Epoch 8...\n",
            "Epoch 8/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9803 - auc: 0.9970\n",
            "Epoch 8: val_loss did not improve from 0.03719\n",
            "Completed Epoch 8, Loss: 0.0522, Val Loss: 0.0414\n",
            "344/344 [==============================] - 131s 379ms/step - loss: 0.0522 - accuracy: 0.9803 - auc: 0.9970 - val_loss: 0.0414 - val_accuracy: 0.9856 - val_auc: 0.9993\n",
            "\n",
            "Starting Epoch 9...\n",
            "Epoch 9/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9846 - auc: 0.9982\n",
            "Epoch 9: val_loss improved from 0.03719 to 0.02995, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 9, Loss: 0.0428, Val Loss: 0.0299\n",
            "344/344 [==============================] - 136s 397ms/step - loss: 0.0428 - accuracy: 0.9846 - auc: 0.9982 - val_loss: 0.0299 - val_accuracy: 0.9889 - val_auc: 0.9985\n",
            "\n",
            "Starting Epoch 10...\n",
            "Epoch 10/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9821 - auc: 0.9979\n",
            "Epoch 10: val_loss improved from 0.02995 to 0.02662, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 10, Loss: 0.0462, Val Loss: 0.0266\n",
            "344/344 [==============================] - 150s 436ms/step - loss: 0.0462 - accuracy: 0.9821 - auc: 0.9979 - val_loss: 0.0266 - val_accuracy: 0.9914 - val_auc: 0.9995\n",
            "\n",
            "Starting Epoch 11...\n",
            "Epoch 11/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9844 - auc: 0.9977\n",
            "Epoch 11: val_loss did not improve from 0.02662\n",
            "Completed Epoch 11, Loss: 0.0433, Val Loss: 0.0305\n",
            "344/344 [==============================] - 136s 395ms/step - loss: 0.0433 - accuracy: 0.9844 - auc: 0.9977 - val_loss: 0.0305 - val_accuracy: 0.9874 - val_auc: 0.9992\n",
            "\n",
            "Starting Epoch 12...\n",
            "Epoch 12/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9840 - auc: 0.9976\n",
            "Epoch 12: val_loss did not improve from 0.02662\n",
            "Completed Epoch 12, Loss: 0.0440, Val Loss: 0.0370\n",
            "344/344 [==============================] - 137s 398ms/step - loss: 0.0440 - accuracy: 0.9840 - auc: 0.9976 - val_loss: 0.0370 - val_accuracy: 0.9867 - val_auc: 0.9989\n",
            "\n",
            "Starting Epoch 13...\n",
            "Epoch 13/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9856 - auc: 0.9981\n",
            "Epoch 13: val_loss improved from 0.02662 to 0.02209, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 13, Loss: 0.0398, Val Loss: 0.0221\n",
            "344/344 [==============================] - 134s 389ms/step - loss: 0.0398 - accuracy: 0.9856 - auc: 0.9981 - val_loss: 0.0221 - val_accuracy: 0.9910 - val_auc: 0.9997\n",
            "\n",
            "Starting Epoch 14...\n",
            "Epoch 14/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9840 - auc: 0.9980\n",
            "Epoch 14: val_loss did not improve from 0.02209\n",
            "Completed Epoch 14, Loss: 0.0410, Val Loss: 0.0249\n",
            "344/344 [==============================] - 137s 398ms/step - loss: 0.0410 - accuracy: 0.9840 - auc: 0.9980 - val_loss: 0.0249 - val_accuracy: 0.9921 - val_auc: 0.9995\n",
            "\n",
            "Starting Epoch 15...\n",
            "Epoch 15/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9865 - auc: 0.9987\n",
            "Epoch 15: val_loss improved from 0.02209 to 0.01700, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 15, Loss: 0.0361, Val Loss: 0.0170\n",
            "344/344 [==============================] - 134s 389ms/step - loss: 0.0361 - accuracy: 0.9865 - auc: 0.9987 - val_loss: 0.0170 - val_accuracy: 0.9935 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 16...\n",
            "Epoch 16/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9874 - auc: 0.9987\n",
            "Epoch 16: val_loss did not improve from 0.01700\n",
            "Completed Epoch 16, Loss: 0.0342, Val Loss: 0.0196\n",
            "344/344 [==============================] - 128s 372ms/step - loss: 0.0342 - accuracy: 0.9874 - auc: 0.9987 - val_loss: 0.0196 - val_accuracy: 0.9910 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 17...\n",
            "Epoch 17/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9852 - auc: 0.9981\n",
            "Epoch 17: val_loss did not improve from 0.01700\n",
            "Completed Epoch 17, Loss: 0.0412, Val Loss: 0.0253\n",
            "344/344 [==============================] - 130s 377ms/step - loss: 0.0412 - accuracy: 0.9852 - auc: 0.9981 - val_loss: 0.0253 - val_accuracy: 0.9899 - val_auc: 0.9996\n",
            "\n",
            "Starting Epoch 18...\n",
            "Epoch 18/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9886 - auc: 0.9981\n",
            "Epoch 18: val_loss did not improve from 0.01700\n",
            "Completed Epoch 18, Loss: 0.0336, Val Loss: 0.0210\n",
            "344/344 [==============================] - 128s 372ms/step - loss: 0.0336 - accuracy: 0.9886 - auc: 0.9981 - val_loss: 0.0210 - val_accuracy: 0.9925 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 19...\n",
            "Epoch 19/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9879 - auc: 0.9981\n",
            "Epoch 19: val_loss did not improve from 0.01700\n",
            "Completed Epoch 19, Loss: 0.0340, Val Loss: 0.0221\n",
            "344/344 [==============================] - 128s 373ms/step - loss: 0.0340 - accuracy: 0.9879 - auc: 0.9981 - val_loss: 0.0221 - val_accuracy: 0.9939 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 20...\n",
            "Epoch 20/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9878 - auc: 0.9988\n",
            "Epoch 20: val_loss did not improve from 0.01700\n",
            "Completed Epoch 20, Loss: 0.0320, Val Loss: 0.0181\n",
            "344/344 [==============================] - 128s 372ms/step - loss: 0.0320 - accuracy: 0.9878 - auc: 0.9988 - val_loss: 0.0181 - val_accuracy: 0.9935 - val_auc: 0.9997\n",
            "\n",
            "Starting Epoch 21...\n",
            "Epoch 21/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9891 - auc: 0.9988\n",
            "Epoch 21: val_loss did not improve from 0.01700\n",
            "Completed Epoch 21, Loss: 0.0302, Val Loss: 0.0206\n",
            "344/344 [==============================] - 127s 368ms/step - loss: 0.0302 - accuracy: 0.9891 - auc: 0.9988 - val_loss: 0.0206 - val_accuracy: 0.9914 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 22...\n",
            "Epoch 22/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9884 - auc: 0.9986\n",
            "Epoch 22: val_loss did not improve from 0.01700\n",
            "Completed Epoch 22, Loss: 0.0315, Val Loss: 0.0172\n",
            "344/344 [==============================] - 124s 361ms/step - loss: 0.0315 - accuracy: 0.9884 - auc: 0.9986 - val_loss: 0.0172 - val_accuracy: 0.9939 - val_auc: 0.9999\n",
            "\n",
            "Starting Epoch 23...\n",
            "Epoch 23/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9865 - auc: 0.9984\n",
            "Epoch 23: val_loss did not improve from 0.01700\n",
            "Completed Epoch 23, Loss: 0.0343, Val Loss: 0.0218\n",
            "344/344 [==============================] - 124s 362ms/step - loss: 0.0343 - accuracy: 0.9865 - auc: 0.9984 - val_loss: 0.0218 - val_accuracy: 0.9889 - val_auc: 0.9998\n",
            "\n",
            "Starting Epoch 24...\n",
            "Epoch 24/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9886 - auc: 0.9992\n",
            "Epoch 24: val_loss improved from 0.01700 to 0.01330, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 24, Loss: 0.0302, Val Loss: 0.0133\n",
            "344/344 [==============================] - 126s 366ms/step - loss: 0.0302 - accuracy: 0.9886 - auc: 0.9992 - val_loss: 0.0133 - val_accuracy: 0.9960 - val_auc: 0.9999\n",
            "\n",
            "Starting Epoch 25...\n",
            "Epoch 25/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9888 - auc: 0.9988\n",
            "Epoch 25: val_loss improved from 0.01330 to 0.01297, saving model to /home/apyba3/MobNetV2/mobnetv2checkpoint.h5\n",
            "Completed Epoch 25, Loss: 0.0294, Val Loss: 0.0130\n",
            "344/344 [==============================] - 122s 355ms/step - loss: 0.0294 - accuracy: 0.9888 - auc: 0.9988 - val_loss: 0.0130 - val_accuracy: 0.9964 - val_auc: 0.9999\n",
            "\n",
            "Starting Epoch 26...\n",
            "Epoch 26/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9905 - auc: 0.9995\n",
            "Epoch 26: val_loss did not improve from 0.01297\n",
            "Completed Epoch 26, Loss: 0.0250, Val Loss: 0.0147\n",
            "344/344 [==============================] - 121s 352ms/step - loss: 0.0250 - accuracy: 0.9905 - auc: 0.9995 - val_loss: 0.0147 - val_accuracy: 0.9950 - val_auc: 0.9999\n",
            "\n",
            "Starting Epoch 27...\n",
            "Epoch 27/45\n",
            "344/344 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9901 - auc: 0.9987"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[75], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m epoch_callback \u001b[38;5;241m=\u001b[39m LambdaCallback(\n\u001b[1;32m     14\u001b[0m     on_epoch_begin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, logs: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Training loop with added callback\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include both callbacks\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define ModelCheckpoint callback\n",
        "checkpoint_filepath = '/home/apyba3/MobNetV2/mobnetv2checkpoint.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,  # Save only weights instead of full model\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define a callback to print epoch tracking info\n",
        "epoch_callback = LambdaCallback(\n",
        "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
        "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
        ")\n",
        "\n",
        "# Training loop with added callback\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=45,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[model_checkpoint, epoch_callback]  # Include both callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "FiHy6opSP2sQ"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV2 - Kaggle/frozentraining_mobnetv2_classification_weights.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "clear keras session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpLHyw20P93U"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session() #Clear keras session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2c) fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "rebuild model after clearing keras session\n",
        "\n",
        "### UNFREEZE MOBNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuKL3X-QP-Ax"
      },
      "outputs": [],
      "source": [
        "mobnetv2 =  tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  mobnetv2,\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dropout(dropoutrate),\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mobnetv2.trainable = True  # UNfreeze mobnetv3small layers\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # deliberately smaller learning rate\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "now load the learned weights from frozen training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oAenzEiP-C-"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/home/apyba3/PICAR-autopilot/MobNetV2 - Kaggle/frozentraining_mobnetv2_classification_weights.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "initiate fine-tuning training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define ModelCheckpoint callback\n",
        "checkpoint_filepath = '/home/apyba3/MobNetV2/mobnetv2checkpoint.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,  # Save only weights instead of full model\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define a callback to print epoch tracking info\n",
        "epoch_callback = LambdaCallback(\n",
        "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
        "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
        ")\n",
        "\n",
        "# Training loop with added callback\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=45,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[model_checkpoint, epoch_callback]  # Include both callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "save the weights learned from fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights('/home/apyba3/PICAR-autopilot/MobNetV2 - Kaggle/FINETUNINGtraining_mobnetv2_classification_weights.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3) CLASSIFICATION SPEED Test-Set Predictions\n",
        "\n",
        "a) load in test data\n",
        "\n",
        "b) convert test images to numerical RGB feature maps\n",
        "\n",
        "c) generate predictions on the test set\n",
        "\n",
        "d) correctly format the predictions into a pandas dataframe\n",
        "\n",
        "e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3a) load in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data'\n",
        "# image_folder_path = '/home/ppyt13/machine-learning-in-science-ii-2025/test_data/test_data' # tylers file path\n",
        "image_file_paths = [\n",
        "    os.path.join(image_folder_path, f)\n",
        "    for f in os.listdir(image_folder_path)\n",
        "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "]\n",
        "\n",
        "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
        "\n",
        "imagefilepaths_df = pd.DataFrame(\n",
        "    image_file_paths,\n",
        "    columns=['image_file_paths'],\n",
        "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
        ")\n",
        "\n",
        "imagefilepaths_df.index.name = 'image_id'\n",
        "imagefilepaths_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b) convert test images to numerical RGB feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_image_no_label(image_path, resized_shape=(224, 224)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)  # Use decode_png for PNG images\n",
        "    image = tf.image.resize(image, resized_shape)  # Resize to uniform shape\n",
        "    image = image / 255.0  # Normalize pixel values to [0,1]\n",
        "    return image\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((imagefilepaths_df[\"image_file_paths\"]))\n",
        "\n",
        "test_dataset = test_dataset.map(process_image_no_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3c) generate predictions on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3d) correctly format the predictions into a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame(predictions, columns=['speed'])\n",
        "\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df[predictions_df['speed'] > 0.5] = 1\n",
        "predictions_df[predictions_df['speed'] < 0.5] = 0\n",
        "\n",
        "predictions_df['speed'] = predictions_df['speed'].astype(int)\n",
        "\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df['speed'].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3e) save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df.to_csv('/home/apyba3/PICAR-autopilot/MobNetV2 - Kaggle/mobnetv2_speedclassification_withvalidation.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "car_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
