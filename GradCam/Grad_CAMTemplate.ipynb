{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylaar1/PICAR-autopilot/blob/main/Grad_CAMTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IetkYdHPM38E"
      },
      "source": [
        "# Template for Grad-CAM\n",
        "Note: Need to add our own trained model first for it to work.\n",
        "\n",
        "what it does?\n",
        "\n",
        "\n",
        "1.   Visiulases how the CNN is making certain decisions\n",
        "2.   It will highlight which areas of our image it is taking into account to make its decision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yNpX0RnfMy9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-07 12:43:57.279633: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (2 for input with 2 dimension(s)\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (2 for input with 2 dimension(s) [Op:Mean]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m img_array \u001b[38;5;241m=\u001b[39m process_image(image_path)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Compute Grad-CAM heatmap\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Change \"conv2d\" to your CNN layer\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Overlay the heatmap on the original image\u001b[39;00m\n\u001b[1;32m     56\u001b[0m overlayed_image \u001b[38;5;241m=\u001b[39m overlay_heatmap(image_path, heatmap)\n",
            "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mcompute_gradcam\u001b[0;34m(model, img_array, layer_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m predictions  \u001b[38;5;66;03m# Assuming regression (steering angle)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, conv_outputs)\n\u001b[0;32m---> 30\u001b[0m pooled_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmultiply(pooled_grads, conv_outputs), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(heatmap[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# ReLU activation\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (2 for input with 2 dimension(s) [Op:Mean]"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your trained autonomous car model\n",
        "model = tf.keras.models.load_model(\"mobnet.h5\")\n",
        "\n",
        "# Function to preprocess input image\n",
        "def process_image(image_path, resized_shape=(224, 224)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, resized_shape)\n",
        "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
        "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Grad-CAM function\n",
        "def compute_gradcam(model, img_array, layer_name=\"conv2d\"):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=[model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions  # Assuming regression (steering angle)\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
        "\n",
        "    heatmap = np.maximum(heatmap[0], 0)  # ReLU activation\n",
        "    heatmap /= np.max(heatmap)  # Normalize between 0 and 1\n",
        "    return heatmap\n",
        "\n",
        "# Overlay Grad-CAM heatmap on original image\n",
        "def overlay_heatmap(img_path, heatmap, alpha=0.4):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)  # Convert to 0-255 scale\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # Apply color map\n",
        "    overlayed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
        "\n",
        "    return overlayed_img\n",
        "\n",
        "# Load an example image from your dataset\n",
        "image_path = \"/home/ppytr13/machine-learning-in-science-ii-2025/training_data/training_data/1.png\"  # Change to an actual image path\n",
        "img_array = process_image(image_path)\n",
        "\n",
        "# Compute Grad-CAM heatmap\n",
        "heatmap = compute_gradcam(model, img_array, layer_name=\"classification\")  # Change \"conv2d\" to your CNN layer\n",
        "\n",
        "# Overlay the heatmap on the original image\n",
        "overlayed_image = overlay_heatmap(image_path, heatmap)\n",
        "\n",
        "# Display the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.imread(image_path)[:, :, ::-1])\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(overlayed_image)\n",
        "plt.title(\"Grad-CAM Overlay\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model layers:\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " MobilenetV3small (Function  (None, 7, 7, 576)            939120    ['input[0][0]']               \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " global_avg_pool (GlobalAve  (None, 576)                  0         ['MobilenetV3small[0][0]']    \n",
            " ragePooling2D)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_shared (Dropout)    (None, 576)                  0         ['global_avg_pool[0][0]']     \n",
            "                                                                                                  \n",
            " dense_class_1 (Dense)       (None, 64)                   36928     ['dropout_shared[0][0]']      \n",
            "                                                                                                  \n",
            " dense_reg_1 (Dense)         (None, 64)                   36928     ['dropout_shared[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_class (Dropout)     (None, 64)                   0         ['dense_class_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_reg (Dropout)       (None, 64)                   0         ['dense_reg_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_class_2 (Dense)       (None, 32)                   2080      ['dropout_class[0][0]']       \n",
            "                                                                                                  \n",
            " dense_reg_2 (Dense)         (None, 32)                   2080      ['dropout_reg[0][0]']         \n",
            "                                                                                                  \n",
            " classification (Dense)      (None, 1)                    33        ['dense_class_2[0][0]']       \n",
            "                                                                                                  \n",
            " regression (Dense)          (None, 1)                    33        ['dense_reg_2[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1017202 (3.88 MB)\n",
            "Trainable params: 1005090 (3.83 MB)\n",
            "Non-trainable params: 12112 (47.31 KB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Selected layer: None\n",
            "Using layer: MobilenetV3small\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-07 12:56:23.165718: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (2 for input with 2 dimension(s)\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (2 for input with 2 dimension(s) [Op:Mean]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSelected layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_conv_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate and display Grad-CAM\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m heatmap, overlay, prediction \u001b[38;5;241m=\u001b[39m \u001b[43mdisplay_gradcam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_conv_layer\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify layer or leave as None for auto-detection\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[5], line 110\u001b[0m, in \u001b[0;36mdisplay_gradcam\u001b[0;34m(image_path, model, target_layer)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a suitable layer for Grad-CAM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Compute Grad-CAM\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Get model prediction\u001b[39;00m\n\u001b[1;32m    113\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img_array)\n",
            "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mcompute_gradcam\u001b[0;34m(model, img_array, layer_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(target_output, conv_outputs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m pooled_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Weight the channels by importance\u001b[39;00m\n\u001b[1;32m     54\u001b[0m weighted_conv_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(conv_outputs[\u001b[38;5;241m0\u001b[39m], pooled_grads)\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/car_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (2 for input with 2 dimension(s) [Op:Mean]"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your trained autonomous car model\n",
        "model = tf.keras.models.load_model(\"mobnet.h5\")\n",
        "\n",
        "# Function to preprocess input image\n",
        "def process_image(image_path, resized_shape=(224, 224)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, resized_shape)\n",
        "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
        "    # Add batch dimension\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "# Grad-CAM function\n",
        "def compute_gradcam(model, img_array, layer_name):\n",
        "    # First, ensure the specified layer exists\n",
        "    try:\n",
        "        grad_model = tf.keras.models.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=[model.get_layer(layer_name).output, model.output]\n",
        "        )\n",
        "    except ValueError:\n",
        "        # If layer not found, find a suitable convolutional layer\n",
        "        conv_layers = [layer.name for layer in model.layers if 'conv' in layer.name.lower()]\n",
        "        if not conv_layers:\n",
        "            raise ValueError(\"No convolutional layer found in the model\")\n",
        "        print(f\"Layer '{layer_name}' not found. Using '{conv_layers[-1]}' instead.\")\n",
        "        layer_name = conv_layers[-1]\n",
        "        grad_model = tf.keras.models.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=[model.get_layer(layer_name).output, model.output]\n",
        "        )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if isinstance(predictions, list):\n",
        "            predictions = predictions[0]  # Handle multiple outputs\n",
        "        \n",
        "        # For regression task (steering angle)\n",
        "        target_output = predictions  # Use the raw prediction\n",
        "\n",
        "    # Calculate gradients\n",
        "    grads = tape.gradient(target_output, conv_outputs)\n",
        "    \n",
        "    # Global average pooling\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    \n",
        "    # Weight the channels by importance\n",
        "    weighted_conv_outputs = tf.multiply(conv_outputs[0], pooled_grads)\n",
        "    \n",
        "    # Average across feature maps\n",
        "    heatmap = tf.reduce_mean(weighted_conv_outputs, axis=-1)\n",
        "    \n",
        "    # ReLU activation & normalization\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    if np.max(heatmap) > 0:  # Avoid division by zero\n",
        "        heatmap /= np.max(heatmap)\n",
        "    \n",
        "    return heatmap\n",
        "\n",
        "# Overlay Grad-CAM heatmap on original image\n",
        "def overlay_heatmap(img_path, heatmap, alpha=0.4):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not read image at {img_path}\")\n",
        "    \n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize heatmap to match the original image dimensions\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    \n",
        "    # Convert heatmap to RGB colormap\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert ColorMap from BGR to RGB\n",
        "    \n",
        "    # Overlay the heatmap on original image\n",
        "    overlayed_img = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
        "    \n",
        "    return overlayed_img\n",
        "\n",
        "# Display function\n",
        "def display_gradcam(image_path, model, target_layer=None):\n",
        "    # Process the image\n",
        "    img_array = process_image(image_path)\n",
        "    \n",
        "    # If no target layer specified, try to find a suitable one\n",
        "    if target_layer is None:\n",
        "        # Find the last convolutional layer\n",
        "        conv_layers = [layer.name for layer in model.layers if 'conv' in layer.name.lower()]\n",
        "        if conv_layers:\n",
        "            target_layer = conv_layers[-1]\n",
        "            print(f\"Using layer: {target_layer}\")\n",
        "        else:\n",
        "            # Try to find any layer that might work\n",
        "            suitable_layers = [layer.name for layer in model.layers \n",
        "                             if len(layer.output_shape) == 4]  # 4D output should be a feature map\n",
        "            if suitable_layers:\n",
        "                target_layer = suitable_layers[-1]\n",
        "                print(f\"Using layer: {target_layer}\")\n",
        "            else:\n",
        "                raise ValueError(\"Couldn't find a suitable layer for Grad-CAM\")\n",
        "    \n",
        "    # Compute Grad-CAM\n",
        "    heatmap = compute_gradcam(model, img_array, layer_name='classification')\n",
        "    \n",
        "    # Get model prediction\n",
        "    prediction = model.predict(img_array)\n",
        "    if isinstance(prediction, list):\n",
        "        prediction = prediction[0]  # Handle multiple outputs\n",
        "    \n",
        "    # Create the visualization\n",
        "    overlayed_image = overlay_heatmap(image_path, heatmap)\n",
        "    \n",
        "    # Display results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 3, 1)\n",
        "    original_img = cv2.imread(image_path)\n",
        "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(heatmap, cmap='jet')\n",
        "    plt.title(\"Grad-CAM Heatmap\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(overlayed_image)\n",
        "    plt.title(f\"Overlay (Prediction: {prediction[0]:.4f})\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return heatmap, overlayed_image, prediction\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/home/ppytr13/machine-learning-in-science-ii-2025/training_data/training_data/1.png\"\n",
        "    \n",
        "    # Print model summary to find layers\n",
        "    print(\"Model layers:\")\n",
        "    model.summary()\n",
        "    \n",
        "    # Try to automatically find a good layer or specify a known one\n",
        "    # For MobileNet, a good choice might be the last convolutional layer\n",
        "    last_conv_layer = None\n",
        "    for layer in model.layers:\n",
        "        if 'conv' in layer.name.lower():\n",
        "            last_conv_layer = layer.name\n",
        "    \n",
        "    print(f\"\\nSelected layer: {last_conv_layer}\")\n",
        "    \n",
        "    # Generate and display Grad-CAM\n",
        "    heatmap, overlay, prediction = display_gradcam(\n",
        "        image_path=image_path,\n",
        "        model=model,\n",
        "        target_layer=last_conv_layer  # Specify layer or leave as None for auto-detection\n",
        "    )\n",
        "    \n",
        "    print(f\"Model prediction: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure:\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " MobilenetV3small (Function  (None, 7, 7, 576)            939120    ['input[0][0]']               \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " global_avg_pool (GlobalAve  (None, 576)                  0         ['MobilenetV3small[0][0]']    \n",
            " ragePooling2D)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_shared (Dropout)    (None, 576)                  0         ['global_avg_pool[0][0]']     \n",
            "                                                                                                  \n",
            " dense_class_1 (Dense)       (None, 64)                   36928     ['dropout_shared[0][0]']      \n",
            "                                                                                                  \n",
            " dense_reg_1 (Dense)         (None, 64)                   36928     ['dropout_shared[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_class (Dropout)     (None, 64)                   0         ['dense_class_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_reg (Dropout)       (None, 64)                   0         ['dense_reg_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_class_2 (Dense)       (None, 32)                   2080      ['dropout_class[0][0]']       \n",
            "                                                                                                  \n",
            " dense_reg_2 (Dense)         (None, 32)                   2080      ['dropout_reg[0][0]']         \n",
            "                                                                                                  \n",
            " classification (Dense)      (None, 1)                    33        ['dense_class_2[0][0]']       \n",
            "                                                                                                  \n",
            " regression (Dense)          (None, 1)                    33        ['dense_reg_2[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1017202 (3.88 MB)\n",
            "Trainable params: 1005090 (3.83 MB)\n",
            "Non-trainable params: 12112 (47.31 KB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Potential layers for Grad-CAM:\n",
            "No suitable convolutional layer found for Grad-CAM\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"mobnet.h5\")\n",
        "\n",
        "# Print model summary to find usable layers\n",
        "print(\"Model structure:\")\n",
        "model.summary()\n",
        "\n",
        "def create_gradcam(img_path, model, last_conv_layer_name, pred_index=None):\n",
        "    # Preprocess the image\n",
        "    img_array = preprocess_image(img_path)\n",
        "    \n",
        "    # Create a model that maps the input image to:\n",
        "    # 1. The activations of the last conv layer\n",
        "    # 2. The model's predictions\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], \n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    \n",
        "    # Then, we compute the gradient of the predicted output with respect to the feature map\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        \n",
        "        # We use the first output (regression value) for gradient calculation\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        \n",
        "        # This is the regression value\n",
        "        target_output = preds[:, pred_index]\n",
        "    \n",
        "    # The gradient of the output with respect to the last conv layer output\n",
        "    grads = tape.gradient(target_output, last_conv_layer_output)\n",
        "    \n",
        "    # Vector of shape (channels,) - importance weights for each channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    \n",
        "    # Shape: (height, width, channels)\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    \n",
        "    # Multiply each channel by how important it is\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    \n",
        "    # Remove the last dimension and normalize\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    \n",
        "    # Normalize to [0, 1]\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    \n",
        "    # Convert to numpy and return\n",
        "    return heatmap.numpy(), preds.numpy()\n",
        "\n",
        "def preprocess_image(img_path, target_size=(224, 224)):\n",
        "    # Read and resize the image\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    \n",
        "    # Expand dimensions for batch and normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0\n",
        "    \n",
        "    return img_array\n",
        "\n",
        "def save_and_display_gradcam(img_path, heatmap, prediction, alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize heatmap to match the image size\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    \n",
        "    # Convert heatmap to RGB\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
        "    \n",
        "    # Display the original image, heatmap, and combined image\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(heatmap)\n",
        "    plt.title(\"Grad-CAM Heatmap\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(superimposed_img)\n",
        "    pred_val = prediction[0][0] if isinstance(prediction[0], np.ndarray) else prediction[0]\n",
        "    plt.title(f\"Overlay (Pred: {pred_val:.4f})\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return superimposed_img\n",
        "\n",
        "# Try to identify reasonable layers to use\n",
        "def find_conv_layers(model):\n",
        "    conv_layers = []\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        # Look for conv layers\n",
        "        if 'conv' in layer.name.lower() and hasattr(layer, 'output_shape'):\n",
        "            if len(layer.output_shape) == 4:  # Should be 4D for feature maps\n",
        "                conv_layers.append((i, layer.name, layer.output_shape))\n",
        "    \n",
        "    return conv_layers\n",
        "\n",
        "# Find convolutional layers\n",
        "conv_layers = find_conv_layers(model)\n",
        "print(\"\\nPotential layers for Grad-CAM:\")\n",
        "for idx, name, shape in conv_layers:\n",
        "    print(f\"{idx}: {name} (shape: {shape})\")\n",
        "\n",
        "# Select a layer to use (usually one of the last conv layers)\n",
        "# Choose based on the model summary - this is just a placeholder\n",
        "try:\n",
        "    # Try to find a good layer automatically\n",
        "    # MobileNet typically has layers with names like 'conv_pw_13' near the end\n",
        "    last_conv_names = ['conv_pw_13', 'block_16_project', 'Conv_1']\n",
        "    \n",
        "    # Find the first matching layer\n",
        "    last_conv_layer = None\n",
        "    for name_pattern in last_conv_names:\n",
        "        matching_layers = [l[1] for l in conv_layers if name_pattern in l[1]]\n",
        "        if matching_layers:\n",
        "            last_conv_layer = matching_layers[0]\n",
        "            break\n",
        "    \n",
        "    # If no match found, use the last conv layer\n",
        "    if not last_conv_layer and conv_layers:\n",
        "        last_conv_layer = conv_layers[-1][1]\n",
        "    \n",
        "    if last_conv_layer:\n",
        "        print(f\"\\nUsing layer: {last_conv_layer}\")\n",
        "        \n",
        "        # Image path (replace with your actual path)\n",
        "        img_path = \"/home/ppytr13/machine-learning-in-science-ii-2025/training_data/training_data/1.png\"\n",
        "        \n",
        "        # Generate Grad-CAM\n",
        "        heatmap, prediction = create_gradcam(\n",
        "            img_path=img_path,\n",
        "            model=model,\n",
        "            last_conv_layer_name=last_conv_layer\n",
        "        )\n",
        "        \n",
        "        # Display the results\n",
        "        save_and_display_gradcam(img_path, heatmap, prediction)\n",
        "        \n",
        "        print(f\"Model prediction: {prediction}\")\n",
        "    else:\n",
        "        print(\"No suitable convolutional layer found for Grad-CAM\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error generating Grad-CAM: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPl27GbaApz71yqgs8LSOPp",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "car_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
