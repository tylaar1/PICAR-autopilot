{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fhwRSFoj6C_"
   },
   "source": [
    "# SWITCH TO **`T4 GPU`** OR THE **`HPC`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4V83PflfFkL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kP6UczzNe1l2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:25:37.096972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-25 14:25:37.187902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-25 14:25:37.210890: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 14:25:37.516074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O24_U-m8q-xv",
    "outputId": "f2298893-2e7e-4b8f-cc38-0caeb1a6a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IF_vPVifaU9V"
   },
   "outputs": [],
   "source": [
    "# makes it so pd dfs aren't truncated\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eocC68amnhEI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_MvRvYnfIM5"
   },
   "source": [
    "# 1) DATA PRE-PROCESSING\n",
    "\n",
    "a) Load in kaggle data labels + image file paths\n",
    "\n",
    "b) combine kaggle data labels and image file paths into one dataframe\n",
    "\n",
    "c) load in the extra 486 image file paths\n",
    "\n",
    "d) extract the speed and angle labels from the file path names\n",
    "\n",
    "e) store that extra data in a pandas df and do the value normalisation\n",
    "\n",
    "f) merge the kaggle and extra data dfs\n",
    "\n",
    "g) EDA\n",
    "\n",
    "h) convert the images to numerical RGB feature maps\n",
    "\n",
    "i) split data into training-validation sets\n",
    "\n",
    "j) data augmentation applied to training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU3TvBZ5hfhX"
   },
   "source": [
    "### 1a) load in kaggle data labels + image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZiNf_BxOfEH-"
   },
   "outputs": [],
   "source": [
    "# labels_file_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_norm.csv' # tylers file path\n",
    "labels_file_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_norm.csv' # ben hpc file path (mlis2 cluster)\n",
    "# labels_file_path = '/home/ppytr13/machine-learning-in-science-ii-2025/training_norm.csv' # tyler hpc file path (mlis2 cluster)\n",
    "labels_df = pd.read_csv(labels_file_path, index_col='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nOXmN--gb-Q9"
   },
   "outputs": [],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data' # OG data ben hpc file path (mlis2 cluster)\n",
    "# image_folder_path = '/home/ppytr13/machine-learning-in-science-ii-2025//training_data/training_data'\n",
    "# image_folder_path = '/content/drive/MyDrive/machine-learning-in-science-ii-2025/training_data/training_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oeuvmeZaGSC"
   },
   "source": [
    "Checking labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pi13TZ2aFhO",
    "outputId": "fc675bb2-271b-48fd-a6c3-43834afb4500"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed\n",
       "image_id               \n",
       "1         0.4375    0.0\n",
       "2         0.8125    1.0\n",
       "3         0.4375    1.0\n",
       "4         0.6250    1.0\n",
       "5         0.5000    0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puEjGoOJaRS4"
   },
   "source": [
    "Checking image file paths dataframe - as you can see the file paths are ordered correctly (1.png, 2.png, 3.png, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1suFSK7aWKH",
    "outputId": "c3cc2d29-d759-48ff-b92c-77dbd178f295"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      image_file_paths\n",
       "image_id                                                                                              \n",
       "1         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/1.png\n",
       "2         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/2.png\n",
       "3         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3.png\n",
       "4         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/4.png\n",
       "5         /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/5.png"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjDdyYd6cMBE"
   },
   "source": [
    "### 1b) Combine the kaggle labels and image file paths into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6NdbonzPcLKB"
   },
   "outputs": [],
   "source": [
    "kaggle_df = pd.merge(labels_df, imagefilepaths_df, on='image_id', how='inner')\n",
    "kaggle_df['speed'] = kaggle_df['speed'].round(6) # to get rid of floating point errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VstirIAdAZi",
    "outputId": "c03ff707-9e8d-4c3a-8965-f795919ace21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13796</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13794     0.6250    1.0   \n",
       "13795     0.4375    1.0   \n",
       "13796     0.5625    0.0   \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13794     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13794.png  \n",
       "13795     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13795.png  \n",
       "13796     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13796.png  \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MgNoL8nfBm2",
    "outputId": "924e7562-25a4-4223-8305-c3fd02452846"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          angle  speed  \\\n",
       "image_id                 \n",
       "3139      0.750    1.0   \n",
       "3140      0.875    1.0   \n",
       "3142      0.625    0.0   \n",
       "3143      0.625    1.0   \n",
       "\n",
       "                                                                                         image_file_paths  \n",
       "image_id                                                                                                   \n",
       "3139      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3139.png  \n",
       "3140      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3140.png  \n",
       "3142      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3142.png  \n",
       "3143      /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/3143.png  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.loc[3139:3143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7PCxqJbmXE6"
   },
   "source": [
    "The above cell shows that:\n",
    "\n",
    " 1) the image files and labels match (see image_id and the number at the end of the file path)\n",
    "\n",
    " 2) the missing rows in labels_df (image_id: 3141, 3999, 4895, 8285, 10171) have been taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOEWqBUYX6DL"
   },
   "source": [
    "### 1c) load in the extra 486 labels image file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wvsDiCCLOvvs"
   },
   "outputs": [],
   "source": [
    "extradata_folder_path = '/home/apyba3/petru_data'\n",
    "\n",
    "extradata_file_paths = [\n",
    "    os.path.join(extradata_folder_path, f)\n",
    "    for f in os.listdir(extradata_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ofcGILO4et"
   },
   "source": [
    "### 1d) extract the speed and angle labels from the file path names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFsEI4MBRf2l"
   },
   "source": [
    "image file path name follows the pattern: `randomnumber_angle_speed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mY5-HDp-PJY9"
   },
   "outputs": [],
   "source": [
    "# Regex pattern to extract angle and speed values\n",
    "pattern = r'(\\d+)_([\\d]+)_([\\d]+)\\.png'\n",
    "\n",
    "angle_value = []\n",
    "speed_value = []\n",
    "\n",
    "# Loop through file paths and extract angle and speed values\n",
    "for file_path in extradata_file_paths:\n",
    "    match = re.search(pattern, file_path)\n",
    "    if match:\n",
    "        # Extract random number, angle, and speed values\n",
    "        random_number = match.group(1)\n",
    "        angle_value.append(int(match.group(2)))\n",
    "        speed_value.append(int(match.group(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F8qIQJ8Y3t8"
   },
   "source": [
    "checking it has stored the labels correctly (check if the angle_value order matches that of the file path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf1bChw_OvsT",
    "outputId": "bdf648d9-3ab3-403e-c977-0c938ae1bf18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 100, 80]\n",
      "['/home/apyba3/petru_data/1712918428740_95_0.png', '/home/apyba3/petru_data/1712923220525_100_50.png', '/home/apyba3/petru_data/1712923068961_80_35.png']\n"
     ]
    }
   ],
   "source": [
    "print(angle_value[:3])\n",
    "print(extradata_file_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyvljUTBZP0E"
   },
   "source": [
    "### 1e) store that extra data in a pandas df and do the value normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tse95lu1OvnY",
    "outputId": "90ed60a7-5f9c-4901-f7ed-7442d739ccfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13801</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712923068961_80_35.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13802</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/apyba3/petru_data/1712921566265_105_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13803</th>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/apyba3/petru_data/1712915924250_70_35.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed                                  image_file_paths\n",
       "image_id                                                                 \n",
       "13799     0.5625      0    /home/apyba3/petru_data/1712918428740_95_0.png\n",
       "13800     0.6250      1  /home/apyba3/petru_data/1712923220525_100_50.png\n",
       "13801     0.3750      1   /home/apyba3/petru_data/1712923068961_80_35.png\n",
       "13802     0.6875      0   /home/apyba3/petru_data/1712921566265_105_0.png\n",
       "13803     0.2500      1   /home/apyba3/petru_data/1712915924250_70_35.png"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extradata_df = pd.DataFrame({\n",
    "    'angle': angle_value,\n",
    "    'speed': speed_value,\n",
    "    'image_file_paths': extradata_file_paths\n",
    "})\n",
    "\n",
    "# conversions (see kaggle data section)\n",
    "extradata_df.loc[extradata_df['speed'] > 0, 'speed'] = 1\n",
    "extradata_df['speed'] = pd.to_numeric(extradata_df['speed'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "extradata_df['angle'] = (extradata_df['angle'] - 50)/80\n",
    "\n",
    "extradata_df.index = pd.RangeIndex(start=13799, stop=13799 + len(extradata_df), step=1)\n",
    "extradata_df.index.name = 'image_id'\n",
    "\n",
    "extradata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv0MwDKsbOef"
   },
   "source": [
    "### 1f) merge the kaggle and extra data dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZMZPUn4b3Kc",
    "outputId": "86bd34db-0b48-442e-b5b2-5ff322d0764b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angle</th>\n",
       "      <th>speed</th>\n",
       "      <th>image_file_paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712918428740_95_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/apyba3/petru_data/1712923220525_100_50.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           angle  speed  \\\n",
       "image_id                  \n",
       "13797     0.6250    0.0   \n",
       "13798     0.6875    1.0   \n",
       "13799     0.5625    0.0   \n",
       "13800     0.6250    1.0   \n",
       "\n",
       "                                                                                          image_file_paths  \n",
       "image_id                                                                                                    \n",
       "13797     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13797.png  \n",
       "13798     /home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/training_data/training_data/13798.png  \n",
       "13799                                                       /home/apyba3/petru_data/1712918428740_95_0.png  \n",
       "13800                                                     /home/apyba3/petru_data/1712923220525_100_50.png  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([kaggle_df, extradata_df])\n",
    "merged_df.loc[13797:13800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3OKLcn9u0Pz"
   },
   "source": [
    "### 1g) EDA - angle column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWQCQrR-oCps",
    "outputId": "88bb4558-2c8a-482b-de5d-8f7876ed9bc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angle\n",
       "0.5000    2172\n",
       "0.7500    2172\n",
       "0.6875    2049\n",
       "0.6250    2000\n",
       "0.5625    1644\n",
       "0.4375    1520\n",
       "0.8125    1162\n",
       "0.3750     447\n",
       "0.8750     308\n",
       "0.3125     229\n",
       "0.1875     119\n",
       "0.2500     118\n",
       "0.1250     114\n",
       "0.0000      75\n",
       "0.9375      74\n",
       "0.0625      38\n",
       "1.0000      38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.value_counts('angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4pZ65pYvdqb"
   },
   "source": [
    "note: imbalance datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMZq41-RkLz0"
   },
   "source": [
    "we want to remove the row containing the erroneous 1.428571 speed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TDMqIiOLSKGX"
   },
   "outputs": [],
   "source": [
    "cleaned_df = merged_df[merged_df['angle'] != 1.428571]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di6F6km_DBmj"
   },
   "source": [
    "### 1h) convert images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oeeBTruNCQ96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:26:00.408290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7916 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2025-03-25 14:26:00.408992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9789 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b3:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path, label, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, resized_shape)\n",
    "    image = image / 255.0  # Normalise pixel values to [0,1]\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((cleaned_df[\"image_file_paths\"], cleaned_df[\"angle\"])) # Convert pd df into a tf ds\n",
    "\n",
    "dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(len(cleaned_df))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOlsWQeVlyC"
   },
   "source": [
    "lets check and see if what we have done works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBTNjNhMVk2g",
    "outputId": "b00f1443-c179-43a2-e6fd-7cc90ff698f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:26:11.051612: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 4715 of 14279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 224, 224, 3) (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:26:25.514177: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-03-25 14:26:25.520111: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md6U_i84SiK5"
   },
   "source": [
    "### 1i) Splitting data into training and validation sets (test set is already provided in kaggle data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yYlssPh5dxaO"
   },
   "outputs": [],
   "source": [
    "# 80-20 split\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPUE6rd8cgQN",
    "outputId": "a418b177-e08d-481c-d272-b9b7494882d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2856, validation size: 714\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {train_size}, validation size: {dataset_size - train_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ujsjhMPSw4f"
   },
   "source": [
    "### 1j) Data Augmentation applied to training set\n",
    "\n",
    "- Random Brightness Adjustment\n",
    "- Random Contrast Adjustment\n",
    "- Random Hue Adjustment\n",
    "- Random Saturation Adjustment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T9r811eWsYfe"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "  seed = (6, 9)\n",
    "  image = tf.image.stateless_random_brightness(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_contrast(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "  image = tf.image.stateless_random_saturation(image, 0.8, 1.2, seed)\n",
    "  image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "  return image, label\n",
    "\n",
    "# Create a dataset of augmented images from the original train_dataset\n",
    "augmented_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "train_dataset = train_dataset.concatenate(augmented_dataset)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(cleaned_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOqizFg7rvKq"
   },
   "source": [
    "count how many images are in the training set - 22016 with no extradata and 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjlyfjAxLsrC",
    "outputId": "14dc79ee-e1b4-4c37-bfb1-b6525bc586c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:26:35.629676: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train_dataset: 22848\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "for image_batch, _ in train_dataset:\n",
    "    total_images += image_batch.shape[0]  # Add the batch size\n",
    "\n",
    "print(f\"Total number of images in train_dataset: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEdi-dUCTND1"
   },
   "source": [
    "checking to see if whats been done was successful or needs debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeboVhsQKGFS",
    "outputId": "b9c6bb08-d7ce-4951-b621-6775a6ee3bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.019931316..1.2450541].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.2737076].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.032286078..1.2559795].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (224, 224, 3)\n",
      "label: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0005798936..1.2616403].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1,10)\n",
    "\n",
    "i = 0\n",
    "for image_batch, label_batch in train_dataset.take(1):  # Take one batch\n",
    "    for image in image_batch:  # Iterate through images in the batch\n",
    "        if i < 10:  # Only display the first 5 images\n",
    "            print('image shape: ', np.shape(image))\n",
    "            tf.print('label:', label_batch[i])  # Print label for the corresponding image\n",
    "            axarr[i].imshow(image)\n",
    "            axarr[i].axis('off')\n",
    "            i += 1\n",
    "        else:\n",
    "            break  # Stop after displaying 5 images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmetmzNHTWzU"
   },
   "source": [
    "# 2) Model Building - ResNet50 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48RHLVshdX5L"
   },
   "source": [
    "### 2a) Set up model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aHjqXG1jSnCr"
   },
   "outputs": [],
   "source": [
    "dropoutrate = 0.2\n",
    "input_shape = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ZPso3wBuN9L3",
    "outputId": "ef11a9ff-7117-4836-dd78-9bcadac15995"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> \n",
       "\n",
       " global_average_pooling2d         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n",
       "\n",
       " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " resnet50 (\u001b[38;5;33mFunctional\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)         \u001b[38;5;34m23,587,712\u001b[0m \n",
       "\n",
       " global_average_pooling2d         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m524,544\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m32,896\u001b[0m \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m8,256\u001b[0m \n",
       "\n",
       " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_3 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n",
       "\n",
       " dense_4 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m33\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,155,521</span> (92.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,155,521\u001b[0m (92.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">567,809</span> (2.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m567,809\u001b[0m (2.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet50 = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  resnet50,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "resnet50.trainable = False  # freeze resnet50 layers\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ca0JFQuuN8oI"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) train the model with the resnet50 layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WPTNOtr7WjLS",
    "outputId": "63cf5fac-6100-43db-dadd-47da686c9712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742912817.985070   72565 service.cc:146] XLA service 0x7f0c180026b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742912817.985221   72565 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "I0000 00:00:1742912817.985224   72565 service.cc:154]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-03-25 14:26:58.147713: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-25 14:26:58.904664: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  12/5712\u001b[0m \u001b[37m\u001b[0m \u001b[1m1:23\u001b[0m 15ms/step - loss: 0.2234   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742912822.815294   72565 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0301  \n",
      "Epoch 1: val_loss improved from inf to 0.01262, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 1, Loss: 0.0193, Val Loss: 0.0126\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 17ms/step - loss: 0.0301 - val_loss: 0.0126\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0133  \n",
      "Epoch 2: val_loss improved from 0.01262 to 0.01156, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 2, Loss: 0.0132, Val Loss: 0.0116\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 16ms/step - loss: 0.0133 - val_loss: 0.0116\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0125  \n",
      "Epoch 3: val_loss improved from 0.01156 to 0.00955, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 3, Loss: 0.0124, Val Loss: 0.0095\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0125 - val_loss: 0.0095\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0121  \n",
      "Epoch 4: val_loss did not improve from 0.00955\n",
      "Completed Epoch 4, Loss: 0.0122, Val Loss: 0.0109\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0121 - val_loss: 0.0109\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0117  \n",
      "Epoch 5: val_loss did not improve from 0.00955\n",
      "Completed Epoch 5, Loss: 0.0119, Val Loss: 0.0114\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0117 - val_loss: 0.0114\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6/50\n",
      "\u001b[1m5708/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0116  \n",
      "Epoch 6: val_loss did not improve from 0.00955\n",
      "Completed Epoch 6, Loss: 0.0114, Val Loss: 0.0098\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0116 - val_loss: 0.0098\n",
      "\n",
      "Starting Epoch 7...\n",
      "Epoch 7/50\n",
      "\u001b[1m5708/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0114  \n",
      "Epoch 7: val_loss did not improve from 0.00955\n",
      "Completed Epoch 7, Loss: 0.0113, Val Loss: 0.0106\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0114 - val_loss: 0.0106\n",
      "\n",
      "Starting Epoch 8...\n",
      "Epoch 8/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0114  \n",
      "Epoch 8: val_loss did not improve from 0.00955\n",
      "Completed Epoch 8, Loss: 0.0112, Val Loss: 0.0102\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 14ms/step - loss: 0.0114 - val_loss: 0.0102\n",
      "\n",
      "Starting Epoch 9...\n",
      "Epoch 9/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0113  \n",
      "Epoch 9: val_loss did not improve from 0.00955\n",
      "Completed Epoch 9, Loss: 0.0113, Val Loss: 0.0101\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0113 - val_loss: 0.0101\n",
      "\n",
      "Starting Epoch 10...\n",
      "Epoch 10/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0111  \n",
      "Epoch 10: val_loss improved from 0.00955 to 0.00915, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 10, Loss: 0.0109, Val Loss: 0.0092\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 19ms/step - loss: 0.0111 - val_loss: 0.0092\n",
      "\n",
      "Starting Epoch 11...\n",
      "Epoch 11/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107  \n",
      "Epoch 11: val_loss did not improve from 0.00915\n",
      "Completed Epoch 11, Loss: 0.0109, Val Loss: 0.0092\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0092\n",
      "\n",
      "Starting Epoch 12...\n",
      "Epoch 12/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0106  \n",
      "Epoch 12: val_loss did not improve from 0.00915\n",
      "Completed Epoch 12, Loss: 0.0108, Val Loss: 0.0119\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "\n",
      "Starting Epoch 13...\n",
      "Epoch 13/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0111  \n",
      "Epoch 13: val_loss did not improve from 0.00915\n",
      "Completed Epoch 13, Loss: 0.0108, Val Loss: 0.0109\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "\n",
      "Starting Epoch 14...\n",
      "Epoch 14/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0111  \n",
      "Epoch 14: val_loss did not improve from 0.00915\n",
      "Completed Epoch 14, Loss: 0.0109, Val Loss: 0.0097\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 14ms/step - loss: 0.0111 - val_loss: 0.0097\n",
      "\n",
      "Starting Epoch 15...\n",
      "Epoch 15/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0106  \n",
      "Epoch 15: val_loss improved from 0.00915 to 0.00901, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 15, Loss: 0.0106, Val Loss: 0.0090\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - loss: 0.0106 - val_loss: 0.0090\n",
      "\n",
      "Starting Epoch 16...\n",
      "Epoch 16/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0104  \n",
      "Epoch 16: val_loss improved from 0.00901 to 0.00872, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 16, Loss: 0.0104, Val Loss: 0.0087\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 17ms/step - loss: 0.0104 - val_loss: 0.0087\n",
      "\n",
      "Starting Epoch 17...\n",
      "Epoch 17/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0112  \n",
      "Epoch 17: val_loss did not improve from 0.00872\n",
      "Completed Epoch 17, Loss: 0.0106, Val Loss: 0.0103\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0112 - val_loss: 0.0103\n",
      "\n",
      "Starting Epoch 18...\n",
      "Epoch 18/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107  \n",
      "Epoch 18: val_loss did not improve from 0.00872\n",
      "Completed Epoch 18, Loss: 0.0104, Val Loss: 0.0090\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0090\n",
      "\n",
      "Starting Epoch 19...\n",
      "Epoch 19/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103  \n",
      "Epoch 19: val_loss did not improve from 0.00872\n",
      "Completed Epoch 19, Loss: 0.0105, Val Loss: 0.0089\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0089\n",
      "\n",
      "Starting Epoch 20...\n",
      "Epoch 20/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102  \n",
      "Epoch 20: val_loss did not improve from 0.00872\n",
      "Completed Epoch 20, Loss: 0.0103, Val Loss: 0.0088\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0088\n",
      "\n",
      "Starting Epoch 21...\n",
      "Epoch 21/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0105  \n",
      "Epoch 21: val_loss did not improve from 0.00872\n",
      "Completed Epoch 21, Loss: 0.0102, Val Loss: 0.0106\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "\n",
      "Starting Epoch 22...\n",
      "Epoch 22/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0101  \n",
      "Epoch 22: val_loss did not improve from 0.00872\n",
      "Completed Epoch 22, Loss: 0.0104, Val Loss: 0.0088\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - loss: 0.0101 - val_loss: 0.0088\n",
      "\n",
      "Starting Epoch 23...\n",
      "Epoch 23/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0100  \n",
      "Epoch 23: val_loss did not improve from 0.00872\n",
      "Completed Epoch 23, Loss: 0.0103, Val Loss: 0.0095\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0095\n",
      "\n",
      "Starting Epoch 24...\n",
      "Epoch 24/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102  \n",
      "Epoch 24: val_loss improved from 0.00872 to 0.00865, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 24, Loss: 0.0104, Val Loss: 0.0086\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0086\n",
      "\n",
      "Starting Epoch 25...\n",
      "Epoch 25/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102  \n",
      "Epoch 25: val_loss did not improve from 0.00865\n",
      "Completed Epoch 25, Loss: 0.0101, Val Loss: 0.0097\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0097\n",
      "\n",
      "Starting Epoch 26...\n",
      "Epoch 26/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103  \n",
      "Epoch 26: val_loss did not improve from 0.00865\n",
      "Completed Epoch 26, Loss: 0.0104, Val Loss: 0.0100\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0100\n",
      "\n",
      "Starting Epoch 27...\n",
      "Epoch 27/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0104  \n",
      "Epoch 27: val_loss did not improve from 0.00865\n",
      "Completed Epoch 27, Loss: 0.0109, Val Loss: 0.0091\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0104 - val_loss: 0.0091\n",
      "\n",
      "Starting Epoch 28...\n",
      "Epoch 28/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0100  \n",
      "Epoch 28: val_loss did not improve from 0.00865\n",
      "Completed Epoch 28, Loss: 0.0102, Val Loss: 0.0095\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0095\n",
      "\n",
      "Starting Epoch 29...\n",
      "Epoch 29/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103  \n",
      "Epoch 29: val_loss did not improve from 0.00865\n",
      "Completed Epoch 29, Loss: 0.0106, Val Loss: 0.0093\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0093\n",
      "\n",
      "Starting Epoch 30...\n",
      "Epoch 30/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107  \n",
      "Epoch 30: val_loss did not improve from 0.00865\n",
      "Completed Epoch 30, Loss: 0.0105, Val Loss: 0.0114\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "\n",
      "Starting Epoch 31...\n",
      "Epoch 31/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0107  \n",
      "Epoch 31: val_loss improved from 0.00865 to 0.00785, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 31, Loss: 0.0104, Val Loss: 0.0079\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0079\n",
      "\n",
      "Starting Epoch 32...\n",
      "Epoch 32/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103  \n",
      "Epoch 32: val_loss improved from 0.00785 to 0.00784, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 32, Loss: 0.0102, Val Loss: 0.0078\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0103 - val_loss: 0.0078\n",
      "\n",
      "Starting Epoch 33...\n",
      "Epoch 33/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0100  \n",
      "Epoch 33: val_loss did not improve from 0.00784\n",
      "Completed Epoch 33, Loss: 0.0101, Val Loss: 0.0099\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "\n",
      "Starting Epoch 34...\n",
      "Epoch 34/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101  \n",
      "Epoch 34: val_loss did not improve from 0.00784\n",
      "Completed Epoch 34, Loss: 0.0102, Val Loss: 0.0094\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0101 - val_loss: 0.0094\n",
      "\n",
      "Starting Epoch 35...\n",
      "Epoch 35/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0100  \n",
      "Epoch 35: val_loss did not improve from 0.00784\n",
      "Completed Epoch 35, Loss: 0.0100, Val Loss: 0.0087\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 14ms/step - loss: 0.0100 - val_loss: 0.0087\n",
      "\n",
      "Starting Epoch 36...\n",
      "Epoch 36/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0099  \n",
      "Epoch 36: val_loss did not improve from 0.00784\n",
      "Completed Epoch 36, Loss: 0.0098, Val Loss: 0.0087\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0099 - val_loss: 0.0087\n",
      "\n",
      "Starting Epoch 37...\n",
      "Epoch 37/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101  \n",
      "Epoch 37: val_loss did not improve from 0.00784\n",
      "Completed Epoch 37, Loss: 0.0099, Val Loss: 0.0091\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0101 - val_loss: 0.0091\n",
      "\n",
      "Starting Epoch 38...\n",
      "Epoch 38/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0099  \n",
      "Epoch 38: val_loss did not improve from 0.00784\n",
      "Completed Epoch 38, Loss: 0.0098, Val Loss: 0.0082\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 16ms/step - loss: 0.0099 - val_loss: 0.0082\n",
      "\n",
      "Starting Epoch 39...\n",
      "Epoch 39/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0099  \n",
      "Epoch 39: val_loss did not improve from 0.00784\n",
      "Completed Epoch 39, Loss: 0.0098, Val Loss: 0.0080\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - loss: 0.0099 - val_loss: 0.0080\n",
      "\n",
      "Starting Epoch 40...\n",
      "Epoch 40/50\n",
      "\u001b[1m5709/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0104  \n",
      "Epoch 40: val_loss did not improve from 0.00784\n",
      "Completed Epoch 40, Loss: 0.0100, Val Loss: 0.0087\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 15ms/step - loss: 0.0104 - val_loss: 0.0087\n",
      "\n",
      "Starting Epoch 41...\n",
      "Epoch 41/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102  \n",
      "Epoch 41: val_loss did not improve from 0.00784\n",
      "Completed Epoch 41, Loss: 0.0101, Val Loss: 0.0097\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0097\n",
      "\n",
      "Starting Epoch 42...\n",
      "Epoch 42/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0099  \n",
      "Epoch 42: val_loss did not improve from 0.00784\n",
      "Completed Epoch 42, Loss: 0.0099, Val Loss: 0.0096\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0099 - val_loss: 0.0096\n",
      "\n",
      "Starting Epoch 43...\n",
      "Epoch 43/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098  \n",
      "Epoch 43: val_loss did not improve from 0.00784\n",
      "Completed Epoch 43, Loss: 0.0098, Val Loss: 0.0088\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0088\n",
      "\n",
      "Starting Epoch 44...\n",
      "Epoch 44/50\n",
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0098  \n",
      "Epoch 44: val_loss did not improve from 0.00784\n",
      "Completed Epoch 44, Loss: 0.0097, Val Loss: 0.0079\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0079\n",
      "\n",
      "Starting Epoch 45...\n",
      "Epoch 45/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098  \n",
      "Epoch 45: val_loss did not improve from 0.00784\n",
      "Completed Epoch 45, Loss: 0.0102, Val Loss: 0.0093\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0093\n",
      "\n",
      "Starting Epoch 46...\n",
      "Epoch 46/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098  \n",
      "Epoch 46: val_loss did not improve from 0.00784\n",
      "Completed Epoch 46, Loss: 0.0100, Val Loss: 0.0096\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "\n",
      "Starting Epoch 47...\n",
      "Epoch 47/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0097  \n",
      "Epoch 47: val_loss did not improve from 0.00784\n",
      "Completed Epoch 47, Loss: 0.0097, Val Loss: 0.0094\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - loss: 0.0097 - val_loss: 0.0094\n",
      "\n",
      "Starting Epoch 48...\n",
      "Epoch 48/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098  \n",
      "Epoch 48: val_loss did not improve from 0.00784\n",
      "Completed Epoch 48, Loss: 0.0098, Val Loss: 0.0199\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 16ms/step - loss: 0.0098 - val_loss: 0.0199\n",
      "\n",
      "Starting Epoch 49...\n",
      "Epoch 49/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098  \n",
      "Epoch 49: val_loss did not improve from 0.00784\n",
      "Completed Epoch 49, Loss: 0.0098, Val Loss: 0.0092\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 16ms/step - loss: 0.0098 - val_loss: 0.0092\n",
      "\n",
      "Starting Epoch 50...\n",
      "Epoch 50/50\n",
      "\u001b[1m5710/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100  \n",
      "Epoch 50: val_loss improved from 0.00784 to 0.00762, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 50, Loss: 0.0101, Val Loss: 0.0076\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - loss: 0.0100 - val_loss: 0.0076\n"
     ]
    }
   ],
   "source": [
    "# Define ModelCheckpoint callback\n",
    "checkpoint_filepath = '/home/apyba3/ResNet50/resnet50checkpoint.keras'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define a callback to print epoch tracking info\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training loop with added callback\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]  # Include both callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FiHy6opSP2sQ"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/apyba3/car_frozen_regression_resnet50.weights.h5')\n",
    "# model.save_weights('/home/ppytr13/car_frozen.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FpLHyw20P93U"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #Clear keras session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENHbUvQdvyFe"
   },
   "source": [
    "### 2d) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ek_ytyw0KB"
   },
   "source": [
    "rebuild model after clearing keras session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> \n",
       "\n",
       " global_average_pooling2d         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n",
       "\n",
       " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " resnet50 (\u001b[38;5;33mFunctional\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)         \u001b[38;5;34m23,587,712\u001b[0m \n",
       "\n",
       " global_average_pooling2d         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m524,544\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m32,896\u001b[0m \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m8,256\u001b[0m \n",
       "\n",
       " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_3 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n",
       "\n",
       " dense_4 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m33\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,155,521</span> (92.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,155,521\u001b[0m (92.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,102,401</span> (91.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,102,401\u001b[0m (91.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet50 = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  resnet50,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(dropoutrate),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "resnet50.trainable = True  # Make the entire model trainable\n",
    "\n",
    "model.summary() # print the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # deliberately smaller learning rate\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now load in the learned weights from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8oAenzEiP-C-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apyba3/anaconda3/envs/mlis2cluster/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('/home/apyba3/car_frozen_regression_resnet50.weights.h5')\n",
    "# model.load_weights('/home/ppytr13/car_frozen.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDtRxBow89t"
   },
   "source": [
    "Initiate fine-tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 15:47:26.813212: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_56', 480 bytes spill stores, 488 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_273', 288 bytes spill stores, 320 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5711/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0242  \n",
      "Epoch 1: val_loss improved from inf to 0.01212, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 1, Loss: 0.0162, Val Loss: 0.0121\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 49ms/step - loss: 0.0242 - val_loss: 0.0121\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0137  \n",
      "Epoch 2: val_loss improved from 0.01212 to 0.00894, saving model to /home/apyba3/ResNet50/resnet50checkpoint.keras\n",
      "Completed Epoch 2, Loss: 0.0134, Val Loss: 0.0089\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 51ms/step - loss: 0.0137 - val_loss: 0.0089\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0096  \n",
      "Epoch 3: val_loss did not improve from 0.00894\n",
      "Completed Epoch 3, Loss: 0.0092, Val Loss: 0.0095\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 44ms/step - loss: 0.0096 - val_loss: 0.0095\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4/50\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0086  \n",
      "Epoch 4: val_loss did not improve from 0.00894\n",
      "Completed Epoch 4, Loss: 0.0087, Val Loss: 0.0143\n",
      "\u001b[1m5712/5712\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 43ms/step - loss: 0.0086 - val_loss: 0.0143\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5/50\n"
     ]
    }
   ],
   "source": [
    "# Define ModelCheckpoint callback\n",
    "checkpoint_filepath = '/home/apyba3/ResNet50/resnet50checkpoint.keras'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define a callback to print epoch tracking info\n",
    "epoch_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f\"\\nStarting Epoch {epoch + 1}...\"),\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Completed Epoch {epoch + 1}, Loss: {logs['loss']:.4f}, Val Loss: {logs['val_loss']:.4f}\")\n",
    ")\n",
    "\n",
    "# Training loop with added callback\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[model_checkpoint, epoch_callback]  # Include both callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the weights learned from fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O14u6175RLjA"
   },
   "outputs": [],
   "source": [
    "model.save_weights('car_unfrozen_regression_resnet50.weights.h5')\n",
    "# model.save_weights('/home/ppytr13/car_unfrozen.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCbo4VcLxLgQ"
   },
   "source": [
    "# 3) Test-Set Predictions\n",
    "\n",
    "a) load in test data\n",
    "\n",
    "b) convert test images to numerical RGB feature maps\n",
    "\n",
    "c) generate predictions on the test set\n",
    "\n",
    "d) correctly format the predictions into a pandas dataframe\n",
    "\n",
    "e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnygDJsKxYhA"
   },
   "source": [
    "### 3a) load in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "W-e59lQQRXKK",
    "outputId": "aa8566ec-e472-47a6-c7a0-92266b567a62"
   },
   "outputs": [],
   "source": [
    "image_folder_path = '/home/apyba3/KAGGLEDATAmachine-learning-in-science-ii-2025/test_data/test_data'\n",
    "# image_folder_path = '/home/ppyt13/machine-learning-in-science-ii-2025/test_data/test_data' # tylers file path\n",
    "image_file_paths = [\n",
    "    os.path.join(image_folder_path, f)\n",
    "    for f in os.listdir(image_folder_path)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "]\n",
    "\n",
    "image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0])) # sorts the files in the right order (1.png, 2.png, 3.png, ...)\n",
    "\n",
    "imagefilepaths_df = pd.DataFrame(\n",
    "    image_file_paths,\n",
    "    columns=['image_file_paths'],\n",
    "    index=[int(os.path.splitext(os.path.basename(path))[0]) for path in image_file_paths]\n",
    ")\n",
    "\n",
    "imagefilepaths_df.index.name = 'image_id'\n",
    "imagefilepaths_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-9i5trTyDTf"
   },
   "source": [
    "### 3b) convert test images to numerical RGB feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hT_c1s5TAR-"
   },
   "outputs": [],
   "source": [
    "def process_image_no_label(image_path, resized_shape=(224, 224)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Use decode_png for PNG images\n",
    "    image = tf.image.resize(image, resized_shape)  # Resize to uniform shape\n",
    "    image = image / 255.0  # Normalize pixel values to [0,1]\n",
    "    return image\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((imagefilepaths_df[\"image_file_paths\"]))\n",
    "\n",
    "test_dataset = test_dataset.map(process_image_no_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gobnK7PhyLa2"
   },
   "source": [
    "### 3c) generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtqcOFr7TAXa",
    "outputId": "73b4c96b-51bf-4e1c-e1b6-e8cde1321984"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT1LJxHTPeQT"
   },
   "source": [
    "### 3d) correctly format the predictions into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFVWGi04fza7"
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['angle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OnO0K1rReHOT",
    "outputId": "d9cebb2e-3d36-4c7a-b024-eabb646e3bbb"
   },
   "outputs": [],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CcRKL9KTAfs",
    "outputId": "277533cd-06aa-4709-d44e-9027cc7e9438"
   },
   "outputs": [],
   "source": [
    "predictions_df['angle'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU-PhskZPaHD"
   },
   "source": [
    "### 3e) save predictions to a file inside the hpc (to then later send from hpc to my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deXjPTO0TAiL"
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv('/home/apyba3/resnet50_angleregression_withvalidation_withpetrudata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsp7UPIJQlKB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
